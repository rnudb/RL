{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-4k6sAMSEEi"
      },
      "source": [
        "#### Arnaud Baradat\n",
        "\n",
        "Final Project of Reinforcement Learning\n",
        "## Deep Q Network - Human-level control through deep reinforcement learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjluRK_lSEEo",
        "outputId": "fb45a18b-6205-4068-919f-a6c9a2e4fbf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (4.8.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (0.4.2)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from gymnasium[accept-rom-license,atari]) (0.2.1)\n",
            "Requirement already satisfied: click in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (0.6.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2023.7.22)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[atari,accept-rom-license]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z-mk1lULSEEq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "[Powered by Stella]\n"
          ]
        }
      ],
      "source": [
        "import typing as t\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import RMSprop, Adam\n",
        "\n",
        "env = gym.make(\"ALE/Breakout-v5\", frameskip=1)\n",
        "env = gym.wrappers.AtariPreprocessing(env, grayscale_obs=False)\n",
        "env = gym.wrappers.FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3a6VwAjSEEr",
        "outputId": "57751bcf-0b4e-4356-ace7-05b5abcc1663"
      },
      "outputs": [],
      "source": [
        "# Create a Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, action_size, seed):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.normalize = lambda x: x / 255.0\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(64*7*7, 512)\n",
        "        self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.normalize(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # x = F.dropout(x, p=0.2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        # x = F.dropout(x, p=0.2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def step(self, state):\n",
        "        state = state.float().to(self.device)\n",
        "        # self.eval()\n",
        "        # with torch.no_grad():\n",
        "        action_values = self.forward(state)\n",
        "        # self.train()\n",
        "        # print(\"action values: \", action_values.cpu().data.numpy())\n",
        "        action = np.argmax(action_values.cpu().data.numpy()[-1])\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60l42LL8SEEs"
      },
      "source": [
        "#### Train:\n",
        "\n",
        "- Use RMSProp optimizer\n",
        "- Can do up to k times the same actions between each model update\n",
        "- our algorithm only stores the last N experience tuples in the replay\n",
        "memory, and samples uniformly at random from D when performing updates\n",
        "- clipping the squared error to be between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wa5Q3z49SEEt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def preprocess_image_stack(img_stack):\n",
        "    res = []\n",
        "    for idx, image in enumerate(img_stack):\n",
        "        # First, to encode a singleframe we take the maximum value for each pixel colour\n",
        "        # value over the frame being encoded and the previous frame.\n",
        "        img = np.array(image)\n",
        "        prev_img = img_stack[idx - 1]\n",
        "        img = np.maximum(img, prev_img)\n",
        "\n",
        "        # Second, we then extract the Y channel, also known as luminance, from the RGB frame\n",
        "        luminance = 0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] + 0.114 * img[:, :, 2]\n",
        "\n",
        "        # Add luminance as 4th channel to img\n",
        "        img = np.dstack((img, luminance))\n",
        "\n",
        "        # Resize to 84 x 84\n",
        "        # img = cv2.resize(img, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        res.append(img.reshape(4, 84, 84))\n",
        "\n",
        "    # Stack into a single tensor\n",
        "    return torch.from_numpy(np.stack(res, axis=0)).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "StbiF0LaSEEv"
      },
      "outputs": [],
      "source": [
        "def declare_model_and_optimier(model_state_dict=None, optimizer_state_dict=None):\n",
        "    \"\"\"\n",
        "    Returns new model and load state dict if needed\n",
        "    Declares a new optimizer with the model\n",
        "    \"\"\"\n",
        "    model = QNetwork(action_size=env.action_space.n, seed=42)\n",
        "    if model_state_dict is not None:\n",
        "        model.load_state_dict(model_state_dict)\n",
        "\n",
        "\n",
        "    optimizer = RMSprop(model.parameters(), lr=1e-3, momentum=0.95, eps=0.01)\n",
        "    # optimizer = Adam(params=model.parameters(), lr=1e-4, eps=1e-6)\n",
        "    if optimizer_state_dict is not None:\n",
        "        optimizer.load_state_dict(optimizer_state_dict)\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_replay_buffer(replay_buffer, buffer_size=100000):\n",
        "    \"\"\"\n",
        "    Fills the replay buffer with random actions\n",
        "    \"\"\"\n",
        "    env.reset()\n",
        "    state = env.step(0)[0]\n",
        "    state = preprocess_image_stack(state)\n",
        "    state = state\n",
        "    for i in range(buffer_size):\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        next_state = preprocess_image_stack(next_state)\n",
        "        next_state = next_state\n",
        "        replay_buffer.append((\n",
        "            torch.tensor(np.array(state)[-1, :, :]),\n",
        "            action,\n",
        "            reward,\n",
        "            torch.tensor(np.array(next_state)[-1, :, :]),\n",
        "            done\n",
        "            ))\n",
        "        state = next_state\n",
        "        if done:\n",
        "            env.reset()\n",
        "            state = env.step(0)[0]\n",
        "            state = preprocess_image_stack(state)\n",
        "            state = state\n",
        "    \n",
        "    return replay_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fill the replay buffer with random actions\n",
        "replay_buffer = fill_replay_buffer([], 10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D94b6XM6SEEw",
        "outputId": "e7a46d5f-c2d5-4e6d-f40e-462260168689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOTAL EPOCH: 1, Episode: 0, Epoch: 0, Loss: 0.025829294696450233, Mean Reward: 1.0, Actions in batch: {0: 3, 1: 7, 2: 20, 3: 2}\n",
            "TOTAL EPOCH: 21, Episode: 0, Epoch: 20, Loss: 0.014097596518695354, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 41, Episode: 0, Epoch: 40, Loss: 0.00873621366918087, Mean Reward: 2.34375, Actions in batch: {0: 4, 1: 6, 2: 20, 3: 2}\n",
            "Max reward for this episode:  2.0\n",
            "Saving model that went for  58  epochs with reward  2.0\n",
            "TOTAL EPOCH: 60, Episode: 1, Epoch: 0, Loss: 0.0010396717116236687, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 80, Episode: 1, Epoch: 20, Loss: 0.011931954883038998, Mean Reward: 2.21875, Actions in batch: {0: 3, 1: 4, 2: 21, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 98, Episode: 2, Epoch: 0, Loss: 0.0364127941429615, Mean Reward: 0.1875, Actions in batch: {0: 3, 1: 7, 2: 13, 3: 9}\n",
            "TOTAL EPOCH: 118, Episode: 2, Epoch: 20, Loss: 0.007817868143320084, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 138, Episode: 2, Epoch: 40, Loss: 0.0018195416778326035, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 25, 3: 4}\n",
            "TOTAL EPOCH: 158, Episode: 2, Epoch: 60, Loss: 0.0019331523217260838, Mean Reward: 1.0, Actions in batch: {0: 10, 1: 2, 2: 13, 3: 7}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 161, Episode: 3, Epoch: 0, Loss: 0.011232355609536171, Mean Reward: 1.875, Actions in batch: {0: 4, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 181, Episode: 3, Epoch: 20, Loss: 0.008197294548153877, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 10, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 201, Episode: 3, Epoch: 40, Loss: 0.00413200818002224, Mean Reward: 2.375, Actions in batch: {0: 7, 1: 6, 2: 15, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 211, Episode: 4, Epoch: 0, Loss: 0.004004122223705053, Mean Reward: 1.75, Actions in batch: {0: 8, 1: 1, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 231, Episode: 4, Epoch: 20, Loss: 0.0004613014170899987, Mean Reward: 3.0, Actions in batch: {0: 8, 1: 5, 2: 15, 3: 4}\n",
            "TOTAL EPOCH: 251, Episode: 4, Epoch: 40, Loss: 0.0011577419936656952, Mean Reward: 1.75, Actions in batch: {0: 8, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 271, Episode: 4, Epoch: 60, Loss: 0.0008948324830271304, Mean Reward: 2.65625, Actions in batch: {0: 7, 1: 6, 2: 11, 3: 8}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 279, Episode: 5, Epoch: 0, Loss: 0.00669781444594264, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 10, 2: 6, 3: 6}\n",
            "TOTAL EPOCH: 299, Episode: 5, Epoch: 20, Loss: 0.004144781734794378, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 10, 2: 11, 3: 5}\n",
            "TOTAL EPOCH: 319, Episode: 5, Epoch: 40, Loss: 0.003561855759471655, Mean Reward: 0.75, Actions in batch: {0: 3, 1: 7, 2: 16, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 327, Episode: 6, Epoch: 0, Loss: 0.00022519250342156738, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 10, 2: 10, 3: 5}\n",
            "TOTAL EPOCH: 347, Episode: 6, Epoch: 20, Loss: 0.0050010825507342815, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 367, Episode: 6, Epoch: 40, Loss: 7.140250090742484e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 5, 2: 21, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 382, Episode: 7, Epoch: 0, Loss: 0.02962431311607361, Mean Reward: 0.71875, Actions in batch: {0: 13, 1: 3, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 402, Episode: 7, Epoch: 20, Loss: 0.0012248388957232237, Mean Reward: 1.5625, Actions in batch: {0: 13, 1: 7, 2: 5, 3: 7}\n",
            "TOTAL EPOCH: 422, Episode: 7, Epoch: 40, Loss: 7.610241300426424e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 442, Episode: 7, Epoch: 60, Loss: 0.0016191729810088873, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 10, 2: 12, 3: 6}\n",
            "Max reward for this episode:  3.0\n",
            "Saving model that went for  67  epochs with reward  3.0\n",
            "TOTAL EPOCH: 450, Episode: 8, Epoch: 0, Loss: 0.003240168560296297, Mean Reward: 0.84375, Actions in batch: {0: 5, 1: 13, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 470, Episode: 8, Epoch: 20, Loss: 0.0009791575139388442, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 7, 2: 11, 3: 12}\n",
            "TOTAL EPOCH: 490, Episode: 8, Epoch: 40, Loss: 0.0005652836989611387, Mean Reward: 2.1875, Actions in batch: {0: 4, 1: 1, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 510, Episode: 8, Epoch: 60, Loss: 0.004565161652863026, Mean Reward: 1.6875, Actions in batch: {0: 10, 1: 7, 2: 11, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 511, Episode: 9, Epoch: 0, Loss: 0.0037576546892523766, Mean Reward: 2.78125, Actions in batch: {0: 3, 1: 3, 2: 19, 3: 7}\n",
            "TOTAL EPOCH: 531, Episode: 9, Epoch: 20, Loss: 0.003523140214383602, Mean Reward: 0.875, Actions in batch: {0: 8, 1: 11, 2: 8, 3: 5}\n",
            "TOTAL EPOCH: 551, Episode: 9, Epoch: 40, Loss: 0.0028486326336860657, Mean Reward: 1.21875, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 571, Episode: 9, Epoch: 60, Loss: 0.0067353080958127975, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 576, Episode: 10, Epoch: 0, Loss: 0.0034629993606358767, Mean Reward: 0.0, Actions in batch: {0: 13, 1: 7, 2: 8, 3: 4}\n",
            "TOTAL EPOCH: 596, Episode: 10, Epoch: 20, Loss: 0.001302649499848485, Mean Reward: 2.96875, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 616, Episode: 10, Epoch: 40, Loss: 0.00016910281556192786, Mean Reward: 0.0, Actions in batch: {1: 4, 2: 27, 3: 1}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 626, Episode: 11, Epoch: 0, Loss: 0.00011980629642494023, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 646, Episode: 11, Epoch: 20, Loss: 0.0026466117706149817, Mean Reward: 3.96875, Actions in batch: {0: 7, 1: 7, 2: 6, 3: 12}\n",
            "TOTAL EPOCH: 666, Episode: 11, Epoch: 40, Loss: 0.004261075984686613, Mean Reward: 1.28125, Actions in batch: {0: 7, 1: 10, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 686, Episode: 11, Epoch: 60, Loss: 0.00023987787426449358, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 3, 2: 23, 3: 1}\n",
            "TOTAL EPOCH: 706, Episode: 11, Epoch: 80, Loss: 0.0006451497320085764, Mean Reward: 2.0, Actions in batch: {0: 4, 1: 8, 2: 11, 3: 9}\n",
            "Max reward for this episode:  4.0\n",
            "Saving model that went for  83  epochs with reward  4.0\n",
            "TOTAL EPOCH: 710, Episode: 12, Epoch: 0, Loss: 0.0026820898056030273, Mean Reward: 1.5, Actions in batch: {0: 4, 1: 1, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 730, Episode: 12, Epoch: 20, Loss: 0.0017034730408340693, Mean Reward: 0.96875, Actions in batch: {0: 7, 1: 8, 2: 7, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 744, Episode: 13, Epoch: 0, Loss: 0.000675516203045845, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 764, Episode: 13, Epoch: 20, Loss: 0.001365498173981905, Mean Reward: 1.8125, Actions in batch: {0: 2, 1: 1, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 784, Episode: 13, Epoch: 40, Loss: 0.004280210472643375, Mean Reward: 2.625, Actions in batch: {0: 10, 1: 7, 2: 6, 3: 9}\n",
            "TOTAL EPOCH: 804, Episode: 13, Epoch: 60, Loss: 0.015229219570755959, Mean Reward: 0.25, Actions in batch: {0: 4, 1: 8, 2: 15, 3: 5}\n",
            "TOTAL EPOCH: 824, Episode: 13, Epoch: 80, Loss: 0.00030295230681076646, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 825, Episode: 14, Epoch: 0, Loss: 0.005467764567583799, Mean Reward: 0.5, Actions in batch: {0: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 845, Episode: 14, Epoch: 20, Loss: 0.031167326495051384, Mean Reward: 0.0625, Actions in batch: {0: 6, 1: 8, 2: 9, 3: 9}\n",
            "TOTAL EPOCH: 865, Episode: 14, Epoch: 40, Loss: 0.01788063533604145, Mean Reward: 0.09375, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 885, Episode: 14, Epoch: 60, Loss: 0.007881365716457367, Mean Reward: 0.53125, Actions in batch: {0: 4, 1: 6, 2: 12, 3: 10}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 905, Episode: 15, Epoch: 0, Loss: 0.01050092838704586, Mean Reward: 0.53125, Actions in batch: {0: 8, 1: 6, 2: 7, 3: 11}\n",
            "TOTAL EPOCH: 925, Episode: 15, Epoch: 20, Loss: 0.004113039467483759, Mean Reward: 0.8125, Actions in batch: {0: 10, 1: 7, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 945, Episode: 15, Epoch: 40, Loss: 0.0024726171977818012, Mean Reward: 1.21875, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 965, Episode: 15, Epoch: 60, Loss: 0.00022443413035944104, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 971, Episode: 16, Epoch: 0, Loss: 0.001713237608782947, Mean Reward: 1.71875, Actions in batch: {1: 1, 2: 31}\n",
            "TOTAL EPOCH: 991, Episode: 16, Epoch: 20, Loss: 0.00029856577748432755, Mean Reward: 2.0, Actions in batch: {0: 3, 1: 2, 2: 27}\n",
            "TOTAL EPOCH: 1011, Episode: 16, Epoch: 40, Loss: 4.1797822632361203e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 6, 2: 13, 3: 4}\n",
            "TOTAL EPOCH: 1031, Episode: 16, Epoch: 60, Loss: 0.0017256722785532475, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 2, 2: 28}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 1048, Episode: 17, Epoch: 0, Loss: 0.0001578441442688927, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 30}\n",
            "TOTAL EPOCH: 1068, Episode: 17, Epoch: 20, Loss: 0.007584424689412117, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 8, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 1088, Episode: 17, Epoch: 40, Loss: 1.931103724928107e-06, Mean Reward: 0.0, Actions in batch: {1: 1, 2: 30, 3: 1}\n",
            "TOTAL EPOCH: 1108, Episode: 17, Epoch: 60, Loss: 0.0028783855959773064, Mean Reward: 0.59375, Actions in batch: {0: 4, 1: 4, 2: 18, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 1109, Episode: 18, Epoch: 0, Loss: 0.0013606382999569178, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 16, 3: 8}\n",
            "TOTAL EPOCH: 1129, Episode: 18, Epoch: 20, Loss: 0.0048017497174441814, Mean Reward: 0.5625, Actions in batch: {0: 3, 1: 5, 2: 12, 3: 12}\n",
            "TOTAL EPOCH: 1149, Episode: 18, Epoch: 40, Loss: 0.00012156591401435435, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 8, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 1169, Episode: 18, Epoch: 60, Loss: 0.000575770391151309, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 1189, Episode: 18, Epoch: 80, Loss: 0.0034314459189772606, Mean Reward: 1.71875, Actions in batch: {0: 11, 1: 4, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 1209, Episode: 19, Epoch: 0, Loss: 0.013156605884432793, Mean Reward: 0.3125, Actions in batch: {0: 5, 1: 4, 2: 12, 3: 11}\n",
            "TOTAL EPOCH: 1229, Episode: 19, Epoch: 20, Loss: 0.0019578745122998953, Mean Reward: 2.3125, Actions in batch: {0: 7, 1: 11, 2: 9, 3: 5}\n",
            "TOTAL EPOCH: 1249, Episode: 19, Epoch: 40, Loss: 9.19619487831369e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 8, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 1269, Episode: 19, Epoch: 60, Loss: 0.0008131999638862908, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 9, 2: 6, 3: 10}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 1270, Episode: 20, Epoch: 0, Loss: 0.0021232799626886845, Mean Reward: 0.53125, Actions in batch: {0: 5, 1: 5, 2: 14, 3: 8}\n",
            "TOTAL EPOCH: 1290, Episode: 20, Epoch: 20, Loss: 4.7410176193807274e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 13, 2: 5, 3: 6}\n",
            "TOTAL EPOCH: 1310, Episode: 20, Epoch: 40, Loss: 8.509904546372127e-06, Mean Reward: 0.0, Actions in batch: {0: 5, 2: 22, 3: 5}\n",
            "TOTAL EPOCH: 1330, Episode: 20, Epoch: 60, Loss: 0.008650030940771103, Mean Reward: 0.6875, Actions in batch: {0: 3, 1: 7, 2: 20, 3: 2}\n",
            "TOTAL EPOCH: 1350, Episode: 20, Epoch: 80, Loss: 0.0003895134723279625, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 1353, Episode: 21, Epoch: 0, Loss: 0.005036864895373583, Mean Reward: 0.6875, Actions in batch: {0: 8, 1: 3, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 1373, Episode: 21, Epoch: 20, Loss: 0.0015845048474147916, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 1393, Episode: 21, Epoch: 40, Loss: 4.0636608900967985e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 1412, Episode: 22, Epoch: 0, Loss: 0.00022937663015909493, Mean Reward: 0.8125, Actions in batch: {0: 9, 1: 4, 2: 14, 3: 5}\n",
            "TOTAL EPOCH: 1432, Episode: 22, Epoch: 20, Loss: 4.46772864961531e-05, Mean Reward: 0.75, Actions in batch: {0: 3, 1: 10, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 1452, Episode: 22, Epoch: 40, Loss: 0.001866261358372867, Mean Reward: 0.875, Actions in batch: {0: 3, 1: 3, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 1472, Episode: 22, Epoch: 60, Loss: 0.0025123325176537037, Mean Reward: 2.0, Actions in batch: {0: 7, 1: 1, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 1492, Episode: 22, Epoch: 80, Loss: 0.00010335780825698748, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 1512, Episode: 23, Epoch: 0, Loss: 0.00018320974777452648, Mean Reward: 0.0, Actions in batch: {0: 11, 1: 9, 2: 8, 3: 4}\n",
            "TOTAL EPOCH: 1532, Episode: 23, Epoch: 20, Loss: 0.0035859139170497656, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 1552, Episode: 23, Epoch: 40, Loss: 0.0013102024095132947, Mean Reward: 0.90625, Actions in batch: {0: 7, 1: 6, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 1572, Episode: 23, Epoch: 60, Loss: 0.00038547907024621964, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 26, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 1591, Episode: 24, Epoch: 0, Loss: 0.0003450394724495709, Mean Reward: 1.0, Actions in batch: {0: 7, 1: 2, 2: 12, 3: 11}\n",
            "TOTAL EPOCH: 1611, Episode: 24, Epoch: 20, Loss: 0.0002098180411849171, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 20, 3: 6}\n",
            "TOTAL EPOCH: 1631, Episode: 24, Epoch: 40, Loss: 0.002459402894601226, Mean Reward: 1.96875, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 1651, Episode: 24, Epoch: 60, Loss: 0.009301834739744663, Mean Reward: 0.28125, Actions in batch: {0: 8, 1: 9, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 1671, Episode: 24, Epoch: 80, Loss: 0.0012725478736683726, Mean Reward: 2.46875, Actions in batch: {0: 4, 1: 2, 2: 22, 3: 4}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 1676, Episode: 25, Epoch: 0, Loss: 0.009957820177078247, Mean Reward: 0.375, Actions in batch: {0: 1, 1: 2, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 1696, Episode: 25, Epoch: 20, Loss: 0.0021840007975697517, Mean Reward: 1.53125, Actions in batch: {0: 6, 1: 10, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 1716, Episode: 25, Epoch: 40, Loss: 1.6497910110047087e-05, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 11, 2: 5, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 1720, Episode: 26, Epoch: 0, Loss: 0.004232871346175671, Mean Reward: 1.25, Actions in batch: {0: 9, 1: 7, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 1740, Episode: 26, Epoch: 20, Loss: 0.005283229053020477, Mean Reward: 0.6875, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 1760, Episode: 26, Epoch: 40, Loss: 0.0021447050385177135, Mean Reward: 1.71875, Actions in batch: {0: 3, 1: 3, 2: 22, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1762, Episode: 27, Epoch: 0, Loss: 0.0003809288900811225, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 1782, Episode: 27, Epoch: 20, Loss: 0.005626429803669453, Mean Reward: 0.4375, Actions in batch: {0: 6, 1: 10, 2: 7, 3: 9}\n",
            "TOTAL EPOCH: 1802, Episode: 27, Epoch: 40, Loss: 0.0056351167149841785, Mean Reward: 0.75, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 1822, Episode: 27, Epoch: 60, Loss: 0.009702191688120365, Mean Reward: 0.71875, Actions in batch: {0: 7, 1: 9, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 1842, Episode: 27, Epoch: 80, Loss: 0.0002243874769192189, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 1862, Episode: 28, Epoch: 0, Loss: 0.0019760900177061558, Mean Reward: 0.46875, Actions in batch: {0: 5, 1: 5, 2: 12, 3: 10}\n",
            "TOTAL EPOCH: 1882, Episode: 28, Epoch: 20, Loss: 7.169845048338175e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 4, 2: 13, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1901, Episode: 29, Epoch: 0, Loss: 0.006822825409471989, Mean Reward: 1.34375, Actions in batch: {0: 7, 1: 10, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 1921, Episode: 29, Epoch: 20, Loss: 0.0013423326890915632, Mean Reward: 3.4375, Actions in batch: {0: 8, 1: 7, 2: 11, 3: 6}\n",
            "TOTAL EPOCH: 1941, Episode: 29, Epoch: 40, Loss: 0.0018414233345538378, Mean Reward: 2.0625, Actions in batch: {0: 9, 1: 6, 2: 12, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1945, Episode: 30, Epoch: 0, Loss: 0.0010149967856705189, Mean Reward: 1.75, Actions in batch: {0: 1, 1: 4, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 1965, Episode: 30, Epoch: 20, Loss: 0.00014574969827663153, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 7, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 1985, Episode: 30, Epoch: 40, Loss: 0.00041983494884334505, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 2005, Episode: 30, Epoch: 60, Loss: 1.2500750017352402e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 2025, Episode: 30, Epoch: 80, Loss: 6.799801485612988e-05, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 29, 3: 1}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 2026, Episode: 31, Epoch: 0, Loss: 0.0029367830138653517, Mean Reward: 1.9375, Actions in batch: {0: 10, 1: 7, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 2046, Episode: 31, Epoch: 20, Loss: 0.011511488817632198, Mean Reward: 0.15625, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 2066, Episode: 31, Epoch: 40, Loss: 0.006777310743927956, Mean Reward: 0.59375, Actions in batch: {0: 7, 1: 9, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 2086, Episode: 31, Epoch: 60, Loss: 0.004618530161678791, Mean Reward: 0.625, Actions in batch: {0: 5, 1: 6, 2: 12, 3: 9}\n",
            "TOTAL EPOCH: 2106, Episode: 31, Epoch: 80, Loss: 3.877806375385262e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 30}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 2116, Episode: 32, Epoch: 0, Loss: 0.00024118393776006997, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 8, 3: 9}\n",
            "TOTAL EPOCH: 2136, Episode: 32, Epoch: 20, Loss: 0.0030445631127804518, Mean Reward: 1.65625, Actions in batch: {0: 9, 1: 7, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 2156, Episode: 32, Epoch: 40, Loss: 4.440291377250105e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 27, 3: 1}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2161, Episode: 33, Epoch: 0, Loss: 0.002713664434850216, Mean Reward: 0.75, Actions in batch: {0: 9, 1: 8, 2: 11, 3: 4}\n",
            "TOTAL EPOCH: 2181, Episode: 33, Epoch: 20, Loss: 0.004938408732414246, Mean Reward: 0.75, Actions in batch: {0: 9, 1: 2, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 2201, Episode: 33, Epoch: 40, Loss: 0.00036746938712894917, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 4, 2: 12, 3: 8}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2217, Episode: 34, Epoch: 0, Loss: 0.0059670046903193, Mean Reward: 1.59375, Actions in batch: {0: 6, 1: 10, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 2237, Episode: 34, Epoch: 20, Loss: 2.6716930733527988e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 2257, Episode: 34, Epoch: 40, Loss: 0.0019963178783655167, Mean Reward: 0.4375, Actions in batch: {0: 9, 1: 9, 2: 8, 3: 6}\n",
            "TOTAL EPOCH: 2277, Episode: 34, Epoch: 60, Loss: 0.002527140313759446, Mean Reward: 0.625, Actions in batch: {0: 8, 1: 12, 2: 7, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2284, Episode: 35, Epoch: 0, Loss: 0.0015733662294223905, Mean Reward: 2.65625, Actions in batch: {0: 9, 1: 6, 2: 11, 3: 6}\n",
            "TOTAL EPOCH: 2304, Episode: 35, Epoch: 20, Loss: 1.1504776921356097e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 2324, Episode: 35, Epoch: 40, Loss: 0.02862503007054329, Mean Reward: 0.46875, Actions in batch: {0: 7, 1: 5, 2: 10, 3: 10}\n",
            "TOTAL EPOCH: 2344, Episode: 35, Epoch: 60, Loss: 0.0029972807969897985, Mean Reward: 0.75, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2345, Episode: 36, Epoch: 0, Loss: 5.1656770665431395e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 2365, Episode: 36, Epoch: 20, Loss: 0.005249632988125086, Mean Reward: 1.15625, Actions in batch: {0: 5, 1: 8, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 2385, Episode: 36, Epoch: 40, Loss: 0.0008753573638387024, Mean Reward: 0.875, Actions in batch: {0: 7, 1: 9, 2: 9, 3: 7}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2402, Episode: 37, Epoch: 0, Loss: 1.1514991456351709e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 6, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 2422, Episode: 37, Epoch: 20, Loss: 0.0014895758358761668, Mean Reward: 2.03125, Actions in batch: {0: 8, 1: 8, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 2442, Episode: 37, Epoch: 40, Loss: 0.0009208995033986866, Mean Reward: 0.0, Actions in batch: {0: 11, 1: 6, 2: 9, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2455, Episode: 38, Epoch: 0, Loss: 0.00020322937052696943, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 2475, Episode: 38, Epoch: 20, Loss: 5.8904352044919506e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 2495, Episode: 38, Epoch: 40, Loss: 0.008744942024350166, Mean Reward: 0.375, Actions in batch: {0: 1, 2: 29, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2515, Episode: 39, Epoch: 0, Loss: 0.0042580496519804, Mean Reward: 1.5, Actions in batch: {0: 3, 1: 2, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 2535, Episode: 39, Epoch: 20, Loss: 0.0003647232661023736, Mean Reward: 2.0, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 2555, Episode: 39, Epoch: 40, Loss: 0.002287867944687605, Mean Reward: 4.5, Actions in batch: {0: 12, 1: 6, 2: 11, 3: 3}\n",
            "TOTAL EPOCH: 2575, Episode: 39, Epoch: 60, Loss: 0.0010977236088365316, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2581, Episode: 40, Epoch: 0, Loss: 0.00035424638190306723, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 7, 2: 22, 3: 1}\n",
            "TOTAL EPOCH: 2601, Episode: 40, Epoch: 20, Loss: 0.0006228704587556422, Mean Reward: 2.53125, Actions in batch: {0: 9, 1: 8, 2: 10, 3: 5}\n",
            "TOTAL EPOCH: 2621, Episode: 40, Epoch: 40, Loss: 0.002693086164072156, Mean Reward: 1.15625, Actions in batch: {0: 7, 1: 3, 2: 14, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2635, Episode: 41, Epoch: 0, Loss: 0.0005720378248952329, Mean Reward: 0.0, Actions in batch: {1: 4, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 2655, Episode: 41, Epoch: 20, Loss: 0.0032563982531428337, Mean Reward: 1.59375, Actions in batch: {0: 7, 1: 4, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 2675, Episode: 41, Epoch: 40, Loss: 0.004946272820234299, Mean Reward: 0.78125, Actions in batch: {0: 7, 1: 6, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 2695, Episode: 41, Epoch: 60, Loss: 0.0008760966593399644, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 5, 2: 13, 3: 5}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2696, Episode: 42, Epoch: 0, Loss: 0.01772773638367653, Mean Reward: 1.34375, Actions in batch: {0: 7, 1: 10, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 2716, Episode: 42, Epoch: 20, Loss: 0.003958162851631641, Mean Reward: 1.9375, Actions in batch: {0: 4, 1: 8, 2: 9, 3: 11}\n",
            "TOTAL EPOCH: 2736, Episode: 42, Epoch: 40, Loss: 0.024699583649635315, Mean Reward: 0.09375, Actions in batch: {0: 12, 1: 7, 2: 6, 3: 7}\n",
            "TOTAL EPOCH: 2756, Episode: 42, Epoch: 60, Loss: 0.0021877989638596773, Mean Reward: 1.75, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2761, Episode: 43, Epoch: 0, Loss: 0.020324094220995903, Mean Reward: 0.15625, Actions in batch: {0: 8, 1: 2, 2: 15, 3: 7}\n",
            "TOTAL EPOCH: 2781, Episode: 43, Epoch: 20, Loss: 0.0012716037454083562, Mean Reward: 0.8125, Actions in batch: {0: 8, 1: 6, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 2801, Episode: 43, Epoch: 40, Loss: 0.0044212546199560165, Mean Reward: 0.75, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2809, Episode: 44, Epoch: 0, Loss: 0.00014214422844816, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 2829, Episode: 44, Epoch: 20, Loss: 1.695783794275485e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 7, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 2849, Episode: 44, Epoch: 40, Loss: 4.581571829476161e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 2869, Episode: 44, Epoch: 60, Loss: 0.00014896661741659045, Mean Reward: 2.0, Actions in batch: {0: 3, 1: 6, 2: 11, 3: 12}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2874, Episode: 45, Epoch: 0, Loss: 0.004365643486380577, Mean Reward: 0.53125, Actions in batch: {0: 5, 1: 7, 2: 11, 3: 9}\n",
            "TOTAL EPOCH: 2894, Episode: 45, Epoch: 20, Loss: 0.0006642818916589022, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 8, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 2914, Episode: 45, Epoch: 40, Loss: 0.0034460495226085186, Mean Reward: 1.25, Actions in batch: {0: 4, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 2934, Episode: 45, Epoch: 60, Loss: 0.0031819429714232683, Mean Reward: 1.46875, Actions in batch: {0: 6, 1: 5, 2: 14, 3: 7}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 2942, Episode: 46, Epoch: 0, Loss: 0.0010735723190009594, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 4, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 2962, Episode: 46, Epoch: 20, Loss: 0.005610259249806404, Mean Reward: 0.21875, Actions in batch: {0: 10, 1: 6, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 2982, Episode: 46, Epoch: 40, Loss: 0.0002705038932617754, Mean Reward: 2.625, Actions in batch: {0: 8, 1: 5, 2: 14, 3: 5}\n",
            "TOTAL EPOCH: 3002, Episode: 46, Epoch: 60, Loss: 0.0012588956160470843, Mean Reward: 2.65625, Actions in batch: {0: 4, 1: 9, 2: 11, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3005, Episode: 47, Epoch: 0, Loss: 0.000178611371666193, Mean Reward: 1.5, Actions in batch: {0: 5, 1: 5, 2: 12, 3: 10}\n",
            "TOTAL EPOCH: 3025, Episode: 47, Epoch: 20, Loss: 0.00047063123201951385, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 1, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 3045, Episode: 47, Epoch: 40, Loss: 0.0012624711962416768, Mean Reward: 0.78125, Actions in batch: {0: 13, 1: 9, 2: 5, 3: 5}\n",
            "TOTAL EPOCH: 3065, Episode: 47, Epoch: 60, Loss: 0.001589872408658266, Mean Reward: 1.0, Actions in batch: {0: 6, 1: 5, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 3085, Episode: 47, Epoch: 80, Loss: 0.008042537607252598, Mean Reward: 0.4375, Actions in batch: {0: 7, 1: 9, 2: 9, 3: 7}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 3089, Episode: 48, Epoch: 0, Loss: 0.0004417444870341569, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 30, 3: 1}\n",
            "TOTAL EPOCH: 3109, Episode: 48, Epoch: 20, Loss: 0.000576141697820276, Mean Reward: 2.375, Actions in batch: {0: 5, 1: 8, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 3129, Episode: 48, Epoch: 40, Loss: 2.4092676540021785e-05, Mean Reward: 0.0, Actions in batch: {0: 12, 1: 4, 2: 6, 3: 10}\n",
            "TOTAL EPOCH: 3149, Episode: 48, Epoch: 60, Loss: 0.0007643821882084012, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 28, 3: 2}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 3158, Episode: 49, Epoch: 0, Loss: 2.011005017266143e-05, Mean Reward: 2.0, Actions in batch: {0: 12, 1: 6, 2: 8, 3: 6}\n",
            "TOTAL EPOCH: 3178, Episode: 49, Epoch: 20, Loss: 0.004707001615315676, Mean Reward: 0.5625, Actions in batch: {1: 2, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 3198, Episode: 49, Epoch: 40, Loss: 0.004865236580371857, Mean Reward: 1.25, Actions in batch: {0: 9, 1: 6, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 3218, Episode: 49, Epoch: 60, Loss: 0.016929645091295242, Mean Reward: 0.125, Actions in batch: {0: 5, 1: 2, 2: 16, 3: 9}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 3228, Episode: 50, Epoch: 0, Loss: 0.004546420648694038, Mean Reward: 0.78125, Actions in batch: {0: 9, 1: 7, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 3248, Episode: 50, Epoch: 20, Loss: 1.7311500414507464e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 11, 3: 11}\n",
            "TOTAL EPOCH: 3268, Episode: 50, Epoch: 40, Loss: 0.0001235517702298239, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 11, 3: 10}\n",
            "TOTAL EPOCH: 3288, Episode: 50, Epoch: 60, Loss: 0.0023040533997118473, Mean Reward: 0.6875, Actions in batch: {0: 7, 1: 6, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 3308, Episode: 50, Epoch: 80, Loss: 0.003155273152515292, Mean Reward: 4.375, Actions in batch: {0: 9, 1: 6, 2: 6, 3: 11}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 3313, Episode: 51, Epoch: 0, Loss: 0.007561711128801107, Mean Reward: 0.375, Actions in batch: {0: 8, 1: 8, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 3333, Episode: 51, Epoch: 20, Loss: 0.0037928083911538124, Mean Reward: 1.09375, Actions in batch: {0: 6, 1: 11, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 3353, Episode: 51, Epoch: 40, Loss: 0.004442618694156408, Mean Reward: 1.34375, Actions in batch: {0: 8, 1: 8, 2: 9, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3357, Episode: 52, Epoch: 0, Loss: 0.006545356009155512, Mean Reward: 0.3125, Actions in batch: {0: 7, 1: 6, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 3377, Episode: 52, Epoch: 20, Loss: 0.004767633508890867, Mean Reward: 1.5, Actions in batch: {0: 4, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 3397, Episode: 52, Epoch: 40, Loss: 0.00894723366945982, Mean Reward: 0.375, Actions in batch: {0: 4, 1: 7, 2: 14, 3: 7}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 3409, Episode: 53, Epoch: 0, Loss: 0.00016836791473906487, Mean Reward: 0.0, Actions in batch: {0: 13, 1: 6, 2: 6, 3: 7}\n",
            "TOTAL EPOCH: 3429, Episode: 53, Epoch: 20, Loss: 6.139796460047364e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 3449, Episode: 53, Epoch: 40, Loss: 0.003564624348655343, Mean Reward: 0.4375, Actions in batch: {0: 8, 1: 10, 2: 4, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3468, Episode: 54, Epoch: 0, Loss: 0.005712147336453199, Mean Reward: 0.625, Actions in batch: {0: 5, 1: 6, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 3488, Episode: 54, Epoch: 20, Loss: 1.4151707546261605e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 11, 3: 11}\n",
            "TOTAL EPOCH: 3508, Episode: 54, Epoch: 40, Loss: 0.0008150490466505289, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 2, 2: 28}\n",
            "TOTAL EPOCH: 3528, Episode: 54, Epoch: 60, Loss: 0.0036597452126443386, Mean Reward: 1.09375, Actions in batch: {0: 10, 1: 6, 2: 10, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3533, Episode: 55, Epoch: 0, Loss: 0.0008860945235937834, Mean Reward: 1.25, Actions in batch: {0: 11, 1: 5, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 3553, Episode: 55, Epoch: 20, Loss: 0.0004225064185447991, Mean Reward: 0.5625, Actions in batch: {0: 6, 1: 7, 2: 14, 3: 5}\n",
            "TOTAL EPOCH: 3573, Episode: 55, Epoch: 40, Loss: 9.857278200797737e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 3584, Episode: 56, Epoch: 0, Loss: 0.00027436227537691593, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 11, 2: 10, 3: 5}\n",
            "TOTAL EPOCH: 3604, Episode: 56, Epoch: 20, Loss: 0.003825629595667124, Mean Reward: 2.1875, Actions in batch: {0: 10, 1: 6, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 3624, Episode: 56, Epoch: 40, Loss: 9.011873771669343e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 21, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3644, Episode: 57, Epoch: 0, Loss: 0.0014489056775346398, Mean Reward: 2.53125, Actions in batch: {0: 5, 1: 3, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 3664, Episode: 57, Epoch: 20, Loss: 0.00040436963899992406, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 3684, Episode: 57, Epoch: 40, Loss: 0.00011419992370065302, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 3704, Episode: 57, Epoch: 60, Loss: 1.92388233699603e-05, Mean Reward: 1.0, Actions in batch: {0: 11, 1: 5, 2: 13, 3: 3}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 3719, Episode: 58, Epoch: 0, Loss: 8.167673513526097e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 3739, Episode: 58, Epoch: 20, Loss: 0.008611714467406273, Mean Reward: 1.59375, Actions in batch: {0: 6, 1: 6, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 3759, Episode: 58, Epoch: 40, Loss: 0.0016074051382020116, Mean Reward: 4.0625, Actions in batch: {0: 8, 1: 5, 2: 12, 3: 7}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 3768, Episode: 59, Epoch: 0, Loss: 9.417530964128673e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 3, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 3788, Episode: 59, Epoch: 20, Loss: 0.00030185875948518515, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 3808, Episode: 59, Epoch: 40, Loss: 0.003232013899832964, Mean Reward: 1.5, Actions in batch: {0: 10, 1: 6, 2: 10, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3816, Episode: 60, Epoch: 0, Loss: 0.017026271671056747, Mean Reward: 0.0625, Actions in batch: {0: 7, 1: 6, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 3836, Episode: 60, Epoch: 20, Loss: 0.0013500466011464596, Mean Reward: 0.875, Actions in batch: {0: 4, 1: 6, 2: 11, 3: 11}\n",
            "TOTAL EPOCH: 3856, Episode: 60, Epoch: 40, Loss: 2.8800694053643383e-05, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 1, 2: 9, 3: 12}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3857, Episode: 61, Epoch: 0, Loss: 9.05400429473957e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 3877, Episode: 61, Epoch: 20, Loss: 0.00328554748557508, Mean Reward: 1.78125, Actions in batch: {0: 5, 1: 11, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 3897, Episode: 61, Epoch: 40, Loss: 0.0047548431903123856, Mean Reward: 2.4375, Actions in batch: {0: 5, 1: 12, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 3917, Episode: 61, Epoch: 60, Loss: 0.000397343683289364, Mean Reward: 1.78125, Actions in batch: {0: 10, 1: 6, 2: 8, 3: 8}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 3929, Episode: 62, Epoch: 0, Loss: 0.0003971275291405618, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 7, 2: 14, 3: 9}\n",
            "TOTAL EPOCH: 3949, Episode: 62, Epoch: 20, Loss: 5.062881427875254e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 3969, Episode: 62, Epoch: 40, Loss: 0.004012022167444229, Mean Reward: 1.53125, Actions in batch: {0: 8, 1: 10, 2: 9, 3: 5}\n",
            "TOTAL EPOCH: 3989, Episode: 62, Epoch: 60, Loss: 0.0004941998049616814, Mean Reward: 0.6875, Actions in batch: {0: 7, 1: 6, 2: 15, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 4007, Episode: 63, Epoch: 0, Loss: 0.0028320045676082373, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 4027, Episode: 63, Epoch: 20, Loss: 0.0030956906266510487, Mean Reward: 1.1875, Actions in batch: {0: 6, 1: 13, 2: 10, 3: 3}\n",
            "TOTAL EPOCH: 4047, Episode: 63, Epoch: 40, Loss: 0.012661446817219257, Mean Reward: 0.09375, Actions in batch: {0: 13, 1: 5, 2: 10, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4049, Episode: 64, Epoch: 0, Loss: 0.002455837558954954, Mean Reward: 2.03125, Actions in batch: {0: 9, 1: 10, 2: 7, 3: 6}\n",
            "TOTAL EPOCH: 4069, Episode: 64, Epoch: 20, Loss: 0.0021978074219077826, Mean Reward: 3.4375, Actions in batch: {0: 11, 1: 4, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 4089, Episode: 64, Epoch: 40, Loss: 0.0007390674436464906, Mean Reward: 1.0, Actions in batch: {0: 11, 1: 5, 2: 10, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4105, Episode: 65, Epoch: 0, Loss: 0.0008473351481370628, Mean Reward: 2.0625, Actions in batch: {0: 8, 1: 9, 2: 6, 3: 9}\n",
            "TOTAL EPOCH: 4125, Episode: 65, Epoch: 20, Loss: 0.0013707746984437108, Mean Reward: 1.0, Actions in batch: {0: 9, 1: 7, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 4145, Episode: 65, Epoch: 40, Loss: 3.0267563488450833e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 14, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4157, Episode: 66, Epoch: 0, Loss: 1.1869024092447944e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 7, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 4177, Episode: 66, Epoch: 20, Loss: 4.10264874517452e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 10, 2: 14, 3: 6}\n",
            "TOTAL EPOCH: 4197, Episode: 66, Epoch: 40, Loss: 0.000144317775266245, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 17, 3: 8}\n",
            "TOTAL EPOCH: 4217, Episode: 66, Epoch: 60, Loss: 0.002362598665058613, Mean Reward: 1.21875, Actions in batch: {0: 5, 1: 11, 2: 10, 3: 6}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 4231, Episode: 67, Epoch: 0, Loss: 0.0032272415701299906, Mean Reward: 0.84375, Actions in batch: {0: 9, 1: 7, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 4251, Episode: 67, Epoch: 20, Loss: 0.00035347449011169374, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 7, 2: 6, 3: 10}\n",
            "TOTAL EPOCH: 4271, Episode: 67, Epoch: 40, Loss: 0.0020999389234930277, Mean Reward: 3.46875, Actions in batch: {1: 4, 2: 28}\n",
            "TOTAL EPOCH: 4291, Episode: 67, Epoch: 60, Loss: 0.00016091507859528065, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 14, 3: 9}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4295, Episode: 68, Epoch: 0, Loss: 0.000151664309669286, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 10, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 4315, Episode: 68, Epoch: 20, Loss: 0.00369896343909204, Mean Reward: 1.34375, Actions in batch: {0: 4, 1: 7, 2: 11, 3: 10}\n",
            "TOTAL EPOCH: 4335, Episode: 68, Epoch: 40, Loss: 0.005273962393403053, Mean Reward: 1.78125, Actions in batch: {0: 4, 1: 9, 2: 9, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4347, Episode: 69, Epoch: 0, Loss: 0.007318159565329552, Mean Reward: 0.28125, Actions in batch: {0: 8, 1: 12, 2: 9, 3: 3}\n",
            "TOTAL EPOCH: 4367, Episode: 69, Epoch: 20, Loss: 0.0007536693010479212, Mean Reward: 1.875, Actions in batch: {0: 1, 1: 4, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 4387, Episode: 69, Epoch: 40, Loss: 3.441059016040526e-05, Mean Reward: 0.0, Actions in batch: {0: 11, 1: 4, 2: 10, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4394, Episode: 70, Epoch: 0, Loss: 1.1360565622453578e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 3, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 4414, Episode: 70, Epoch: 20, Loss: 7.075870144035434e-06, Mean Reward: 0.78125, Actions in batch: {0: 7, 1: 9, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 4434, Episode: 70, Epoch: 40, Loss: 0.0006991430418565869, Mean Reward: 1.0, Actions in batch: {0: 8, 1: 5, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 4454, Episode: 70, Epoch: 60, Loss: 0.005456370767205954, Mean Reward: 0.21875, Actions in batch: {0: 7, 1: 8, 2: 9, 3: 8}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 4464, Episode: 71, Epoch: 0, Loss: 0.00010323472088202834, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 10, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 4484, Episode: 71, Epoch: 20, Loss: 0.0005256384611129761, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 5, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 4504, Episode: 71, Epoch: 40, Loss: 0.0033860402181744576, Mean Reward: 1.0, Actions in batch: {0: 7, 1: 12, 2: 8, 3: 5}\n",
            "TOTAL EPOCH: 4524, Episode: 71, Epoch: 60, Loss: 0.00013964061508886516, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 8, 2: 9, 3: 5}\n",
            "TOTAL EPOCH: 4544, Episode: 71, Epoch: 80, Loss: 0.019320381805300713, Mean Reward: 0.15625, Actions in batch: {0: 8, 1: 2, 2: 15, 3: 7}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 4551, Episode: 72, Epoch: 0, Loss: 5.7663848565425724e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 4571, Episode: 72, Epoch: 20, Loss: 0.002952955197542906, Mean Reward: 0.28125, Actions in batch: {0: 4, 1: 8, 2: 15, 3: 5}\n",
            "TOTAL EPOCH: 4591, Episode: 72, Epoch: 40, Loss: 0.001563960569910705, Mean Reward: 1.6875, Actions in batch: {0: 3, 1: 10, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 4611, Episode: 72, Epoch: 60, Loss: 0.010257058776915073, Mean Reward: 0.46875, Actions in batch: {0: 5, 1: 4, 2: 15, 3: 8}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 4631, Episode: 73, Epoch: 0, Loss: 0.00046109704999253154, Mean Reward: 0.59375, Actions in batch: {0: 4, 1: 7, 2: 9, 3: 12}\n",
            "TOTAL EPOCH: 4651, Episode: 73, Epoch: 20, Loss: 0.005545800551772118, Mean Reward: 1.34375, Actions in batch: {0: 9, 1: 8, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 4671, Episode: 73, Epoch: 40, Loss: 0.00252781854942441, Mean Reward: 0.46875, Actions in batch: {0: 5, 1: 13, 2: 8, 3: 6}\n",
            "TOTAL EPOCH: 4691, Episode: 73, Epoch: 60, Loss: 0.009616782888770103, Mean Reward: 1.5625, Actions in batch: {0: 5, 1: 8, 2: 13, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4693, Episode: 74, Epoch: 0, Loss: 0.015277611091732979, Mean Reward: 0.0625, Actions in batch: {0: 9, 1: 10, 2: 8, 3: 5}\n",
            "TOTAL EPOCH: 4713, Episode: 74, Epoch: 20, Loss: 5.501695704879239e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 13, 3: 3}\n",
            "TOTAL EPOCH: 4733, Episode: 74, Epoch: 40, Loss: 8.475715003442019e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 4, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 4753, Episode: 74, Epoch: 60, Loss: 0.01549012865871191, Mean Reward: 0.0625, Actions in batch: {0: 9, 1: 5, 2: 9, 3: 9}\n",
            "TOTAL EPOCH: 4773, Episode: 74, Epoch: 80, Loss: 0.0014673754340037704, Mean Reward: 0.625, Actions in batch: {0: 7, 1: 12, 2: 6, 3: 7}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 4780, Episode: 75, Epoch: 0, Loss: 0.003408910008147359, Mean Reward: 0.90625, Actions in batch: {0: 6, 1: 5, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 4800, Episode: 75, Epoch: 20, Loss: 0.003549081040546298, Mean Reward: 1.40625, Actions in batch: {0: 7, 1: 4, 2: 17, 3: 4}\n",
            "TOTAL EPOCH: 4820, Episode: 75, Epoch: 40, Loss: 0.001343592768535018, Mean Reward: 0.75, Actions in batch: {0: 9, 1: 4, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 4840, Episode: 75, Epoch: 60, Loss: 4.591117431118619e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 24, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4855, Episode: 76, Epoch: 0, Loss: 0.0007499572238884866, Mean Reward: 1.0, Actions in batch: {0: 11, 1: 4, 2: 13, 3: 4}\n",
            "TOTAL EPOCH: 4875, Episode: 76, Epoch: 20, Loss: 0.009530073963105679, Mean Reward: 0.25, Actions in batch: {0: 5, 1: 4, 2: 12, 3: 11}\n",
            "TOTAL EPOCH: 4895, Episode: 76, Epoch: 40, Loss: 0.0013062763027846813, Mean Reward: 0.75, Actions in batch: {0: 10, 1: 7, 2: 6, 3: 9}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4902, Episode: 77, Epoch: 0, Loss: 0.010365812107920647, Mean Reward: 0.125, Actions in batch: {0: 7, 1: 7, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 4922, Episode: 77, Epoch: 20, Loss: 1.7755273802322336e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 7, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 4942, Episode: 77, Epoch: 40, Loss: 0.0011495666112750769, Mean Reward: 3.71875, Actions in batch: {0: 8, 1: 7, 2: 11, 3: 6}\n",
            "TOTAL EPOCH: 4962, Episode: 77, Epoch: 60, Loss: 4.25346806878224e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 4, 2: 14, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4973, Episode: 78, Epoch: 0, Loss: 0.0056067220866680145, Mean Reward: 0.375, Actions in batch: {0: 7, 1: 11, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 4993, Episode: 78, Epoch: 20, Loss: 0.0022700950503349304, Mean Reward: 0.5, Actions in batch: {0: 9, 1: 9, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 5013, Episode: 78, Epoch: 40, Loss: 3.7851965316804126e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 9, 3: 9}\n",
            "TOTAL EPOCH: 5033, Episode: 78, Epoch: 60, Loss: 0.0023909565061330795, Mean Reward: 1.6875, Actions in batch: {0: 5, 1: 6, 2: 12, 3: 9}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 5052, Episode: 79, Epoch: 0, Loss: 0.002342707011848688, Mean Reward: 1.09375, Actions in batch: {0: 4, 1: 4, 2: 15, 3: 9}\n",
            "TOTAL EPOCH: 5072, Episode: 79, Epoch: 20, Loss: 2.508579200366512e-06, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 9, 2: 11, 3: 5}\n",
            "TOTAL EPOCH: 5092, Episode: 79, Epoch: 40, Loss: 0.0004338220169302076, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 2, 2: 16, 3: 7}\n",
            "TOTAL EPOCH: 5112, Episode: 79, Epoch: 60, Loss: 0.012131919153034687, Mean Reward: 0.1875, Actions in batch: {0: 3, 1: 12, 2: 12, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5123, Episode: 80, Epoch: 0, Loss: 0.0013754002284258604, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 6, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 5143, Episode: 80, Epoch: 20, Loss: 4.850280220125569e-06, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 5163, Episode: 80, Epoch: 40, Loss: 2.058597601717338e-05, Mean Reward: 0.0, Actions in batch: {0: 13, 1: 1, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 5183, Episode: 80, Epoch: 60, Loss: 0.0009645199752412736, Mean Reward: 2.625, Actions in batch: {0: 4, 1: 10, 2: 10, 3: 8}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 5193, Episode: 81, Epoch: 0, Loss: 2.172005451939185e-06, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 9, 3: 9}\n",
            "TOTAL EPOCH: 5213, Episode: 81, Epoch: 20, Loss: 0.0022831340320408344, Mean Reward: 1.0, Actions in batch: {0: 10, 1: 8, 2: 5, 3: 9}\n",
            "TOTAL EPOCH: 5233, Episode: 81, Epoch: 40, Loss: 0.0003325288707856089, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 3, 2: 10, 3: 9}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 5240, Episode: 82, Epoch: 0, Loss: 0.009287675842642784, Mean Reward: 0.46875, Actions in batch: {0: 7, 1: 12, 2: 7, 3: 6}\n",
            "TOTAL EPOCH: 5260, Episode: 82, Epoch: 20, Loss: 0.00011234864359721541, Mean Reward: 0.6875, Actions in batch: {0: 6, 1: 5, 2: 19, 3: 2}\n",
            "TOTAL EPOCH: 5280, Episode: 82, Epoch: 40, Loss: 0.00549732893705368, Mean Reward: 2.9375, Actions in batch: {0: 7, 1: 7, 2: 11, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5289, Episode: 83, Epoch: 0, Loss: 0.00023079829406924546, Mean Reward: 0.84375, Actions in batch: {0: 7, 1: 9, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 5309, Episode: 83, Epoch: 20, Loss: 0.00016495422460138798, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 4, 2: 14, 3: 4}\n",
            "TOTAL EPOCH: 5329, Episode: 83, Epoch: 40, Loss: 9.411507562617771e-06, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 11, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5337, Episode: 84, Epoch: 0, Loss: 0.0015953528927639127, Mean Reward: 1.09375, Actions in batch: {0: 7, 1: 6, 2: 14, 3: 5}\n",
            "TOTAL EPOCH: 5357, Episode: 84, Epoch: 20, Loss: 0.0014611369697377086, Mean Reward: 1.0, Actions in batch: {0: 9, 1: 5, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 5377, Episode: 84, Epoch: 40, Loss: 1.544343467685394e-05, Mean Reward: 0.0, Actions in batch: {1: 1, 2: 28, 3: 3}\n",
            "TOTAL EPOCH: 5397, Episode: 84, Epoch: 60, Loss: 0.009269759990274906, Mean Reward: 0.21875, Actions in batch: {0: 6, 1: 7, 2: 14, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 5411, Episode: 85, Epoch: 0, Loss: 0.00241577485576272, Mean Reward: 0.46875, Actions in batch: {0: 6, 1: 5, 2: 9, 3: 12}\n",
            "TOTAL EPOCH: 5431, Episode: 85, Epoch: 20, Loss: 0.0001798832236090675, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 5451, Episode: 85, Epoch: 40, Loss: 0.004531620070338249, Mean Reward: 1.5625, Actions in batch: {0: 9, 1: 5, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 5471, Episode: 85, Epoch: 60, Loss: 0.00013023898645769805, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 23, 3: 6}\n",
            "TOTAL EPOCH: 5491, Episode: 85, Epoch: 80, Loss: 0.00015785251162014902, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 14, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 5511, Episode: 86, Epoch: 0, Loss: 0.0006098675075918436, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 5, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 5531, Episode: 86, Epoch: 20, Loss: 0.00194788898807019, Mean Reward: 0.5, Actions in batch: {0: 5, 1: 4, 2: 12, 3: 11}\n",
            "TOTAL EPOCH: 5551, Episode: 86, Epoch: 40, Loss: 0.0021763865370303392, Mean Reward: 0.65625, Actions in batch: {0: 10, 1: 7, 2: 9, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 5566, Episode: 87, Epoch: 0, Loss: 7.355751586146653e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 5586, Episode: 87, Epoch: 20, Loss: 0.001781540340743959, Mean Reward: 0.6875, Actions in batch: {0: 10, 1: 6, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 5606, Episode: 87, Epoch: 40, Loss: 0.0001675576640991494, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 5, 2: 8, 3: 10}\n",
            "TOTAL EPOCH: 5626, Episode: 87, Epoch: 60, Loss: 2.2549586446984904e-06, Mean Reward: 1.6875, Actions in batch: {0: 7, 1: 6, 2: 12, 3: 7}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 5638, Episode: 88, Epoch: 0, Loss: 0.0001372898113913834, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 3, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 5658, Episode: 88, Epoch: 20, Loss: 0.004081835970282555, Mean Reward: 0.96875, Actions in batch: {0: 3, 1: 9, 2: 8, 3: 12}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5675, Episode: 89, Epoch: 0, Loss: 0.005916934460401535, Mean Reward: 1.8125, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 5695, Episode: 89, Epoch: 20, Loss: 0.0001725885522319004, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 5, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 5715, Episode: 89, Epoch: 40, Loss: 0.004196641501039267, Mean Reward: 0.71875, Actions in batch: {0: 3, 1: 6, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 5735, Episode: 89, Epoch: 60, Loss: 0.0016999656800180674, Mean Reward: 2.90625, Actions in batch: {0: 7, 1: 9, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 5755, Episode: 89, Epoch: 80, Loss: 0.0053822314366698265, Mean Reward: 0.6875, Actions in batch: {0: 5, 1: 3, 2: 17, 3: 7}\n",
            "TOTAL EPOCH: 5775, Episode: 90, Epoch: 0, Loss: 0.0016802492318674922, Mean Reward: 0.40625, Actions in batch: {0: 7, 1: 5, 2: 8, 3: 12}\n",
            "TOTAL EPOCH: 5795, Episode: 90, Epoch: 20, Loss: 0.014458037912845612, Mean Reward: 0.0625, Actions in batch: {0: 6, 1: 8, 2: 14, 3: 4}\n",
            "TOTAL EPOCH: 5815, Episode: 90, Epoch: 40, Loss: 0.001981498673558235, Mean Reward: 1.40625, Actions in batch: {0: 11, 1: 7, 2: 9, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5817, Episode: 91, Epoch: 0, Loss: 0.00286834966391325, Mean Reward: 0.84375, Actions in batch: {0: 6, 1: 5, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 5837, Episode: 91, Epoch: 20, Loss: 9.6816525910981e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 7, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 5857, Episode: 91, Epoch: 40, Loss: 9.267488394471002e-07, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 3, 2: 10, 3: 11}\n",
            "TOTAL EPOCH: 5877, Episode: 91, Epoch: 60, Loss: 0.0024722472298890352, Mean Reward: 1.0, Actions in batch: {0: 6, 1: 8, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 5897, Episode: 91, Epoch: 80, Loss: 0.00022272829664871097, Mean Reward: 0.59375, Actions in batch: {0: 4, 1: 12, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 5917, Episode: 92, Epoch: 0, Loss: 0.002626792760565877, Mean Reward: 0.5, Actions in batch: {0: 7, 1: 12, 2: 9, 3: 4}\n",
            "TOTAL EPOCH: 5937, Episode: 92, Epoch: 20, Loss: 7.92650316725485e-05, Mean Reward: 4.0, Actions in batch: {0: 12, 1: 7, 2: 8, 3: 5}\n",
            "TOTAL EPOCH: 5957, Episode: 92, Epoch: 40, Loss: 0.002503551309928298, Mean Reward: 0.90625, Actions in batch: {0: 6, 1: 12, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 5977, Episode: 92, Epoch: 60, Loss: 0.01114193256944418, Mean Reward: 0.125, Actions in batch: {0: 5, 1: 9, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 5997, Episode: 92, Epoch: 80, Loss: 0.0018105950439348817, Mean Reward: 1.90625, Actions in batch: {0: 4, 1: 7, 2: 9, 3: 12}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 5998, Episode: 93, Epoch: 0, Loss: 0.00022946056560613215, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 7, 2: 13, 3: 9}\n",
            "TOTAL EPOCH: 6018, Episode: 93, Epoch: 20, Loss: 7.401206676149741e-05, Mean Reward: 1.625, Actions in batch: {0: 6, 1: 10, 2: 5, 3: 11}\n",
            "TOTAL EPOCH: 6038, Episode: 93, Epoch: 40, Loss: 0.0044381809420883656, Mean Reward: 0.6875, Actions in batch: {0: 5, 1: 4, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 6058, Episode: 93, Epoch: 60, Loss: 1.7239215594599955e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 11, 3: 11}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 6061, Episode: 94, Epoch: 0, Loss: 0.004433137364685535, Mean Reward: 1.15625, Actions in batch: {0: 6, 1: 6, 2: 14, 3: 6}\n",
            "TOTAL EPOCH: 6081, Episode: 94, Epoch: 20, Loss: 0.0014321423368528485, Mean Reward: 0.75, Actions in batch: {0: 6, 1: 12, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 6101, Episode: 94, Epoch: 40, Loss: 0.00028086150996387005, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 12, 3: 9}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 6114, Episode: 95, Epoch: 0, Loss: 0.002186435740441084, Mean Reward: 0.5, Actions in batch: {0: 5, 1: 9, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 6134, Episode: 95, Epoch: 20, Loss: 0.0015124398050829768, Mean Reward: 2.0, Actions in batch: {0: 5, 1: 4, 2: 12, 3: 11}\n",
            "TOTAL EPOCH: 6154, Episode: 95, Epoch: 40, Loss: 0.0019278849940747023, Mean Reward: 2.09375, Actions in batch: {0: 11, 1: 7, 2: 9, 3: 5}\n",
            "TOTAL EPOCH: 6174, Episode: 95, Epoch: 60, Loss: 0.00017728366947267205, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 14, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 6186, Episode: 96, Epoch: 0, Loss: 0.011575048789381981, Mean Reward: 0.1875, Actions in batch: {0: 4, 1: 7, 2: 10, 3: 11}\n",
            "TOTAL EPOCH: 6206, Episode: 96, Epoch: 20, Loss: 0.0029824567027390003, Mean Reward: 2.3125, Actions in batch: {0: 8, 1: 6, 2: 6, 3: 12}\n",
            "TOTAL EPOCH: 6226, Episode: 96, Epoch: 40, Loss: 0.0017805718816816807, Mean Reward: 0.65625, Actions in batch: {1: 6, 2: 20, 3: 6}\n",
            "TOTAL EPOCH: 6246, Episode: 96, Epoch: 60, Loss: 0.002171772997826338, Mean Reward: 1.125, Actions in batch: {0: 5, 1: 11, 2: 11, 3: 5}\n",
            "TOTAL EPOCH: 6266, Episode: 96, Epoch: 80, Loss: 0.004918615333735943, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 8, 2: 14, 3: 5}\n",
            "TOTAL EPOCH: 6286, Episode: 97, Epoch: 0, Loss: 0.002719390671700239, Mean Reward: 0.84375, Actions in batch: {0: 11, 1: 4, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 6306, Episode: 97, Epoch: 20, Loss: 0.0024539572186768055, Mean Reward: 0.625, Actions in batch: {0: 9, 1: 6, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 6326, Episode: 97, Epoch: 40, Loss: 0.00032846350222826004, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 15, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6342, Episode: 98, Epoch: 0, Loss: 1.8559262571216095e-06, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 2, 2: 17, 3: 5}\n",
            "TOTAL EPOCH: 6362, Episode: 98, Epoch: 20, Loss: 7.048860425129533e-05, Mean Reward: 1.625, Actions in batch: {0: 8, 1: 8, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 6382, Episode: 98, Epoch: 40, Loss: 0.012638311833143234, Mean Reward: 0.1875, Actions in batch: {0: 11, 1: 5, 2: 13, 3: 3}\n",
            "TOTAL EPOCH: 6402, Episode: 98, Epoch: 60, Loss: 0.000664839637465775, Mean Reward: 3.0, Actions in batch: {0: 4, 1: 7, 2: 12, 3: 9}\n",
            "TOTAL EPOCH: 6422, Episode: 98, Epoch: 80, Loss: 0.008625190705060959, Mean Reward: 1.6875, Actions in batch: {0: 5, 1: 10, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 6442, Episode: 99, Epoch: 0, Loss: 0.009619642049074173, Mean Reward: 0.78125, Actions in batch: {0: 5, 1: 10, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 6462, Episode: 99, Epoch: 20, Loss: 0.0033940784633159637, Mean Reward: 1.0625, Actions in batch: {0: 6, 1: 9, 2: 13, 3: 4}\n",
            "TOTAL EPOCH: 6482, Episode: 99, Epoch: 40, Loss: 0.002491668099537492, Mean Reward: 0.5625, Actions in batch: {0: 5, 1: 4, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 6502, Episode: 99, Epoch: 60, Loss: 0.0034623942337930202, Mean Reward: 1.09375, Actions in batch: {0: 10, 1: 6, 2: 10, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 6511, Episode: 100, Epoch: 0, Loss: 0.0005164922913536429, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 13, 3: 3}\n",
            "TOTAL EPOCH: 6531, Episode: 100, Epoch: 20, Loss: 0.0013759852154180408, Mean Reward: 3.40625, Actions in batch: {0: 5, 1: 13, 2: 7, 3: 7}\n",
            "TOTAL EPOCH: 6551, Episode: 100, Epoch: 40, Loss: 0.005481403321027756, Mean Reward: 1.28125, Actions in batch: {0: 6, 1: 4, 2: 14, 3: 8}\n",
            "TOTAL EPOCH: 6571, Episode: 100, Epoch: 60, Loss: 0.0023975258227437735, Mean Reward: 0.5, Actions in batch: {0: 8, 1: 5, 2: 12, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6573, Episode: 101, Epoch: 0, Loss: 0.0005375434993766248, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 6593, Episode: 101, Epoch: 20, Loss: 0.0025598725769668818, Mean Reward: 0.96875, Actions in batch: {0: 4, 1: 6, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 6613, Episode: 101, Epoch: 40, Loss: 2.748157930909656e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 12, 2: 7, 3: 5}\n",
            "TOTAL EPOCH: 6633, Episode: 101, Epoch: 60, Loss: 1.6242589708781452e-06, Mean Reward: 0.0, Actions in batch: {0: 12, 1: 9, 2: 2, 3: 9}\n",
            "TOTAL EPOCH: 6653, Episode: 101, Epoch: 80, Loss: 0.0015316646313294768, Mean Reward: 1.5625, Actions in batch: {0: 9, 1: 5, 2: 10, 3: 8}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 6669, Episode: 102, Epoch: 0, Loss: 0.0033710934221744537, Mean Reward: 1.5, Actions in batch: {0: 10, 1: 10, 2: 4, 3: 8}\n",
            "TOTAL EPOCH: 6689, Episode: 102, Epoch: 20, Loss: 3.5314321849000407e-06, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 2, 2: 18, 3: 3}\n",
            "TOTAL EPOCH: 6709, Episode: 102, Epoch: 40, Loss: 0.0032331771217286587, Mean Reward: 1.03125, Actions in batch: {0: 6, 1: 11, 2: 6, 3: 9}\n",
            "TOTAL EPOCH: 6729, Episode: 102, Epoch: 60, Loss: 0.0013473880244418979, Mean Reward: 2.21875, Actions in batch: {0: 8, 1: 11, 2: 10, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6737, Episode: 103, Epoch: 0, Loss: 8.430037269135937e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 8, 3: 11}\n",
            "TOTAL EPOCH: 6757, Episode: 103, Epoch: 20, Loss: 0.01969868689775467, Mean Reward: 0.09375, Actions in batch: {0: 9, 1: 10, 2: 4, 3: 9}\n",
            "TOTAL EPOCH: 6777, Episode: 103, Epoch: 40, Loss: 0.0032733245752751827, Mean Reward: 0.75, Actions in batch: {0: 9, 1: 7, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 6797, Episode: 103, Epoch: 60, Loss: 0.001468293834477663, Mean Reward: 1.90625, Actions in batch: {0: 6, 1: 8, 2: 14, 3: 4}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 6804, Episode: 104, Epoch: 0, Loss: 0.0017256868304684758, Mean Reward: 1.5, Actions in batch: {0: 7, 1: 7, 2: 10, 3: 8}\n",
            "TOTAL EPOCH: 6824, Episode: 104, Epoch: 20, Loss: 0.007156576961278915, Mean Reward: 1.6875, Actions in batch: {0: 5, 1: 10, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 6844, Episode: 104, Epoch: 40, Loss: 0.0015464696334674954, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 5, 2: 13, 3: 9}\n",
            "TOTAL EPOCH: 6864, Episode: 104, Epoch: 60, Loss: 5.233184128883295e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 4, 2: 9, 3: 10}\n",
            "TOTAL EPOCH: 6884, Episode: 104, Epoch: 80, Loss: 0.001072895945981145, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 6904, Episode: 105, Epoch: 0, Loss: 0.0008267629891633987, Mean Reward: 0.0, Actions in batch: {0: 11, 1: 8, 2: 7, 3: 6}\n",
            "TOTAL EPOCH: 6924, Episode: 105, Epoch: 20, Loss: 0.0020990893244743347, Mean Reward: 1.46875, Actions in batch: {0: 6, 1: 9, 2: 11, 3: 6}\n",
            "TOTAL EPOCH: 6944, Episode: 105, Epoch: 40, Loss: 0.004378566052764654, Mean Reward: 0.8125, Actions in batch: {0: 8, 1: 9, 2: 9, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6950, Episode: 106, Epoch: 0, Loss: 5.122814036440104e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 7, 2: 9, 3: 9}\n",
            "TOTAL EPOCH: 6970, Episode: 106, Epoch: 20, Loss: 0.0009092550026252866, Mean Reward: 1.96875, Actions in batch: {0: 3, 1: 8, 2: 10, 3: 11}\n",
            "TOTAL EPOCH: 6990, Episode: 106, Epoch: 40, Loss: 0.006540562957525253, Mean Reward: 0.28125, Actions in batch: {0: 4, 1: 7, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 7010, Episode: 106, Epoch: 60, Loss: 2.770819719444262e-06, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 7018, Episode: 107, Epoch: 0, Loss: 0.0013364898040890694, Mean Reward: 3.25, Actions in batch: {0: 9, 1: 8, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 7038, Episode: 107, Epoch: 20, Loss: 0.009946335107088089, Mean Reward: 0.21875, Actions in batch: {0: 11, 1: 6, 2: 6, 3: 9}\n",
            "TOTAL EPOCH: 7058, Episode: 107, Epoch: 40, Loss: 1.3499052329279948e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 6, 2: 15, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7060, Episode: 108, Epoch: 0, Loss: 0.00340173183940351, Mean Reward: 1.59375, Actions in batch: {0: 5, 1: 4, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 7080, Episode: 108, Epoch: 20, Loss: 0.0020805299282073975, Mean Reward: 3.4375, Actions in batch: {0: 14, 1: 5, 2: 6, 3: 7}\n",
            "TOTAL EPOCH: 7100, Episode: 108, Epoch: 40, Loss: 0.012980147264897823, Mean Reward: 0.1875, Actions in batch: {0: 7, 1: 8, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 7120, Episode: 108, Epoch: 60, Loss: 0.004062729887664318, Mean Reward: 0.4375, Actions in batch: {0: 4, 1: 8, 2: 14, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 7124, Episode: 109, Epoch: 0, Loss: 5.396809137891978e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 20, 3: 2}\n",
            "TOTAL EPOCH: 7144, Episode: 109, Epoch: 20, Loss: 0.0007727311458438635, Mean Reward: 2.84375, Actions in batch: {0: 8, 1: 8, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 7164, Episode: 109, Epoch: 40, Loss: 0.012061191722750664, Mean Reward: 0.375, Actions in batch: {0: 5, 1: 7, 2: 14, 3: 6}\n",
            "TOTAL EPOCH: 7184, Episode: 109, Epoch: 60, Loss: 0.000846583629027009, Mean Reward: 1.8125, Actions in batch: {0: 11, 1: 8, 2: 6, 3: 7}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 7193, Episode: 110, Epoch: 0, Loss: 0.00015579810133203864, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 8, 2: 9, 3: 9}\n",
            "TOTAL EPOCH: 7213, Episode: 110, Epoch: 20, Loss: 0.001289563369937241, Mean Reward: 1.65625, Actions in batch: {0: 10, 1: 6, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 7233, Episode: 110, Epoch: 40, Loss: 0.01610492542386055, Mean Reward: 0.0625, Actions in batch: {0: 6, 1: 8, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 7253, Episode: 110, Epoch: 60, Loss: 4.638943210011348e-06, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 4, 2: 16, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 7262, Episode: 111, Epoch: 0, Loss: 0.0012413165532052517, Mean Reward: 3.0, Actions in batch: {0: 8, 1: 8, 2: 11, 3: 5}\n",
            "TOTAL EPOCH: 7282, Episode: 111, Epoch: 20, Loss: 0.0040593221783638, Mean Reward: 1.0, Actions in batch: {0: 9, 1: 6, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 7302, Episode: 111, Epoch: 40, Loss: 0.00011726994853233919, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 5, 2: 8, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7318, Episode: 112, Epoch: 0, Loss: 0.0007618310628458858, Mean Reward: 3.03125, Actions in batch: {0: 5, 1: 8, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 7338, Episode: 112, Epoch: 20, Loss: 0.0014206210616976023, Mean Reward: 0.84375, Actions in batch: {0: 4, 1: 9, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 7358, Episode: 112, Epoch: 40, Loss: 0.006158660631626844, Mean Reward: 0.25, Actions in batch: {0: 6, 1: 6, 2: 10, 3: 10}\n",
            "TOTAL EPOCH: 7378, Episode: 112, Epoch: 60, Loss: 0.0006022154120728374, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 15, 3: 9}\n",
            "TOTAL EPOCH: 7398, Episode: 112, Epoch: 80, Loss: 0.0016876317095011473, Mean Reward: 1.0, Actions in batch: {0: 8, 1: 6, 2: 13, 3: 5}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 7409, Episode: 113, Epoch: 0, Loss: 0.0013271152274683118, Mean Reward: 0.90625, Actions in batch: {0: 4, 1: 10, 2: 10, 3: 8}\n",
            "TOTAL EPOCH: 7429, Episode: 113, Epoch: 20, Loss: 0.007426361087709665, Mean Reward: 0.4375, Actions in batch: {0: 7, 1: 9, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 7449, Episode: 113, Epoch: 40, Loss: 0.0032172093633562326, Mean Reward: 2.1875, Actions in batch: {0: 5, 1: 5, 2: 11, 3: 11}\n",
            "TOTAL EPOCH: 7469, Episode: 113, Epoch: 60, Loss: 0.002048584632575512, Mean Reward: 2.75, Actions in batch: {0: 3, 1: 8, 2: 19, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 7470, Episode: 114, Epoch: 0, Loss: 0.0007222804706543684, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 15, 3: 8}\n",
            "TOTAL EPOCH: 7490, Episode: 114, Epoch: 20, Loss: 3.455275509622879e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 6, 2: 8, 3: 12}\n",
            "TOTAL EPOCH: 7510, Episode: 114, Epoch: 40, Loss: 6.961139661143534e-06, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 8, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 7530, Episode: 114, Epoch: 60, Loss: 0.005125382915139198, Mean Reward: 1.46875, Actions in batch: {0: 2, 1: 3, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 7550, Episode: 114, Epoch: 80, Loss: 0.00048203178448602557, Mean Reward: 1.75, Actions in batch: {0: 9, 1: 8, 2: 7, 3: 8}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 7566, Episode: 115, Epoch: 0, Loss: 0.0029521025717258453, Mean Reward: 1.875, Actions in batch: {0: 6, 1: 7, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 7586, Episode: 115, Epoch: 20, Loss: 0.002410170389339328, Mean Reward: 2.875, Actions in batch: {0: 6, 1: 4, 2: 13, 3: 9}\n",
            "TOTAL EPOCH: 7606, Episode: 115, Epoch: 40, Loss: 0.0016098503256216645, Mean Reward: 1.0, Actions in batch: {0: 9, 1: 8, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 7626, Episode: 115, Epoch: 60, Loss: 0.0016257831593975425, Mean Reward: 3.78125, Actions in batch: {0: 11, 1: 5, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 7646, Episode: 115, Epoch: 80, Loss: 2.4813377876853338e-06, Mean Reward: 1.0, Actions in batch: {0: 11, 1: 4, 2: 13, 3: 4}\n",
            "Max reward for this episode:  5.0\n",
            "Saving model that went for  95  epochs with reward  5.0\n",
            "TOTAL EPOCH: 7662, Episode: 116, Epoch: 0, Loss: 0.007157033775001764, Mean Reward: 0.25, Actions in batch: {0: 5, 1: 4, 2: 11, 3: 12}\n",
            "TOTAL EPOCH: 7682, Episode: 116, Epoch: 20, Loss: 2.6782299755723216e-05, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 5, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 7702, Episode: 116, Epoch: 40, Loss: 5.517453973880038e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 8, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 7722, Episode: 116, Epoch: 60, Loss: 6.159394979476929e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7729, Episode: 117, Epoch: 0, Loss: 0.0013548231218010187, Mean Reward: 0.46875, Actions in batch: {0: 7, 1: 9, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 7749, Episode: 117, Epoch: 20, Loss: 0.00015001736755948514, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 9, 2: 7, 3: 6}\n",
            "TOTAL EPOCH: 7769, Episode: 117, Epoch: 40, Loss: 0.0005972727085463703, Mean Reward: 2.53125, Actions in batch: {0: 8, 1: 8, 2: 8, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7770, Episode: 118, Epoch: 0, Loss: 0.0006415476673282683, Mean Reward: 2.65625, Actions in batch: {0: 12, 1: 3, 2: 12, 3: 5}\n",
            "TOTAL EPOCH: 7790, Episode: 118, Epoch: 20, Loss: 0.007449738215655088, Mean Reward: 0.1875, Actions in batch: {0: 11, 1: 6, 2: 8, 3: 7}\n",
            "TOTAL EPOCH: 7810, Episode: 118, Epoch: 40, Loss: 6.979869795031846e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 5, 2: 10, 3: 10}\n",
            "TOTAL EPOCH: 7830, Episode: 118, Epoch: 60, Loss: 0.00025531506980769336, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 19, 3: 3}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 7849, Episode: 119, Epoch: 0, Loss: 0.010060788132250309, Mean Reward: 0.21875, Actions in batch: {0: 3, 1: 8, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 7869, Episode: 119, Epoch: 20, Loss: 0.0028018031734973192, Mean Reward: 1.5, Actions in batch: {0: 9, 1: 8, 2: 10, 3: 5}\n",
            "TOTAL EPOCH: 7889, Episode: 119, Epoch: 40, Loss: 0.0015630293637514114, Mean Reward: 0.625, Actions in batch: {0: 5, 1: 12, 2: 5, 3: 10}\n",
            "TOTAL EPOCH: 7909, Episode: 119, Epoch: 60, Loss: 0.0002920442202594131, Mean Reward: 4.0625, Actions in batch: {0: 8, 1: 5, 2: 12, 3: 7}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 7919, Episode: 120, Epoch: 0, Loss: 0.004667348228394985, Mean Reward: 1.8125, Actions in batch: {0: 6, 1: 6, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 7939, Episode: 120, Epoch: 20, Loss: 0.0031005688942968845, Mean Reward: 1.03125, Actions in batch: {0: 14, 1: 4, 2: 10, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7957, Episode: 121, Epoch: 0, Loss: 0.01430829893797636, Mean Reward: 0.3125, Actions in batch: {0: 5, 1: 9, 2: 11, 3: 7}\n",
            "TOTAL EPOCH: 7977, Episode: 121, Epoch: 20, Loss: 8.42396002553869e-06, Mean Reward: 2.0, Actions in batch: {0: 10, 1: 6, 2: 10, 3: 6}\n",
            "TOTAL EPOCH: 7997, Episode: 121, Epoch: 40, Loss: 0.0017848460702225566, Mean Reward: 0.6875, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8017, Episode: 122, Epoch: 0, Loss: 0.002942602150142193, Mean Reward: 2.65625, Actions in batch: {0: 4, 1: 11, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 8037, Episode: 122, Epoch: 20, Loss: 6.797374953748658e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 7, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 8057, Episode: 122, Epoch: 40, Loss: 2.8142270821263082e-05, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 9, 2: 7, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8077, Episode: 123, Epoch: 0, Loss: 0.00511438213288784, Mean Reward: 0.6875, Actions in batch: {0: 2, 1: 8, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 8097, Episode: 123, Epoch: 20, Loss: 1.077793785952963e-05, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 6, 2: 8, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8115, Episode: 124, Epoch: 0, Loss: 0.001881714561022818, Mean Reward: 0.6875, Actions in batch: {0: 6, 1: 7, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 8135, Episode: 124, Epoch: 20, Loss: 3.838768316200003e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 1, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 8155, Episode: 124, Epoch: 40, Loss: 0.0029140987899154425, Mean Reward: 0.5, Actions in batch: {0: 4, 1: 8, 2: 10, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8175, Episode: 125, Epoch: 0, Loss: 2.1365099200920667e-06, Mean Reward: 2.625, Actions in batch: {0: 4, 1: 8, 2: 18, 3: 2}\n",
            "TOTAL EPOCH: 8195, Episode: 125, Epoch: 20, Loss: 2.747599864960648e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 2, 2: 14, 3: 8}\n",
            "TOTAL EPOCH: 8215, Episode: 125, Epoch: 40, Loss: 2.9636472390848212e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 8235, Episode: 125, Epoch: 60, Loss: 0.0004328876966610551, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 11, 2: 7, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8254, Episode: 126, Epoch: 0, Loss: 2.5576686311978847e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 13, 3: 9}\n",
            "TOTAL EPOCH: 8274, Episode: 126, Epoch: 20, Loss: 0.0006100267637521029, Mean Reward: 2.71875, Actions in batch: {0: 9, 1: 5, 2: 11, 3: 7}\n",
            "TOTAL EPOCH: 8294, Episode: 126, Epoch: 40, Loss: 0.000786669785156846, Mean Reward: 2.5625, Actions in batch: {0: 4, 1: 9, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 8314, Episode: 126, Epoch: 60, Loss: 6.241387745831162e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 16, 3: 5}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 8328, Episode: 127, Epoch: 0, Loss: 0.016453243792057037, Mean Reward: 0.125, Actions in batch: {0: 3, 1: 8, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 8348, Episode: 127, Epoch: 20, Loss: 0.0003730412863660604, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 8368, Episode: 127, Epoch: 40, Loss: 0.01480356976389885, Mean Reward: 0.15625, Actions in batch: {0: 8, 1: 10, 2: 11, 3: 3}\n",
            "TOTAL EPOCH: 8388, Episode: 127, Epoch: 60, Loss: 0.003722393186762929, Mean Reward: 0.8125, Actions in batch: {0: 3, 1: 10, 2: 7, 3: 12}\n",
            "TOTAL EPOCH: 8408, Episode: 127, Epoch: 80, Loss: 0.006911260541528463, Mean Reward: 0.375, Actions in batch: {0: 4, 1: 7, 2: 14, 3: 7}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 8418, Episode: 128, Epoch: 0, Loss: 0.0018686384428292513, Mean Reward: 1.625, Actions in batch: {0: 6, 1: 10, 2: 5, 3: 11}\n",
            "TOTAL EPOCH: 8438, Episode: 128, Epoch: 20, Loss: 0.0018044828902930021, Mean Reward: 3.375, Actions in batch: {0: 5, 1: 12, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 8458, Episode: 128, Epoch: 40, Loss: 1.5273163853635197e-06, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 15, 3: 4}\n",
            "TOTAL EPOCH: 8478, Episode: 128, Epoch: 60, Loss: 0.005464160814881325, Mean Reward: 0.6875, Actions in batch: {0: 2, 1: 8, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 8498, Episode: 128, Epoch: 80, Loss: 0.0002765785320661962, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 4, 2: 10, 3: 8}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 8511, Episode: 129, Epoch: 0, Loss: 0.0004441746277734637, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 15, 3: 4}\n",
            "TOTAL EPOCH: 8531, Episode: 129, Epoch: 20, Loss: 0.0015204313676804304, Mean Reward: 2.5625, Actions in batch: {0: 2, 1: 8, 2: 21, 3: 1}\n",
            "TOTAL EPOCH: 8551, Episode: 129, Epoch: 40, Loss: 0.0014154653763398528, Mean Reward: 3.0, Actions in batch: {0: 8, 1: 5, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 8571, Episode: 129, Epoch: 60, Loss: 0.0002214124979218468, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 16, 3: 3}\n",
            "TOTAL EPOCH: 8591, Episode: 129, Epoch: 80, Loss: 0.0041662659496068954, Mean Reward: 1.125, Actions in batch: {0: 8, 1: 9, 2: 9, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 8598, Episode: 130, Epoch: 0, Loss: 0.0026318540330976248, Mean Reward: 1.71875, Actions in batch: {0: 9, 1: 9, 2: 5, 3: 9}\n",
            "TOTAL EPOCH: 8618, Episode: 130, Epoch: 20, Loss: 0.010100826621055603, Mean Reward: 0.25, Actions in batch: {0: 1, 1: 7, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 8638, Episode: 130, Epoch: 40, Loss: 0.001149710500612855, Mean Reward: 4.15625, Actions in batch: {0: 10, 1: 7, 2: 9, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8653, Episode: 131, Epoch: 0, Loss: 0.00013481610221788287, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 16, 3: 6}\n",
            "TOTAL EPOCH: 8673, Episode: 131, Epoch: 20, Loss: 0.009714812971651554, Mean Reward: 1.25, Actions in batch: {0: 7, 1: 5, 2: 15, 3: 5}\n",
            "TOTAL EPOCH: 8693, Episode: 131, Epoch: 40, Loss: 7.357064896496013e-05, Mean Reward: 1.9375, Actions in batch: {0: 6, 1: 11, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 8713, Episode: 131, Epoch: 60, Loss: 0.006258049514144659, Mean Reward: 1.0, Actions in batch: {0: 4, 1: 1, 2: 23, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 8715, Episode: 132, Epoch: 0, Loss: 0.0006885958136990666, Mean Reward: 1.0, Actions in batch: {0: 6, 1: 4, 2: 14, 3: 8}\n",
            "TOTAL EPOCH: 8735, Episode: 132, Epoch: 20, Loss: 0.006589473690837622, Mean Reward: 0.78125, Actions in batch: {0: 6, 1: 6, 2: 8, 3: 12}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 8751, Episode: 133, Epoch: 0, Loss: 0.005582732614129782, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 10, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 8771, Episode: 133, Epoch: 20, Loss: 7.874082803027704e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 3, 2: 17, 3: 4}\n",
            "TOTAL EPOCH: 8791, Episode: 133, Epoch: 40, Loss: 0.0006654844037257135, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 14, 3: 4}\n",
            "TOTAL EPOCH: 8811, Episode: 133, Epoch: 60, Loss: 0.010640843771398067, Mean Reward: 0.21875, Actions in batch: {0: 3, 1: 7, 2: 16, 3: 6}\n",
            "TOTAL EPOCH: 8831, Episode: 133, Epoch: 80, Loss: 0.0033165081404149532, Mean Reward: 0.34375, Actions in batch: {0: 7, 1: 10, 2: 10, 3: 5}\n",
            "TOTAL EPOCH: 8851, Episode: 134, Epoch: 0, Loss: 0.00014659311273135245, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 19, 3: 2}\n",
            "TOTAL EPOCH: 8871, Episode: 134, Epoch: 20, Loss: 2.7056878025177866e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 11, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 8891, Episode: 134, Epoch: 40, Loss: 0.0012898579007014632, Mean Reward: 3.375, Actions in batch: {0: 4, 1: 6, 2: 17, 3: 5}\n",
            "TOTAL EPOCH: 8911, Episode: 134, Epoch: 60, Loss: 0.00261381221935153, Mean Reward: 1.40625, Actions in batch: {0: 5, 1: 3, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 8931, Episode: 134, Epoch: 80, Loss: 0.006661153864115477, Mean Reward: 0.40625, Actions in batch: {0: 11, 1: 8, 2: 6, 3: 7}\n",
            "TOTAL EPOCH: 8951, Episode: 135, Epoch: 0, Loss: 0.000751661485992372, Mean Reward: 2.71875, Actions in batch: {0: 7, 1: 8, 2: 13, 3: 4}\n",
            "TOTAL EPOCH: 8971, Episode: 135, Epoch: 20, Loss: 0.00022520404309034348, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 8991, Episode: 135, Epoch: 40, Loss: 0.0006885339389555156, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 18, 3: 9}\n",
            "TOTAL EPOCH: 9011, Episode: 135, Epoch: 60, Loss: 0.001045667682774365, Mean Reward: 1.84375, Actions in batch: {0: 2, 1: 5, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 9031, Episode: 135, Epoch: 80, Loss: 3.847485641017556e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 9035, Episode: 136, Epoch: 0, Loss: 0.00365645345300436, Mean Reward: 0.9375, Actions in batch: {0: 7, 1: 8, 2: 14, 3: 3}\n",
            "TOTAL EPOCH: 9055, Episode: 136, Epoch: 20, Loss: 0.0007772741955704987, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 1, 2: 17, 3: 8}\n",
            "TOTAL EPOCH: 9075, Episode: 136, Epoch: 40, Loss: 0.0015086800558492541, Mean Reward: 2.59375, Actions in batch: {0: 5, 1: 7, 2: 12, 3: 8}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 9084, Episode: 137, Epoch: 0, Loss: 0.0015184663934633136, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 9104, Episode: 137, Epoch: 20, Loss: 0.00046838613343425095, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 4, 2: 10, 3: 11}\n",
            "TOTAL EPOCH: 9124, Episode: 137, Epoch: 40, Loss: 0.004007159266620874, Mean Reward: 0.46875, Actions in batch: {0: 4, 1: 9, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 9144, Episode: 137, Epoch: 60, Loss: 2.2908896426088177e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 3, 2: 15, 3: 7}\n",
            "TOTAL EPOCH: 9164, Episode: 137, Epoch: 80, Loss: 0.0047369347885251045, Mean Reward: 0.34375, Actions in batch: {0: 4, 1: 7, 2: 19, 3: 2}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 9170, Episode: 138, Epoch: 0, Loss: 1.853386675065849e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 7, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 9190, Episode: 138, Epoch: 20, Loss: 0.011917577125132084, Mean Reward: 1.09375, Actions in batch: {0: 3, 1: 8, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 9210, Episode: 138, Epoch: 40, Loss: 6.664810371148633e-06, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 7, 2: 9, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 9229, Episode: 139, Epoch: 0, Loss: 0.003274547168985009, Mean Reward: 1.1875, Actions in batch: {0: 8, 1: 5, 2: 17, 3: 2}\n",
            "TOTAL EPOCH: 9249, Episode: 139, Epoch: 20, Loss: 0.0051133097149431705, Mean Reward: 0.6875, Actions in batch: {0: 6, 1: 8, 2: 14, 3: 4}\n",
            "TOTAL EPOCH: 9269, Episode: 139, Epoch: 40, Loss: 0.0121812978759408, Mean Reward: 1.28125, Actions in batch: {0: 4, 1: 8, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 9289, Episode: 139, Epoch: 60, Loss: 0.001137599116191268, Mean Reward: 2.375, Actions in batch: {0: 4, 1: 9, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 9309, Episode: 139, Epoch: 80, Loss: 0.0027850402984768152, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 6, 2: 17, 3: 4}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 9325, Episode: 140, Epoch: 0, Loss: 0.003232336603105068, Mean Reward: 0.84375, Actions in batch: {0: 7, 1: 8, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 9345, Episode: 140, Epoch: 20, Loss: 0.009434111416339874, Mean Reward: 0.3125, Actions in batch: {0: 5, 1: 7, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 9365, Episode: 140, Epoch: 40, Loss: 0.009868187829852104, Mean Reward: 0.21875, Actions in batch: {0: 3, 1: 8, 2: 10, 3: 11}\n",
            "TOTAL EPOCH: 9385, Episode: 140, Epoch: 60, Loss: 6.157807092677103e-06, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 16, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 9402, Episode: 141, Epoch: 0, Loss: 0.013165242969989777, Mean Reward: 0.375, Actions in batch: {0: 7, 1: 6, 2: 16, 3: 3}\n",
            "TOTAL EPOCH: 9422, Episode: 141, Epoch: 20, Loss: 4.083920066477731e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 2, 2: 18, 3: 7}\n",
            "TOTAL EPOCH: 9442, Episode: 141, Epoch: 40, Loss: 0.0024945815093815327, Mean Reward: 1.15625, Actions in batch: {0: 9, 1: 9, 2: 6, 3: 8}\n",
            "TOTAL EPOCH: 9462, Episode: 141, Epoch: 60, Loss: 2.033640703302808e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 6, 2: 18, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 9480, Episode: 142, Epoch: 0, Loss: 0.0006524958298541605, Mean Reward: 1.9375, Actions in batch: {0: 2, 1: 3, 2: 20, 3: 7}\n",
            "TOTAL EPOCH: 9500, Episode: 142, Epoch: 20, Loss: 0.00201107794418931, Mean Reward: 1.4375, Actions in batch: {0: 10, 1: 9, 2: 5, 3: 8}\n",
            "TOTAL EPOCH: 9520, Episode: 142, Epoch: 40, Loss: 2.299818333995063e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 4, 2: 14, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 9531, Episode: 143, Epoch: 0, Loss: 8.145066385623068e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 2, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 9551, Episode: 143, Epoch: 20, Loss: 0.00010240884876111522, Mean Reward: 0.8125, Actions in batch: {0: 5, 1: 4, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 9571, Episode: 143, Epoch: 40, Loss: 0.002796570770442486, Mean Reward: 0.9375, Actions in batch: {0: 5, 1: 3, 2: 14, 3: 10}\n",
            "TOTAL EPOCH: 9591, Episode: 143, Epoch: 60, Loss: 2.171916548832087e-06, Mean Reward: 0.0, Actions in batch: {1: 6, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 9611, Episode: 143, Epoch: 80, Loss: 0.0004419224278535694, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 2, 2: 18, 3: 6}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 9612, Episode: 144, Epoch: 0, Loss: 0.0003151644195895642, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 14, 3: 8}\n",
            "TOTAL EPOCH: 9632, Episode: 144, Epoch: 20, Loss: 0.00017618376296013594, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 20, 3: 4}\n",
            "TOTAL EPOCH: 9652, Episode: 144, Epoch: 40, Loss: 0.005209129303693771, Mean Reward: 0.75, Actions in batch: {0: 10, 1: 7, 2: 12, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 9670, Episode: 145, Epoch: 0, Loss: 0.0020181292202323675, Mean Reward: 1.0, Actions in batch: {0: 3, 1: 1, 2: 23, 3: 5}\n",
            "TOTAL EPOCH: 9690, Episode: 145, Epoch: 20, Loss: 0.001104914816096425, Mean Reward: 2.6875, Actions in batch: {0: 6, 1: 11, 2: 12, 3: 3}\n",
            "TOTAL EPOCH: 9710, Episode: 145, Epoch: 40, Loss: 0.0014714411227032542, Mean Reward: 1.5625, Actions in batch: {0: 6, 1: 7, 2: 11, 3: 8}\n",
            "TOTAL EPOCH: 9730, Episode: 145, Epoch: 60, Loss: 0.0031573865562677383, Mean Reward: 1.59375, Actions in batch: {0: 4, 1: 9, 2: 14, 3: 5}\n",
            "TOTAL EPOCH: 9750, Episode: 145, Epoch: 80, Loss: 0.00038427242543548346, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 5, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 9770, Episode: 146, Epoch: 0, Loss: 0.015030419453978539, Mean Reward: 0.125, Actions in batch: {0: 6, 1: 8, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 9790, Episode: 146, Epoch: 20, Loss: 0.0016418315935879946, Mean Reward: 1.46875, Actions in batch: {0: 6, 1: 11, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 9810, Episode: 146, Epoch: 40, Loss: 0.031620293855667114, Mean Reward: 0.03125, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 9830, Episode: 146, Epoch: 60, Loss: 1.7506821677670814e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 9850, Episode: 146, Epoch: 80, Loss: 0.0022809847723692656, Mean Reward: 3.1875, Actions in batch: {0: 11, 1: 6, 2: 8, 3: 7}\n",
            "Max reward for this episode:  5.0\n",
            "TOTAL EPOCH: 9870, Episode: 147, Epoch: 0, Loss: 0.0025099783670157194, Mean Reward: 1.1875, Actions in batch: {0: 9, 1: 7, 2: 13, 3: 3}\n",
            "TOTAL EPOCH: 9890, Episode: 147, Epoch: 20, Loss: 0.0001524850376881659, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 8, 3: 9}\n",
            "TOTAL EPOCH: 9910, Episode: 147, Epoch: 40, Loss: 2.88210685539525e-05, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 3, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 9930, Episode: 147, Epoch: 60, Loss: 0.0007999222143553197, Mean Reward: 1.3125, Actions in batch: {0: 2, 1: 8, 2: 12, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 9939, Episode: 148, Epoch: 0, Loss: 0.00014853103493805975, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 9959, Episode: 148, Epoch: 20, Loss: 0.004965708125382662, Mean Reward: 0.375, Actions in batch: {0: 2, 1: 9, 2: 11, 3: 10}\n",
            "TOTAL EPOCH: 9979, Episode: 148, Epoch: 40, Loss: 4.0510254621040076e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 2, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 9999, Episode: 148, Epoch: 60, Loss: 0.00021451024804264307, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 11, 3: 7}\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 10019, Episode: 148, Epoch: 80, Loss: 0.00517554534599185, Mean Reward: 0.5, Actions in batch: {0: 5, 1: 9, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 10039, Episode: 149, Epoch: 0, Loss: 0.0028268664609640837, Mean Reward: 0.625, Actions in batch: {0: 9, 1: 7, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 10059, Episode: 149, Epoch: 20, Loss: 0.0002961919526569545, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 4, 2: 9, 3: 10}\n",
            "TOTAL EPOCH: 10079, Episode: 149, Epoch: 40, Loss: 3.1452827897737734e-07, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 8, 2: 14, 3: 4}\n",
            "TOTAL EPOCH: 10099, Episode: 149, Epoch: 60, Loss: 0.0003367275930941105, Mean Reward: 1.0, Actions in batch: {0: 6, 1: 4, 2: 14, 3: 8}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 10101, Episode: 150, Epoch: 0, Loss: 0.002073546638712287, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 10121, Episode: 150, Epoch: 20, Loss: 0.00042284265509806573, Mean Reward: 1.96875, Actions in batch: {0: 4, 1: 9, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 10141, Episode: 150, Epoch: 40, Loss: 0.0043425122275948524, Mean Reward: 2.625, Actions in batch: {0: 3, 1: 7, 2: 19, 3: 3}\n",
            "TOTAL EPOCH: 10161, Episode: 150, Epoch: 60, Loss: 1.2537488146335818e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 10181, Episode: 150, Epoch: 80, Loss: 0.002319947350770235, Mean Reward: 1.875, Actions in batch: {0: 8, 1: 7, 2: 12, 3: 5}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 10183, Episode: 151, Epoch: 0, Loss: 0.003289211308583617, Mean Reward: 1.40625, Actions in batch: {0: 4, 1: 4, 2: 16, 3: 8}\n",
            "TOTAL EPOCH: 10203, Episode: 151, Epoch: 20, Loss: 0.0040762280113995075, Mean Reward: 0.53125, Actions in batch: {0: 5, 1: 7, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 10223, Episode: 151, Epoch: 40, Loss: 0.007639680057764053, Mean Reward: 0.3125, Actions in batch: {0: 3, 1: 5, 2: 16, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10229, Episode: 152, Epoch: 0, Loss: 0.0003734149213414639, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 5, 2: 13, 3: 4}\n",
            "TOTAL EPOCH: 10249, Episode: 152, Epoch: 20, Loss: 0.0014286525547504425, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 19, 3: 3}\n",
            "TOTAL EPOCH: 10269, Episode: 152, Epoch: 40, Loss: 4.233006256981753e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 10, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 10289, Episode: 152, Epoch: 60, Loss: 0.0004702886799350381, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 19, 3: 8}\n",
            "TOTAL EPOCH: 10309, Episode: 152, Epoch: 80, Loss: 0.001044428558088839, Mean Reward: 1.375, Actions in batch: {0: 4, 1: 5, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 10329, Episode: 153, Epoch: 0, Loss: 0.0049248854629695415, Mean Reward: 0.65625, Actions in batch: {0: 5, 1: 9, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 10349, Episode: 153, Epoch: 20, Loss: 0.0004031205317005515, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 15, 3: 8}\n",
            "TOTAL EPOCH: 10369, Episode: 153, Epoch: 40, Loss: 0.003181921551004052, Mean Reward: 1.1875, Actions in batch: {0: 8, 1: 11, 2: 9, 3: 4}\n",
            "TOTAL EPOCH: 10389, Episode: 153, Epoch: 60, Loss: 8.843500108923763e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 3, 2: 13, 3: 11}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10403, Episode: 154, Epoch: 0, Loss: 3.1271261832444e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 10423, Episode: 154, Epoch: 20, Loss: 3.8459202187368646e-05, Mean Reward: 1.0625, Actions in batch: {0: 3, 1: 9, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 10443, Episode: 154, Epoch: 40, Loss: 0.00013680109987035394, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10461, Episode: 155, Epoch: 0, Loss: 0.0032084216363728046, Mean Reward: 1.8125, Actions in batch: {0: 2, 1: 8, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 10481, Episode: 155, Epoch: 20, Loss: 0.0036806066054850817, Mean Reward: 3.28125, Actions in batch: {0: 3, 1: 5, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 10501, Episode: 155, Epoch: 40, Loss: 0.0038055800832808018, Mean Reward: 1.1875, Actions in batch: {0: 7, 1: 3, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 10521, Episode: 155, Epoch: 60, Loss: 0.0011510199401527643, Mean Reward: 0.71875, Actions in batch: {0: 8, 1: 5, 2: 11, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10534, Episode: 156, Epoch: 0, Loss: 0.003018366638571024, Mean Reward: 1.25, Actions in batch: {0: 3, 1: 4, 2: 19, 3: 6}\n",
            "TOTAL EPOCH: 10554, Episode: 156, Epoch: 20, Loss: 0.002301745815202594, Mean Reward: 0.5, Actions in batch: {0: 6, 1: 7, 2: 7, 3: 12}\n",
            "TOTAL EPOCH: 10574, Episode: 156, Epoch: 40, Loss: 1.343359599559335e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 17, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10579, Episode: 157, Epoch: 0, Loss: 0.0021191954147070646, Mean Reward: 0.5625, Actions in batch: {0: 11, 1: 4, 2: 14, 3: 3}\n",
            "TOTAL EPOCH: 10599, Episode: 157, Epoch: 20, Loss: 6.380835657182615e-06, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 2, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 10619, Episode: 157, Epoch: 40, Loss: 0.007070457562804222, Mean Reward: 0.34375, Actions in batch: {0: 5, 1: 2, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 10639, Episode: 157, Epoch: 60, Loss: 0.00018010713392868638, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 2, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 10659, Episode: 157, Epoch: 80, Loss: 8.721521226107143e-06, Mean Reward: 1.25, Actions in batch: {0: 9, 1: 9, 2: 7, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10666, Episode: 158, Epoch: 0, Loss: 0.0016755759716033936, Mean Reward: 0.8125, Actions in batch: {0: 11, 1: 7, 2: 5, 3: 9}\n",
            "TOTAL EPOCH: 10686, Episode: 158, Epoch: 20, Loss: 0.0031035421416163445, Mean Reward: 2.40625, Actions in batch: {0: 1, 1: 4, 2: 16, 3: 11}\n",
            "TOTAL EPOCH: 10706, Episode: 158, Epoch: 40, Loss: 0.00119707768317312, Mean Reward: 0.75, Actions in batch: {0: 10, 1: 7, 2: 6, 3: 9}\n",
            "TOTAL EPOCH: 10726, Episode: 158, Epoch: 60, Loss: 0.0021613279823213816, Mean Reward: 1.78125, Actions in batch: {0: 5, 1: 3, 2: 21, 3: 3}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 10738, Episode: 159, Epoch: 0, Loss: 0.0002241917682113126, Mean Reward: 2.8125, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 10758, Episode: 159, Epoch: 20, Loss: 0.0011694782879203558, Mean Reward: 0.6875, Actions in batch: {0: 5, 1: 11, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 10778, Episode: 159, Epoch: 40, Loss: 0.008387098088860512, Mean Reward: 0.375, Actions in batch: {0: 7, 1: 5, 2: 18, 3: 2}\n",
            "TOTAL EPOCH: 10798, Episode: 159, Epoch: 60, Loss: 0.004175073001533747, Mean Reward: 0.375, Actions in batch: {0: 7, 1: 8, 2: 13, 3: 4}\n",
            "TOTAL EPOCH: 10818, Episode: 159, Epoch: 80, Loss: 9.875893010757864e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 7, 2: 17, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10834, Episode: 160, Epoch: 0, Loss: 0.0008889408782124519, Mean Reward: 0.90625, Actions in batch: {0: 9, 1: 10, 2: 9, 3: 4}\n",
            "TOTAL EPOCH: 10854, Episode: 160, Epoch: 20, Loss: 0.0049033015966415405, Mean Reward: 1.15625, Actions in batch: {0: 6, 1: 3, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 10874, Episode: 160, Epoch: 40, Loss: 0.0017735088476911187, Mean Reward: 0.625, Actions in batch: {0: 8, 1: 7, 2: 7, 3: 10}\n",
            "TOTAL EPOCH: 10894, Episode: 160, Epoch: 60, Loss: 0.00011953414650633931, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 16, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 10895, Episode: 161, Epoch: 0, Loss: 0.0002574738464318216, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 2, 2: 16, 3: 6}\n",
            "TOTAL EPOCH: 10915, Episode: 161, Epoch: 20, Loss: 0.0012499381555244327, Mean Reward: 2.0, Actions in batch: {0: 4, 1: 9, 2: 17, 3: 2}\n",
            "TOTAL EPOCH: 10935, Episode: 161, Epoch: 40, Loss: 0.00071462377673015, Mean Reward: 0.875, Actions in batch: {0: 8, 1: 8, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 10955, Episode: 161, Epoch: 60, Loss: 9.18196474231081e-06, Mean Reward: 0.0, Actions in batch: {1: 8, 2: 17, 3: 7}\n",
            "TOTAL EPOCH: 10975, Episode: 161, Epoch: 80, Loss: 0.0016411824617534876, Mean Reward: 2.65625, Actions in batch: {0: 3, 1: 7, 2: 19, 3: 3}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 10991, Episode: 162, Epoch: 0, Loss: 0.0001494966127211228, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 9, 3: 7}\n",
            "TOTAL EPOCH: 11011, Episode: 162, Epoch: 20, Loss: 0.0008810795261524618, Mean Reward: 4.0, Actions in batch: {0: 5, 1: 2, 2: 16, 3: 9}\n",
            "TOTAL EPOCH: 11031, Episode: 162, Epoch: 40, Loss: 0.0006806207820773125, Mean Reward: 1.25, Actions in batch: {0: 5, 1: 9, 2: 15, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 11049, Episode: 163, Epoch: 0, Loss: 0.01015565637499094, Mean Reward: 0.25, Actions in batch: {0: 4, 1: 4, 2: 13, 3: 11}\n",
            "TOTAL EPOCH: 11069, Episode: 163, Epoch: 20, Loss: 0.023488298058509827, Mean Reward: 0.0625, Actions in batch: {0: 4, 1: 4, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 11089, Episode: 163, Epoch: 40, Loss: 0.00022256156080402434, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 11109, Episode: 163, Epoch: 60, Loss: 0.0017953906208276749, Mean Reward: 2.125, Actions in batch: {0: 5, 1: 3, 2: 13, 3: 11}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 11129, Episode: 164, Epoch: 0, Loss: 0.002066073939204216, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 11149, Episode: 164, Epoch: 20, Loss: 0.0014584569726139307, Mean Reward: 1.875, Actions in batch: {0: 4, 1: 4, 2: 14, 3: 10}\n",
            "TOTAL EPOCH: 11169, Episode: 164, Epoch: 40, Loss: 0.0025508208200335503, Mean Reward: 0.5625, Actions in batch: {0: 3, 1: 6, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 11189, Episode: 164, Epoch: 60, Loss: 0.0049607399851083755, Mean Reward: 3.125, Actions in batch: {0: 5, 1: 3, 2: 13, 3: 11}\n",
            "TOTAL EPOCH: 11209, Episode: 164, Epoch: 80, Loss: 0.0001441836793674156, Mean Reward: 3.09375, Actions in batch: {0: 3, 1: 5, 2: 19, 3: 5}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 11222, Episode: 165, Epoch: 0, Loss: 0.00010196336370427161, Mean Reward: 1.0, Actions in batch: {0: 4, 1: 3, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 11242, Episode: 165, Epoch: 20, Loss: 0.006773645523935556, Mean Reward: 1.625, Actions in batch: {0: 5, 1: 6, 2: 13, 3: 8}\n",
            "TOTAL EPOCH: 11262, Episode: 165, Epoch: 40, Loss: 0.003961723763495684, Mean Reward: 2.15625, Actions in batch: {0: 8, 1: 3, 2: 13, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 11271, Episode: 166, Epoch: 0, Loss: 0.012864813208580017, Mean Reward: 0.1875, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 11291, Episode: 166, Epoch: 20, Loss: 0.0005230522365309298, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 6, 2: 21, 3: 1}\n",
            "TOTAL EPOCH: 11311, Episode: 166, Epoch: 40, Loss: 0.009013595059514046, Mean Reward: 0.28125, Actions in batch: {0: 4, 1: 3, 2: 19, 3: 6}\n",
            "TOTAL EPOCH: 11331, Episode: 166, Epoch: 60, Loss: 0.00472738454118371, Mean Reward: 0.6875, Actions in batch: {0: 4, 1: 3, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 11351, Episode: 166, Epoch: 80, Loss: 1.6173569747479632e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 7, 2: 10, 3: 8}\n",
            "TOTAL EPOCH: 11371, Episode: 167, Epoch: 0, Loss: 0.00016143535322044045, Mean Reward: 3.1875, Actions in batch: {0: 3, 1: 6, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 11391, Episode: 167, Epoch: 20, Loss: 8.349187555722892e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 11411, Episode: 167, Epoch: 40, Loss: 0.0011696593137457967, Mean Reward: 2.0, Actions in batch: {0: 6, 1: 1, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 11431, Episode: 167, Epoch: 60, Loss: 0.004469068255275488, Mean Reward: 1.40625, Actions in batch: {0: 8, 1: 9, 2: 12, 3: 3}\n",
            "TOTAL EPOCH: 11451, Episode: 167, Epoch: 80, Loss: 0.01699126325547695, Mean Reward: 0.15625, Actions in batch: {0: 2, 1: 4, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 11471, Episode: 168, Epoch: 0, Loss: 5.38283129571937e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 5, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 11491, Episode: 168, Epoch: 20, Loss: 0.0007777984137646854, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 11, 3: 7}\n",
            "TOTAL EPOCH: 11511, Episode: 168, Epoch: 40, Loss: 0.0009505514753982425, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 5, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 11531, Episode: 168, Epoch: 60, Loss: 0.0034155447501689196, Mean Reward: 1.71875, Actions in batch: {0: 2, 1: 4, 2: 19, 3: 7}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 11544, Episode: 169, Epoch: 0, Loss: 0.0017037694342434406, Mean Reward: 0.875, Actions in batch: {0: 2, 1: 3, 2: 19, 3: 8}\n",
            "TOTAL EPOCH: 11564, Episode: 169, Epoch: 20, Loss: 0.00010963093518512323, Mean Reward: 1.0, Actions in batch: {0: 10, 1: 8, 2: 11, 3: 3}\n",
            "TOTAL EPOCH: 11584, Episode: 169, Epoch: 40, Loss: 0.0024392518680542707, Mean Reward: 1.34375, Actions in batch: {0: 5, 1: 7, 2: 15, 3: 5}\n",
            "TOTAL EPOCH: 11604, Episode: 169, Epoch: 60, Loss: 4.155560600338504e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 5, 2: 15, 3: 8}\n",
            "TOTAL EPOCH: 11624, Episode: 169, Epoch: 80, Loss: 3.396566171431914e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 4, 2: 16, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 11634, Episode: 170, Epoch: 0, Loss: 6.050696538295597e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 7, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 11654, Episode: 170, Epoch: 20, Loss: 0.001376294530928135, Mean Reward: 0.84375, Actions in batch: {0: 11, 1: 8, 2: 4, 3: 9}\n",
            "TOTAL EPOCH: 11674, Episode: 170, Epoch: 40, Loss: 0.00020830756693612784, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 11694, Episode: 170, Epoch: 60, Loss: 0.003425083588808775, Mean Reward: 1.25, Actions in batch: {0: 9, 1: 7, 2: 14, 3: 2}\n",
            "TOTAL EPOCH: 11714, Episode: 170, Epoch: 80, Loss: 3.41112790920306e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 20, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 11729, Episode: 171, Epoch: 0, Loss: 1.1474536876221464e-07, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 17, 3: 7}\n",
            "TOTAL EPOCH: 11749, Episode: 171, Epoch: 20, Loss: 0.0007582274847663939, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 3, 2: 18, 3: 3}\n",
            "TOTAL EPOCH: 11769, Episode: 171, Epoch: 40, Loss: 0.0001289446372538805, Mean Reward: 0.78125, Actions in batch: {0: 7, 1: 9, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 11789, Episode: 171, Epoch: 60, Loss: 0.031307972967624664, Mean Reward: 0.03125, Actions in batch: {0: 3, 1: 3, 2: 20, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 11796, Episode: 172, Epoch: 0, Loss: 0.0004818710149265826, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 5, 2: 12, 3: 9}\n",
            "TOTAL EPOCH: 11816, Episode: 172, Epoch: 20, Loss: 0.0050298538990318775, Mean Reward: 1.46875, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 11836, Episode: 172, Epoch: 40, Loss: 0.0011962646385654807, Mean Reward: 3.25, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 11856, Episode: 172, Epoch: 60, Loss: 5.160112777957693e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 11876, Episode: 172, Epoch: 80, Loss: 5.045507714385167e-05, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 7, 2: 12, 3: 4}\n",
            "TOTAL EPOCH: 11896, Episode: 173, Epoch: 0, Loss: 0.0033649923279881477, Mean Reward: 1.875, Actions in batch: {0: 4, 1: 2, 2: 20, 3: 6}\n",
            "TOTAL EPOCH: 11916, Episode: 173, Epoch: 20, Loss: 2.595997727894428e-07, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 17, 3: 2}\n",
            "TOTAL EPOCH: 11936, Episode: 173, Epoch: 40, Loss: 0.0030536423437297344, Mean Reward: 1.03125, Actions in batch: {0: 7, 1: 6, 2: 16, 3: 3}\n",
            "TOTAL EPOCH: 11956, Episode: 173, Epoch: 60, Loss: 5.676365617546253e-05, Mean Reward: 0.78125, Actions in batch: {0: 4, 1: 5, 2: 13, 3: 10}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 11963, Episode: 174, Epoch: 0, Loss: 6.513894186355174e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 11983, Episode: 174, Epoch: 20, Loss: 0.004183129407465458, Mean Reward: 3.125, Actions in batch: {0: 5, 1: 3, 2: 13, 3: 11}\n",
            "TOTAL EPOCH: 12003, Episode: 174, Epoch: 40, Loss: 0.03034229576587677, Mean Reward: 0.03125, Actions in batch: {0: 4, 1: 3, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 12023, Episode: 174, Epoch: 60, Loss: 0.005298738367855549, Mean Reward: 0.5, Actions in batch: {0: 6, 1: 7, 2: 12, 3: 7}\n",
            "TOTAL EPOCH: 12043, Episode: 174, Epoch: 80, Loss: 0.00026848079869523644, Mean Reward: 0.8125, Actions in batch: {0: 9, 1: 11, 2: 9, 3: 3}\n",
            "TOTAL EPOCH: 12063, Episode: 175, Epoch: 0, Loss: 0.000683463120367378, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 20, 3: 6}\n",
            "TOTAL EPOCH: 12083, Episode: 175, Epoch: 20, Loss: 0.010112594813108444, Mean Reward: 0.28125, Actions in batch: {0: 4, 1: 2, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 12103, Episode: 175, Epoch: 40, Loss: 3.5491657399688847e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 12123, Episode: 175, Epoch: 60, Loss: 0.00015462978626601398, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 9, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 12143, Episode: 176, Epoch: 0, Loss: 8.19360648165457e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 12163, Episode: 176, Epoch: 20, Loss: 7.849321991670877e-05, Mean Reward: 0.0, Actions in batch: {1: 4, 2: 17, 3: 11}\n",
            "TOTAL EPOCH: 12183, Episode: 176, Epoch: 40, Loss: 0.0017479881644248962, Mean Reward: 0.875, Actions in batch: {0: 6, 1: 6, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 12203, Episode: 176, Epoch: 60, Loss: 0.00812359619885683, Mean Reward: 0.25, Actions in batch: {0: 6, 1: 4, 2: 17, 3: 5}\n",
            "TOTAL EPOCH: 12223, Episode: 176, Epoch: 80, Loss: 0.0004650235641747713, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 14, 3: 8}\n",
            "TOTAL EPOCH: 12243, Episode: 177, Epoch: 0, Loss: 0.002426629653200507, Mean Reward: 3.34375, Actions in batch: {0: 4, 1: 3, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 12263, Episode: 177, Epoch: 20, Loss: 0.001215584808960557, Mean Reward: 0.90625, Actions in batch: {0: 5, 1: 4, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 12283, Episode: 177, Epoch: 40, Loss: 0.008889991790056229, Mean Reward: 1.09375, Actions in batch: {0: 3, 1: 8, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 12303, Episode: 177, Epoch: 60, Loss: 1.618702481209766e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 12323, Episode: 177, Epoch: 80, Loss: 0.002825707197189331, Mean Reward: 1.75, Actions in batch: {0: 4, 1: 10, 2: 12, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 12325, Episode: 178, Epoch: 0, Loss: 0.0021375438664108515, Mean Reward: 2.3125, Actions in batch: {0: 6, 1: 2, 2: 23, 3: 1}\n",
            "TOTAL EPOCH: 12345, Episode: 178, Epoch: 20, Loss: 0.011522218585014343, Mean Reward: 0.46875, Actions in batch: {0: 9, 1: 11, 2: 6, 3: 6}\n",
            "TOTAL EPOCH: 12365, Episode: 178, Epoch: 40, Loss: 0.002822460373863578, Mean Reward: 1.625, Actions in batch: {0: 5, 1: 5, 2: 20, 3: 2}\n",
            "TOTAL EPOCH: 12385, Episode: 178, Epoch: 60, Loss: 0.00813590083271265, Mean Reward: 0.34375, Actions in batch: {0: 7, 1: 7, 2: 15, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 12387, Episode: 179, Epoch: 0, Loss: 0.006031381897628307, Mean Reward: 0.625, Actions in batch: {1: 12, 2: 18, 3: 2}\n",
            "TOTAL EPOCH: 12407, Episode: 179, Epoch: 20, Loss: 0.0009425609605386853, Mean Reward: 2.25, Actions in batch: {0: 4, 1: 11, 2: 9, 3: 8}\n",
            "TOTAL EPOCH: 12427, Episode: 179, Epoch: 40, Loss: 0.008334273472428322, Mean Reward: 0.25, Actions in batch: {0: 4, 1: 7, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 12447, Episode: 179, Epoch: 60, Loss: 1.069603968062438e-06, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 4, 2: 18, 3: 3}\n",
            "TOTAL EPOCH: 12467, Episode: 179, Epoch: 80, Loss: 0.00014958955580368638, Mean Reward: 2.28125, Actions in batch: {0: 5, 1: 9, 2: 16, 3: 2}\n",
            "TOTAL EPOCH: 12487, Episode: 180, Epoch: 0, Loss: 0.002622834872454405, Mean Reward: 2.5625, Actions in batch: {0: 5, 1: 5, 2: 19, 3: 3}\n",
            "TOTAL EPOCH: 12507, Episode: 180, Epoch: 20, Loss: 0.0004205239238217473, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 12527, Episode: 180, Epoch: 40, Loss: 6.719064549542964e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 7, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 12547, Episode: 180, Epoch: 60, Loss: 5.5104028433561325e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 6, 2: 13, 3: 7}\n",
            "TOTAL EPOCH: 12567, Episode: 180, Epoch: 80, Loss: 0.00021669897250831127, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 21, 3: 2}\n",
            "TOTAL EPOCH: 12587, Episode: 181, Epoch: 0, Loss: 4.5787695853505284e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 6, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 12607, Episode: 181, Epoch: 20, Loss: 0.005116557236760855, Mean Reward: 0.4375, Actions in batch: {0: 4, 1: 5, 2: 21, 3: 2}\n",
            "TOTAL EPOCH: 12627, Episode: 181, Epoch: 40, Loss: 0.010972771793603897, Mean Reward: 0.15625, Actions in batch: {0: 9, 1: 8, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 12647, Episode: 181, Epoch: 60, Loss: 0.0037844665348529816, Mean Reward: 0.875, Actions in batch: {0: 1, 1: 2, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 12667, Episode: 181, Epoch: 80, Loss: 0.00010357501014368609, Mean Reward: 2.15625, Actions in batch: {0: 1, 1: 4, 2: 19, 3: 8}\n",
            "TOTAL EPOCH: 12687, Episode: 182, Epoch: 0, Loss: 0.0011404695687815547, Mean Reward: 3.40625, Actions in batch: {0: 2, 1: 4, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 12707, Episode: 182, Epoch: 20, Loss: 0.0035206624306738377, Mean Reward: 1.5625, Actions in batch: {0: 3, 1: 3, 2: 20, 3: 6}\n",
            "TOTAL EPOCH: 12727, Episode: 182, Epoch: 40, Loss: 0.005727171897888184, Mean Reward: 0.5, Actions in batch: {0: 3, 1: 5, 2: 20, 3: 4}\n",
            "TOTAL EPOCH: 12747, Episode: 182, Epoch: 60, Loss: 0.0016647008014842868, Mean Reward: 2.46875, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 12761, Episode: 183, Epoch: 0, Loss: 0.0036522112786769867, Mean Reward: 0.84375, Actions in batch: {0: 3, 1: 5, 2: 20, 3: 4}\n",
            "TOTAL EPOCH: 12781, Episode: 183, Epoch: 20, Loss: 0.001807181746698916, Mean Reward: 3.0, Actions in batch: {0: 4, 1: 2, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 12801, Episode: 183, Epoch: 40, Loss: 5.580931610893458e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 4, 2: 16, 3: 6}\n",
            "TOTAL EPOCH: 12821, Episode: 183, Epoch: 60, Loss: 0.0043500866740942, Mean Reward: 0.875, Actions in batch: {0: 4, 1: 3, 2: 18, 3: 7}\n",
            "TOTAL EPOCH: 12841, Episode: 183, Epoch: 80, Loss: 4.949760841554962e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 5, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 12861, Episode: 184, Epoch: 0, Loss: 0.0015013008378446102, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 12881, Episode: 184, Epoch: 20, Loss: 0.00014876808563712984, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 6, 2: 15, 3: 8}\n",
            "TOTAL EPOCH: 12901, Episode: 184, Epoch: 40, Loss: 1.1326443200232461e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 3, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 12921, Episode: 184, Epoch: 60, Loss: 0.0022217128425836563, Mean Reward: 2.1875, Actions in batch: {0: 6, 1: 5, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 12941, Episode: 184, Epoch: 80, Loss: 0.00027001090347766876, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 24, 3: 2}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 12951, Episode: 185, Epoch: 0, Loss: 0.00032777554588392377, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 12971, Episode: 185, Epoch: 20, Loss: 0.003526730928570032, Mean Reward: 1.1875, Actions in batch: {0: 6, 1: 5, 2: 14, 3: 7}\n",
            "TOTAL EPOCH: 12991, Episode: 185, Epoch: 40, Loss: 7.052987348288298e-05, Mean Reward: 1.25, Actions in batch: {0: 6, 1: 4, 2: 13, 3: 9}\n",
            "TOTAL EPOCH: 13011, Episode: 185, Epoch: 60, Loss: 0.00013017393939662725, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 13020, Episode: 186, Epoch: 0, Loss: 0.0001509287831140682, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 4, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 13040, Episode: 186, Epoch: 20, Loss: 1.0882933565881103e-05, Mean Reward: 0.0, Actions in batch: {1: 3, 2: 23, 3: 6}\n",
            "TOTAL EPOCH: 13060, Episode: 186, Epoch: 40, Loss: 0.00558489840477705, Mean Reward: 1.09375, Actions in batch: {0: 5, 1: 6, 2: 19, 3: 2}\n",
            "TOTAL EPOCH: 13080, Episode: 186, Epoch: 60, Loss: 0.00023968744790181518, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 2, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 13100, Episode: 186, Epoch: 80, Loss: 2.37996646319516e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 13120, Episode: 187, Epoch: 0, Loss: 2.1940779333817773e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 13140, Episode: 187, Epoch: 20, Loss: 0.0003165087546221912, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 2, 2: 21, 3: 2}\n",
            "TOTAL EPOCH: 13160, Episode: 187, Epoch: 40, Loss: 0.009682789444923401, Mean Reward: 0.21875, Actions in batch: {0: 5, 1: 1, 2: 20, 3: 6}\n",
            "TOTAL EPOCH: 13180, Episode: 187, Epoch: 60, Loss: 0.00018483171879779547, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 13200, Episode: 187, Epoch: 80, Loss: 9.27930432226276e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 23, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 13202, Episode: 188, Epoch: 0, Loss: 5.4956320383325874e-08, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 13222, Episode: 188, Epoch: 20, Loss: 5.994757430016762e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 15, 3: 10}\n",
            "TOTAL EPOCH: 13242, Episode: 188, Epoch: 40, Loss: 0.0032171388156712055, Mean Reward: 1.625, Actions in batch: {0: 5, 1: 4, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 13262, Episode: 188, Epoch: 60, Loss: 2.532443431846332e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 6, 2: 15, 3: 7}\n",
            "TOTAL EPOCH: 13282, Episode: 188, Epoch: 80, Loss: 0.00014911897596903145, Mean Reward: 0.96875, Actions in batch: {0: 6, 1: 4, 2: 19, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 13288, Episode: 189, Epoch: 0, Loss: 0.00011995773093076423, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 13308, Episode: 189, Epoch: 20, Loss: 1.8255936083733104e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 2, 2: 17, 3: 5}\n",
            "TOTAL EPOCH: 13328, Episode: 189, Epoch: 40, Loss: 0.0008187807397916913, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 3, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 13348, Episode: 189, Epoch: 60, Loss: 0.0034147268161177635, Mean Reward: 0.46875, Actions in batch: {0: 4, 1: 8, 2: 13, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 13365, Episode: 190, Epoch: 0, Loss: 0.005673428066074848, Mean Reward: 0.5625, Actions in batch: {0: 2, 1: 3, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 13385, Episode: 190, Epoch: 20, Loss: 6.27318195256521e-06, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 1, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 13405, Episode: 190, Epoch: 40, Loss: 0.0008737669559195638, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 6, 2: 17, 3: 4}\n",
            "TOTAL EPOCH: 13425, Episode: 190, Epoch: 60, Loss: 0.0036512294318526983, Mean Reward: 0.4375, Actions in batch: {0: 9, 1: 4, 2: 10, 3: 9}\n",
            "TOTAL EPOCH: 13445, Episode: 190, Epoch: 80, Loss: 0.0021679853089153767, Mean Reward: 3.625, Actions in batch: {0: 3, 1: 2, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 13465, Episode: 191, Epoch: 0, Loss: 0.0095972353592515, Mean Reward: 0.21875, Actions in batch: {0: 2, 1: 5, 2: 19, 3: 6}\n",
            "TOTAL EPOCH: 13485, Episode: 191, Epoch: 20, Loss: 0.0017959583783522248, Mean Reward: 2.46875, Actions in batch: {0: 2, 1: 3, 2: 18, 3: 9}\n",
            "TOTAL EPOCH: 13505, Episode: 191, Epoch: 40, Loss: 0.0011815138859674335, Mean Reward: 0.59375, Actions in batch: {0: 1, 1: 6, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 13525, Episode: 191, Epoch: 60, Loss: 0.0012161018094047904, Mean Reward: 3.65625, Actions in batch: {0: 6, 1: 7, 2: 16, 3: 3}\n",
            "TOTAL EPOCH: 13545, Episode: 191, Epoch: 80, Loss: 0.0068819839507341385, Mean Reward: 0.25, Actions in batch: {0: 7, 1: 5, 2: 16, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 13549, Episode: 192, Epoch: 0, Loss: 0.0028250766918063164, Mean Reward: 0.59375, Actions in batch: {0: 7, 1: 6, 2: 15, 3: 4}\n",
            "TOTAL EPOCH: 13569, Episode: 192, Epoch: 20, Loss: 1.097166432373342e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 8, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 13589, Episode: 192, Epoch: 40, Loss: 0.00960775651037693, Mean Reward: 0.1875, Actions in batch: {0: 4, 1: 7, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 13609, Episode: 192, Epoch: 60, Loss: 0.0013536347541958094, Mean Reward: 2.1875, Actions in batch: {0: 8, 1: 6, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 13629, Episode: 192, Epoch: 80, Loss: 0.004162106197327375, Mean Reward: 1.84375, Actions in batch: {0: 8, 1: 4, 2: 13, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 13644, Episode: 193, Epoch: 0, Loss: 0.000658032950013876, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 12, 3: 6}\n",
            "TOTAL EPOCH: 13664, Episode: 193, Epoch: 20, Loss: 0.005182548891752958, Mean Reward: 0.5625, Actions in batch: {0: 5, 1: 5, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 13684, Episode: 193, Epoch: 40, Loss: 0.011825825087726116, Mean Reward: 0.1875, Actions in batch: {0: 5, 1: 5, 2: 17, 3: 5}\n",
            "TOTAL EPOCH: 13704, Episode: 193, Epoch: 60, Loss: 0.000344836269505322, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 2, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 13724, Episode: 193, Epoch: 80, Loss: 0.0005569845670834184, Mean Reward: 3.0, Actions in batch: {0: 7, 1: 3, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 13744, Episode: 194, Epoch: 0, Loss: 0.0013214311329647899, Mean Reward: 1.40625, Actions in batch: {0: 1, 1: 7, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 13764, Episode: 194, Epoch: 20, Loss: 8.457121111860033e-06, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 3, 2: 23, 3: 1}\n",
            "TOTAL EPOCH: 13784, Episode: 194, Epoch: 40, Loss: 0.0001733757962938398, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 21, 3: 8}\n",
            "TOTAL EPOCH: 13804, Episode: 194, Epoch: 60, Loss: 0.0020889851730316877, Mean Reward: 0.625, Actions in batch: {1: 6, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 13824, Episode: 194, Epoch: 80, Loss: 0.01027672179043293, Mean Reward: 0.5625, Actions in batch: {0: 9, 1: 12, 2: 6, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 13825, Episode: 195, Epoch: 0, Loss: 0.001906536752358079, Mean Reward: 1.625, Actions in batch: {0: 5, 1: 4, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 13845, Episode: 195, Epoch: 20, Loss: 0.012388424947857857, Mean Reward: 0.1875, Actions in batch: {0: 10, 1: 5, 2: 10, 3: 7}\n",
            "TOTAL EPOCH: 13865, Episode: 195, Epoch: 40, Loss: 0.003503450658172369, Mean Reward: 1.4375, Actions in batch: {0: 5, 1: 5, 2: 21, 3: 1}\n",
            "TOTAL EPOCH: 13885, Episode: 195, Epoch: 60, Loss: 0.005700918845832348, Mean Reward: 0.5, Actions in batch: {0: 4, 1: 3, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 13905, Episode: 195, Epoch: 80, Loss: 0.0006463289028033614, Mean Reward: 1.78125, Actions in batch: {0: 4, 1: 6, 2: 18, 3: 4}\n",
            "Max reward for this episode:  5.0\n",
            "TOTAL EPOCH: 13925, Episode: 196, Epoch: 0, Loss: 0.0031706313602626324, Mean Reward: 1.125, Actions in batch: {1: 10, 2: 21, 3: 1}\n",
            "TOTAL EPOCH: 13945, Episode: 196, Epoch: 20, Loss: 0.0003850841603707522, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 16, 3: 7}\n",
            "TOTAL EPOCH: 13965, Episode: 196, Epoch: 40, Loss: 0.001225705025717616, Mean Reward: 3.0, Actions in batch: {0: 4, 1: 7, 2: 12, 3: 9}\n",
            "TOTAL EPOCH: 13985, Episode: 196, Epoch: 60, Loss: 0.017283055931329727, Mean Reward: 0.125, Actions in batch: {0: 6, 1: 4, 2: 11, 3: 11}\n",
            "TOTAL EPOCH: 14005, Episode: 196, Epoch: 80, Loss: 2.5421028112759814e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 19, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 14009, Episode: 197, Epoch: 0, Loss: 0.00022273248760029674, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 14029, Episode: 197, Epoch: 20, Loss: 0.006029179319739342, Mean Reward: 0.53125, Actions in batch: {0: 5, 1: 7, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 14049, Episode: 197, Epoch: 40, Loss: 0.006181146018207073, Mean Reward: 0.5, Actions in batch: {0: 3, 1: 6, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 14069, Episode: 197, Epoch: 60, Loss: 0.0004108445136807859, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 3, 2: 20, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 14083, Episode: 198, Epoch: 0, Loss: 0.0013301505241543055, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 14103, Episode: 198, Epoch: 20, Loss: 0.00036898491089232266, Mean Reward: 2.15625, Actions in batch: {0: 3, 1: 4, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 14123, Episode: 198, Epoch: 40, Loss: 4.283798625692725e-05, Mean Reward: 2.0625, Actions in batch: {0: 4, 1: 3, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 14143, Episode: 198, Epoch: 60, Loss: 0.0007314941613003612, Mean Reward: 2.875, Actions in batch: {0: 4, 1: 5, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 14163, Episode: 198, Epoch: 80, Loss: 0.00012850291386712343, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 14183, Episode: 199, Epoch: 0, Loss: 7.054457819322124e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 14203, Episode: 199, Epoch: 20, Loss: 0.0013726179022341967, Mean Reward: 2.0, Actions in batch: {0: 4, 1: 8, 2: 18, 3: 2}\n",
            "TOTAL EPOCH: 14223, Episode: 199, Epoch: 40, Loss: 0.0010649125324562192, Mean Reward: 2.96875, Actions in batch: {0: 1, 1: 2, 2: 20, 3: 9}\n",
            "TOTAL EPOCH: 14243, Episode: 199, Epoch: 60, Loss: 3.441187436692417e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 14263, Episode: 199, Epoch: 80, Loss: 6.099331585573964e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 14283, Episode: 200, Epoch: 0, Loss: 0.00600604759529233, Mean Reward: 0.3125, Actions in batch: {0: 1, 1: 2, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 14303, Episode: 200, Epoch: 20, Loss: 0.0030028480105102062, Mean Reward: 1.25, Actions in batch: {0: 4, 1: 6, 2: 16, 3: 6}\n",
            "TOTAL EPOCH: 14323, Episode: 200, Epoch: 40, Loss: 2.8769922209903598e-05, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 1, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 14343, Episode: 200, Epoch: 60, Loss: 0.007930217310786247, Mean Reward: 0.28125, Actions in batch: {0: 4, 1: 4, 2: 16, 3: 8}\n",
            "TOTAL EPOCH: 14363, Episode: 200, Epoch: 80, Loss: 0.002626024652272463, Mean Reward: 1.4375, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 14370, Episode: 201, Epoch: 0, Loss: 0.0003263830440118909, Mean Reward: 3.25, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 14390, Episode: 201, Epoch: 20, Loss: 0.003727257950231433, Mean Reward: 0.5625, Actions in batch: {0: 5, 1: 8, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 14410, Episode: 201, Epoch: 40, Loss: 4.417581658344716e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 14430, Episode: 201, Epoch: 60, Loss: 0.00020531349582597613, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 6, 2: 16, 3: 4}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 14440, Episode: 202, Epoch: 0, Loss: 0.0050160582177340984, Mean Reward: 0.40625, Actions in batch: {0: 6, 1: 5, 2: 16, 3: 5}\n",
            "TOTAL EPOCH: 14460, Episode: 202, Epoch: 20, Loss: 0.0013814934063702822, Mean Reward: 2.90625, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 14480, Episode: 202, Epoch: 40, Loss: 0.00014382465451490134, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 1, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 14500, Episode: 202, Epoch: 60, Loss: 0.0003243662358727306, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 19, 3: 6}\n",
            "TOTAL EPOCH: 14520, Episode: 202, Epoch: 80, Loss: 0.00019746311591006815, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 4, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 14540, Episode: 203, Epoch: 0, Loss: 0.002924839034676552, Mean Reward: 2.21875, Actions in batch: {0: 6, 1: 2, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 14560, Episode: 203, Epoch: 20, Loss: 0.0030880700796842575, Mean Reward: 1.125, Actions in batch: {1: 7, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 14580, Episode: 203, Epoch: 40, Loss: 0.0007377180154435337, Mean Reward: 2.8125, Actions in batch: {0: 6, 1: 8, 2: 13, 3: 5}\n",
            "TOTAL EPOCH: 14600, Episode: 203, Epoch: 60, Loss: 0.00521834334358573, Mean Reward: 0.5625, Actions in batch: {0: 3, 1: 5, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 14620, Episode: 203, Epoch: 80, Loss: 4.153268309892155e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 14637, Episode: 204, Epoch: 0, Loss: 0.0010366904316470027, Mean Reward: 1.125, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 14657, Episode: 204, Epoch: 20, Loss: 0.008155462332069874, Mean Reward: 0.46875, Actions in batch: {0: 4, 1: 8, 2: 14, 3: 6}\n",
            "TOTAL EPOCH: 14677, Episode: 204, Epoch: 40, Loss: 0.0025070523843169212, Mean Reward: 2.125, Actions in batch: {0: 4, 1: 5, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 14697, Episode: 204, Epoch: 60, Loss: 0.0041003357619047165, Mean Reward: 1.46875, Actions in batch: {0: 4, 1: 4, 2: 15, 3: 9}\n",
            "TOTAL EPOCH: 14717, Episode: 204, Epoch: 80, Loss: 5.536883691092953e-05, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 2, 2: 15, 3: 8}\n",
            "TOTAL EPOCH: 14737, Episode: 205, Epoch: 0, Loss: 0.0008227991638705134, Mean Reward: 2.03125, Actions in batch: {0: 6, 1: 3, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 14757, Episode: 205, Epoch: 20, Loss: 0.0001212135175592266, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 7, 2: 15, 3: 5}\n",
            "TOTAL EPOCH: 14777, Episode: 205, Epoch: 40, Loss: 1.0767035746539477e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 14797, Episode: 205, Epoch: 60, Loss: 0.00203393935225904, Mean Reward: 1.84375, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 14817, Episode: 205, Epoch: 80, Loss: 1.629469079489354e-05, Mean Reward: 0.0, Actions in batch: {1: 5, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 14837, Episode: 206, Epoch: 0, Loss: 8.80187435541302e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 14857, Episode: 206, Epoch: 20, Loss: 0.002187772188335657, Mean Reward: 1.6875, Actions in batch: {0: 6, 1: 2, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 14877, Episode: 206, Epoch: 40, Loss: 0.0164580587297678, Mean Reward: 0.125, Actions in batch: {0: 2, 1: 3, 2: 19, 3: 8}\n",
            "TOTAL EPOCH: 14897, Episode: 206, Epoch: 60, Loss: 0.0010369615629315376, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 2, 2: 17, 3: 8}\n",
            "TOTAL EPOCH: 14917, Episode: 206, Epoch: 80, Loss: 0.0025600942317396402, Mean Reward: 1.375, Actions in batch: {0: 1, 1: 4, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 14937, Episode: 207, Epoch: 0, Loss: 0.00018812633061315864, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 14957, Episode: 207, Epoch: 20, Loss: 0.00011239657760597765, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 14977, Episode: 207, Epoch: 40, Loss: 0.011887399479746819, Mean Reward: 0.1875, Actions in batch: {0: 6, 1: 5, 2: 21}\n",
            "TOTAL EPOCH: 14997, Episode: 207, Epoch: 60, Loss: 0.00013201229739934206, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 15017, Episode: 207, Epoch: 80, Loss: 0.0011517106322571635, Mean Reward: 3.125, Actions in batch: {0: 4, 1: 5, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 15037, Episode: 208, Epoch: 0, Loss: 5.6889763072831556e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 7, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 15057, Episode: 208, Epoch: 20, Loss: 0.0005124588496983051, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 15077, Episode: 208, Epoch: 40, Loss: 3.49949550582096e-06, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 2, 2: 17, 3: 5}\n",
            "TOTAL EPOCH: 15097, Episode: 208, Epoch: 60, Loss: 1.1756459571188316e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 16, 3: 9}\n",
            "TOTAL EPOCH: 15117, Episode: 208, Epoch: 80, Loss: 0.0003799328696914017, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 15134, Episode: 209, Epoch: 0, Loss: 0.0047544194385409355, Mean Reward: 1.6875, Actions in batch: {0: 8, 1: 11, 2: 10, 3: 3}\n",
            "TOTAL EPOCH: 15154, Episode: 209, Epoch: 20, Loss: 0.002402526792138815, Mean Reward: 2.875, Actions in batch: {1: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 15174, Episode: 209, Epoch: 40, Loss: 0.00039470018236897886, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 1, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 15194, Episode: 209, Epoch: 60, Loss: 0.005794912576675415, Mean Reward: 1.125, Actions in batch: {0: 5, 1: 4, 2: 21, 3: 2}\n",
            "TOTAL EPOCH: 15214, Episode: 209, Epoch: 80, Loss: 0.00015869256458245218, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 25, 3: 3}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 15222, Episode: 210, Epoch: 0, Loss: 0.00011119373812107369, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 15242, Episode: 210, Epoch: 20, Loss: 0.000879676197655499, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 15262, Episode: 210, Epoch: 40, Loss: 0.004042274318635464, Mean Reward: 1.5, Actions in batch: {0: 3, 1: 3, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 15282, Episode: 210, Epoch: 60, Loss: 0.00042060547275468707, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 15302, Episode: 210, Epoch: 80, Loss: 0.00047225478920154274, Mean Reward: 1.375, Actions in batch: {0: 2, 1: 6, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 15322, Episode: 211, Epoch: 0, Loss: 0.003218079684302211, Mean Reward: 1.0625, Actions in batch: {0: 1, 1: 3, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 15342, Episode: 211, Epoch: 20, Loss: 0.003977521322667599, Mean Reward: 0.875, Actions in batch: {0: 2, 1: 4, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 15362, Episode: 211, Epoch: 40, Loss: 0.002161209937185049, Mean Reward: 1.53125, Actions in batch: {0: 5, 1: 4, 2: 22, 3: 1}\n",
            "TOTAL EPOCH: 15382, Episode: 211, Epoch: 60, Loss: 0.0011550004128366709, Mean Reward: 3.34375, Actions in batch: {0: 6, 1: 7, 2: 17, 3: 2}\n",
            "TOTAL EPOCH: 15402, Episode: 211, Epoch: 80, Loss: 0.004496071487665176, Mean Reward: 0.8125, Actions in batch: {0: 3, 1: 8, 2: 17, 3: 4}\n",
            "TOTAL EPOCH: 15422, Episode: 212, Epoch: 0, Loss: 0.003068782389163971, Mean Reward: 2.25, Actions in batch: {1: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 15442, Episode: 212, Epoch: 20, Loss: 9.106017387239262e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 15462, Episode: 212, Epoch: 40, Loss: 0.0004498907073866576, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 15482, Episode: 212, Epoch: 60, Loss: 0.0003275424533057958, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 5, 2: 20, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 15486, Episode: 213, Epoch: 0, Loss: 0.0021878450643271208, Mean Reward: 2.40625, Actions in batch: {0: 1, 1: 4, 2: 16, 3: 11}\n",
            "TOTAL EPOCH: 15506, Episode: 213, Epoch: 20, Loss: 0.004556007217615843, Mean Reward: 0.625, Actions in batch: {0: 3, 1: 6, 2: 17, 3: 6}\n",
            "TOTAL EPOCH: 15526, Episode: 213, Epoch: 40, Loss: 3.277069845353253e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 15546, Episode: 213, Epoch: 60, Loss: 3.103941708104685e-05, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 3, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 15566, Episode: 213, Epoch: 80, Loss: 4.379320307634771e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 1, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 15586, Episode: 214, Epoch: 0, Loss: 0.00638682022690773, Mean Reward: 0.28125, Actions in batch: {0: 6, 1: 1, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 15606, Episode: 214, Epoch: 20, Loss: 0.0007119348738342524, Mean Reward: 3.65625, Actions in batch: {0: 3, 1: 4, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 15626, Episode: 214, Epoch: 40, Loss: 0.002201022347435355, Mean Reward: 2.53125, Actions in batch: {0: 1, 1: 4, 2: 17, 3: 10}\n",
            "TOTAL EPOCH: 15646, Episode: 214, Epoch: 60, Loss: 0.002332150237634778, Mean Reward: 1.5, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 15666, Episode: 214, Epoch: 80, Loss: 0.001374300685711205, Mean Reward: 0.46875, Actions in batch: {0: 5, 1: 9, 2: 11, 3: 7}\n",
            "TOTAL EPOCH: 15686, Episode: 215, Epoch: 0, Loss: 0.00016296160174533725, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 15706, Episode: 215, Epoch: 20, Loss: 0.007332406472414732, Mean Reward: 0.4375, Actions in batch: {0: 4, 1: 2, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 15726, Episode: 215, Epoch: 40, Loss: 0.00045811114250682294, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 15746, Episode: 215, Epoch: 60, Loss: 0.0009639992495067418, Mean Reward: 2.84375, Actions in batch: {0: 2, 1: 1, 2: 29}\n",
            "TOTAL EPOCH: 15766, Episode: 215, Epoch: 80, Loss: 2.3092565243132412e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 15786, Episode: 216, Epoch: 0, Loss: 0.00256241369061172, Mean Reward: 1.4375, Actions in batch: {0: 5, 1: 4, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 15806, Episode: 216, Epoch: 20, Loss: 8.913058991311118e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 6, 2: 19, 3: 3}\n",
            "TOTAL EPOCH: 15826, Episode: 216, Epoch: 40, Loss: 2.226045216957573e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 6, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 15846, Episode: 216, Epoch: 60, Loss: 7.716445543337613e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 6, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 15866, Episode: 216, Epoch: 80, Loss: 0.00250518461689353, Mean Reward: 2.65625, Actions in batch: {0: 1, 1: 5, 2: 18, 3: 8}\n",
            "TOTAL EPOCH: 15886, Episode: 217, Epoch: 0, Loss: 0.0012003518640995026, Mean Reward: 1.96875, Actions in batch: {0: 2, 1: 7, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 15906, Episode: 217, Epoch: 20, Loss: 0.012009208090603352, Mean Reward: 0.15625, Actions in batch: {0: 1, 1: 11, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 15926, Episode: 217, Epoch: 40, Loss: 0.00011939874821109697, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 15946, Episode: 217, Epoch: 60, Loss: 0.010909260250627995, Mean Reward: 0.15625, Actions in batch: {0: 6, 1: 6, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 15966, Episode: 217, Epoch: 80, Loss: 0.0029382978100329638, Mean Reward: 0.9375, Actions in batch: {0: 7, 1: 3, 2: 19, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 15973, Episode: 218, Epoch: 0, Loss: 0.0021891826763749123, Mean Reward: 2.25, Actions in batch: {0: 5, 1: 4, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 15993, Episode: 218, Epoch: 20, Loss: 0.004689684137701988, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 4, 2: 23}\n",
            "TOTAL EPOCH: 16013, Episode: 218, Epoch: 40, Loss: 0.0006049070507287979, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 8, 2: 16, 3: 4}\n",
            "TOTAL EPOCH: 16033, Episode: 218, Epoch: 60, Loss: 0.000697821204084903, Mean Reward: 2.0625, Actions in batch: {0: 5, 1: 3, 2: 20, 3: 4}\n",
            "TOTAL EPOCH: 16053, Episode: 218, Epoch: 80, Loss: 0.0035075610503554344, Mean Reward: 1.375, Actions in batch: {0: 2, 1: 3, 2: 26, 3: 1}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 16059, Episode: 219, Epoch: 0, Loss: 0.000110728302388452, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 26}\n",
            "TOTAL EPOCH: 16079, Episode: 219, Epoch: 20, Loss: 0.0003123034257441759, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 16099, Episode: 219, Epoch: 40, Loss: 6.93736074026674e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 18, 3: 7}\n",
            "TOTAL EPOCH: 16119, Episode: 219, Epoch: 60, Loss: 0.0009263874962925911, Mean Reward: 3.59375, Actions in batch: {0: 6, 1: 2, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 16139, Episode: 219, Epoch: 80, Loss: 0.0025476368609815836, Mean Reward: 1.3125, Actions in batch: {0: 7, 1: 3, 2: 21, 3: 1}\n",
            "TOTAL EPOCH: 16159, Episode: 220, Epoch: 0, Loss: 6.58110411677626e-06, Mean Reward: 0.0, Actions in batch: {2: 29, 3: 3}\n",
            "TOTAL EPOCH: 16179, Episode: 220, Epoch: 20, Loss: 0.003039376577362418, Mean Reward: 3.40625, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 16199, Episode: 220, Epoch: 40, Loss: 0.0033793761394917965, Mean Reward: 1.6875, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 16219, Episode: 220, Epoch: 60, Loss: 2.4036560716922395e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 23, 3: 5}\n",
            "TOTAL EPOCH: 16239, Episode: 220, Epoch: 80, Loss: 0.0003756728838197887, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 16259, Episode: 221, Epoch: 0, Loss: 0.01416079606860876, Mean Reward: 0.28125, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 16279, Episode: 221, Epoch: 20, Loss: 0.0029044027905911207, Mean Reward: 1.25, Actions in batch: {0: 9, 1: 5, 2: 16, 3: 2}\n",
            "TOTAL EPOCH: 16299, Episode: 221, Epoch: 40, Loss: 1.3259706975077279e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 19, 3: 8}\n",
            "TOTAL EPOCH: 16319, Episode: 221, Epoch: 60, Loss: 0.00259975902736187, Mean Reward: 3.03125, Actions in batch: {0: 1, 1: 5, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 16339, Episode: 221, Epoch: 80, Loss: 6.7010037128056865e-06, Mean Reward: 0.8125, Actions in batch: {0: 3, 1: 3, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 16359, Episode: 222, Epoch: 0, Loss: 0.00018599940813146532, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 16379, Episode: 222, Epoch: 20, Loss: 0.0031182547099888325, Mean Reward: 1.1875, Actions in batch: {0: 4, 1: 1, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 16399, Episode: 222, Epoch: 40, Loss: 0.0034828910138458014, Mean Reward: 2.3125, Actions in batch: {0: 3, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 16419, Episode: 222, Epoch: 60, Loss: 0.004881934728473425, Mean Reward: 0.75, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 16439, Episode: 222, Epoch: 80, Loss: 0.0027774989139288664, Mean Reward: 2.625, Actions in batch: {1: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 16459, Episode: 223, Epoch: 0, Loss: 0.00098656234331429, Mean Reward: 2.28125, Actions in batch: {0: 1, 1: 4, 2: 21, 3: 6}\n",
            "TOTAL EPOCH: 16479, Episode: 223, Epoch: 20, Loss: 0.001006175996735692, Mean Reward: 2.09375, Actions in batch: {0: 1, 1: 6, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 16499, Episode: 223, Epoch: 40, Loss: 2.4421653506578878e-05, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 30}\n",
            "TOTAL EPOCH: 16519, Episode: 223, Epoch: 60, Loss: 0.019071955233812332, Mean Reward: 0.03125, Actions in batch: {1: 6, 2: 23, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 16533, Episode: 224, Epoch: 0, Loss: 6.418929115170613e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 5, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 16553, Episode: 224, Epoch: 20, Loss: 2.8477470550569706e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 16573, Episode: 224, Epoch: 40, Loss: 0.005480839870870113, Mean Reward: 1.71875, Actions in batch: {0: 2, 1: 4, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 16593, Episode: 224, Epoch: 60, Loss: 0.0005445965216495097, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 16613, Episode: 224, Epoch: 80, Loss: 0.0008620806038379669, Mean Reward: 3.625, Actions in batch: {0: 6, 1: 2, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 16633, Episode: 225, Epoch: 0, Loss: 0.0004569874727167189, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 16653, Episode: 225, Epoch: 20, Loss: 0.0006387241883203387, Mean Reward: 2.125, Actions in batch: {0: 4, 1: 4, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 16673, Episode: 225, Epoch: 40, Loss: 0.001846278551965952, Mean Reward: 1.71875, Actions in batch: {1: 5, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 16693, Episode: 225, Epoch: 60, Loss: 0.0044194720685482025, Mean Reward: 0.90625, Actions in batch: {0: 5, 1: 5, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 16713, Episode: 225, Epoch: 80, Loss: 0.0009123861091211438, Mean Reward: 3.53125, Actions in batch: {0: 3, 1: 4, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 16733, Episode: 226, Epoch: 0, Loss: 0.002965440507978201, Mean Reward: 1.0625, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 16753, Episode: 226, Epoch: 20, Loss: 0.0033611415419727564, Mean Reward: 0.53125, Actions in batch: {0: 5, 1: 6, 2: 15, 3: 6}\n",
            "TOTAL EPOCH: 16773, Episode: 226, Epoch: 40, Loss: 0.0029009797144681215, Mean Reward: 1.28125, Actions in batch: {0: 3, 1: 1, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 16793, Episode: 226, Epoch: 60, Loss: 0.002569593256339431, Mean Reward: 1.75, Actions in batch: {0: 4, 1: 5, 2: 19, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 16807, Episode: 227, Epoch: 0, Loss: 0.004094635136425495, Mean Reward: 0.75, Actions in batch: {0: 3, 1: 6, 2: 16, 3: 7}\n",
            "TOTAL EPOCH: 16827, Episode: 227, Epoch: 20, Loss: 0.0018304930999875069, Mean Reward: 2.65625, Actions in batch: {0: 6, 1: 8, 2: 11, 3: 7}\n",
            "TOTAL EPOCH: 16847, Episode: 227, Epoch: 40, Loss: 0.0007222385611385107, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 20, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 16864, Episode: 228, Epoch: 0, Loss: 0.002762451535090804, Mean Reward: 1.3125, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 16884, Episode: 228, Epoch: 20, Loss: 1.642068582441425e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 16904, Episode: 228, Epoch: 40, Loss: 0.0029091492760926485, Mean Reward: 1.71875, Actions in batch: {0: 2, 1: 5, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 16924, Episode: 228, Epoch: 60, Loss: 0.000233980143093504, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 16944, Episode: 228, Epoch: 80, Loss: 0.0016464270884171128, Mean Reward: 2.34375, Actions in batch: {0: 3, 1: 4, 2: 21, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 16948, Episode: 229, Epoch: 0, Loss: 0.0012695195619016886, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 16968, Episode: 229, Epoch: 20, Loss: 0.0011946907034143806, Mean Reward: 1.875, Actions in batch: {0: 2, 1: 1, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 16988, Episode: 229, Epoch: 40, Loss: 0.0029524846468120813, Mean Reward: 1.375, Actions in batch: {0: 8, 1: 2, 2: 19, 3: 3}\n",
            "TOTAL EPOCH: 17008, Episode: 229, Epoch: 60, Loss: 8.053957571974024e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 17028, Episode: 229, Epoch: 80, Loss: 0.0009225583053193986, Mean Reward: 3.09375, Actions in batch: {0: 6, 1: 4, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 17048, Episode: 230, Epoch: 0, Loss: 6.820570888521615e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 22, 3: 5}\n",
            "TOTAL EPOCH: 17068, Episode: 230, Epoch: 20, Loss: 0.0051690055988729, Mean Reward: 0.5, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 17088, Episode: 230, Epoch: 40, Loss: 1.6044683661675663e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 17108, Episode: 230, Epoch: 60, Loss: 0.0003155595622956753, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 17128, Episode: 230, Epoch: 80, Loss: 0.0012346559669822454, Mean Reward: 2.4375, Actions in batch: {1: 5, 2: 22, 3: 5}\n",
            "TOTAL EPOCH: 17148, Episode: 231, Epoch: 0, Loss: 0.0028164885006844997, Mean Reward: 1.96875, Actions in batch: {0: 5, 1: 4, 2: 22, 3: 1}\n",
            "TOTAL EPOCH: 17168, Episode: 231, Epoch: 20, Loss: 0.015157786197960377, Mean Reward: 0.0625, Actions in batch: {0: 2, 1: 5, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 17188, Episode: 231, Epoch: 40, Loss: 0.007772029843181372, Mean Reward: 0.46875, Actions in batch: {0: 4, 1: 2, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 17208, Episode: 231, Epoch: 60, Loss: 8.956954843597487e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 17228, Episode: 231, Epoch: 80, Loss: 0.0016976292245090008, Mean Reward: 1.625, Actions in batch: {0: 4, 1: 1, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 17248, Episode: 232, Epoch: 0, Loss: 0.00401736656203866, Mean Reward: 1.75, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 17268, Episode: 232, Epoch: 20, Loss: 0.0023808295372873545, Mean Reward: 1.6875, Actions in batch: {0: 1, 1: 3, 2: 21, 3: 7}\n",
            "TOTAL EPOCH: 17288, Episode: 232, Epoch: 40, Loss: 2.0821682937821606e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 23, 3: 5}\n",
            "TOTAL EPOCH: 17308, Episode: 232, Epoch: 60, Loss: 0.00013334910909179598, Mean Reward: 1.6875, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 17328, Episode: 232, Epoch: 80, Loss: 0.006040837150067091, Mean Reward: 0.28125, Actions in batch: {0: 4, 1: 9, 2: 13, 3: 6}\n",
            "TOTAL EPOCH: 17348, Episode: 233, Epoch: 0, Loss: 5.397691893449519e-06, Mean Reward: 0.0, Actions in batch: {0: 5, 2: 22, 3: 5}\n",
            "TOTAL EPOCH: 17368, Episode: 233, Epoch: 20, Loss: 0.0004308731877245009, Mean Reward: 1.96875, Actions in batch: {0: 4, 1: 2, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 17388, Episode: 233, Epoch: 40, Loss: 0.0014625953044742346, Mean Reward: 2.34375, Actions in batch: {0: 2, 1: 3, 2: 18, 3: 9}\n",
            "TOTAL EPOCH: 17408, Episode: 233, Epoch: 60, Loss: 0.0024282943923026323, Mean Reward: 2.8125, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 17428, Episode: 233, Epoch: 80, Loss: 0.005444425158202648, Mean Reward: 0.625, Actions in batch: {0: 2, 1: 5, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 17448, Episode: 234, Epoch: 0, Loss: 0.003456033067777753, Mean Reward: 1.125, Actions in batch: {0: 6, 1: 3, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 17468, Episode: 234, Epoch: 20, Loss: 0.005940462462604046, Mean Reward: 0.5, Actions in batch: {0: 3, 1: 3, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 17488, Episode: 234, Epoch: 40, Loss: 0.01500184740871191, Mean Reward: 0.1875, Actions in batch: {1: 3, 2: 29}\n",
            "TOTAL EPOCH: 17508, Episode: 234, Epoch: 60, Loss: 3.003809615620412e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 23, 3: 6}\n",
            "TOTAL EPOCH: 17528, Episode: 234, Epoch: 80, Loss: 0.0059134745970368385, Mean Reward: 0.9375, Actions in batch: {0: 1, 1: 2, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 17548, Episode: 235, Epoch: 0, Loss: 1.7408752910341718e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 5, 2: 15, 3: 8}\n",
            "TOTAL EPOCH: 17568, Episode: 235, Epoch: 20, Loss: 0.0020838966593146324, Mean Reward: 3.1875, Actions in batch: {0: 6, 1: 3, 2: 21, 3: 2}\n",
            "TOTAL EPOCH: 17588, Episode: 235, Epoch: 40, Loss: 5.4415493650594726e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 17608, Episode: 235, Epoch: 60, Loss: 0.003470285329967737, Mean Reward: 1.0625, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 17628, Episode: 235, Epoch: 80, Loss: 0.003403829410672188, Mean Reward: 3.0625, Actions in batch: {0: 2, 1: 4, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 17648, Episode: 236, Epoch: 0, Loss: 0.003545641666278243, Mean Reward: 1.1875, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 17668, Episode: 236, Epoch: 20, Loss: 2.8047066734870896e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 17688, Episode: 236, Epoch: 40, Loss: 0.003082737559452653, Mean Reward: 2.4375, Actions in batch: {0: 1, 1: 5, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 17708, Episode: 236, Epoch: 60, Loss: 2.4404998839600012e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 17728, Episode: 236, Epoch: 80, Loss: 0.00015292063471861184, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 26, 3: 1}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 17747, Episode: 237, Epoch: 0, Loss: 0.00306845735758543, Mean Reward: 1.3125, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 17767, Episode: 237, Epoch: 20, Loss: 0.0001878715120255947, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 17787, Episode: 237, Epoch: 40, Loss: 0.0022446811199188232, Mean Reward: 2.6875, Actions in batch: {0: 6, 1: 1, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 17807, Episode: 237, Epoch: 60, Loss: 0.00047591354814358056, Mean Reward: 0.875, Actions in batch: {0: 4, 1: 5, 2: 18, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 17824, Episode: 238, Epoch: 0, Loss: 0.002964174607768655, Mean Reward: 3.0, Actions in batch: {0: 1, 1: 4, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 17844, Episode: 238, Epoch: 20, Loss: 0.002173877554014325, Mean Reward: 1.71875, Actions in batch: {0: 5, 1: 3, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 17864, Episode: 238, Epoch: 40, Loss: 3.089918027399108e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 17884, Episode: 238, Epoch: 60, Loss: 0.0004406411899253726, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 17904, Episode: 238, Epoch: 80, Loss: 0.0003185023961123079, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 17924, Episode: 239, Epoch: 0, Loss: 7.956886111060157e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 17944, Episode: 239, Epoch: 20, Loss: 0.0006048196810297668, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 1, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 17964, Episode: 239, Epoch: 40, Loss: 0.006284656003117561, Mean Reward: 0.75, Actions in batch: {0: 2, 1: 3, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 17984, Episode: 239, Epoch: 60, Loss: 0.00018262852972839028, Mean Reward: 2.0, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 18004, Episode: 239, Epoch: 80, Loss: 0.004900713451206684, Mean Reward: 0.75, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 18024, Episode: 240, Epoch: 0, Loss: 0.002039477927610278, Mean Reward: 0.8125, Actions in batch: {0: 1, 1: 3, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 18044, Episode: 240, Epoch: 20, Loss: 0.00018391069897916168, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 18064, Episode: 240, Epoch: 40, Loss: 0.0010096004698425531, Mean Reward: 3.03125, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 18084, Episode: 240, Epoch: 60, Loss: 0.00018637022003531456, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 18104, Episode: 240, Epoch: 80, Loss: 0.0022737879771739244, Mean Reward: 1.90625, Actions in batch: {0: 7, 1: 5, 2: 15, 3: 5}\n",
            "TOTAL EPOCH: 18124, Episode: 241, Epoch: 0, Loss: 0.01613110676407814, Mean Reward: 0.0625, Actions in batch: {1: 8, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 18144, Episode: 241, Epoch: 20, Loss: 0.0001872782886493951, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 2, 2: 19, 3: 3}\n",
            "TOTAL EPOCH: 18164, Episode: 241, Epoch: 40, Loss: 0.0027255434542894363, Mean Reward: 1.3125, Actions in batch: {0: 5, 1: 4, 2: 18, 3: 5}\n",
            "TOTAL EPOCH: 18184, Episode: 241, Epoch: 60, Loss: 0.0028892389964312315, Mean Reward: 0.34375, Actions in batch: {0: 5, 1: 2, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 18204, Episode: 241, Epoch: 80, Loss: 0.0002954912488348782, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 18210, Episode: 242, Epoch: 0, Loss: 0.0041504548862576485, Mean Reward: 0.6875, Actions in batch: {0: 1, 1: 4, 2: 20, 3: 7}\n",
            "TOTAL EPOCH: 18230, Episode: 242, Epoch: 20, Loss: 3.982782072853297e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 18250, Episode: 242, Epoch: 40, Loss: 0.004968436434864998, Mean Reward: 1.5625, Actions in batch: {0: 4, 1: 6, 2: 18, 3: 4}\n",
            "TOTAL EPOCH: 18270, Episode: 242, Epoch: 60, Loss: 0.004463822580873966, Mean Reward: 0.3125, Actions in batch: {0: 1, 1: 1, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 18290, Episode: 242, Epoch: 80, Loss: 0.002731557469815016, Mean Reward: 1.3125, Actions in batch: {0: 4, 1: 4, 2: 17, 3: 7}\n",
            "TOTAL EPOCH: 18310, Episode: 243, Epoch: 0, Loss: 0.0019191679311916232, Mean Reward: 1.8125, Actions in batch: {0: 1, 2: 28, 3: 3}\n",
            "TOTAL EPOCH: 18330, Episode: 243, Epoch: 20, Loss: 0.007871036417782307, Mean Reward: 0.21875, Actions in batch: {0: 2, 1: 5, 2: 19, 3: 6}\n",
            "TOTAL EPOCH: 18350, Episode: 243, Epoch: 40, Loss: 1.8720181742537534e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 18370, Episode: 243, Epoch: 60, Loss: 0.0008306035888381302, Mean Reward: 3.0, Actions in batch: {0: 3, 1: 5, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 18390, Episode: 243, Epoch: 80, Loss: 0.0036259416956454515, Mean Reward: 1.15625, Actions in batch: {0: 5, 1: 3, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 18410, Episode: 244, Epoch: 0, Loss: 0.00011699121387209743, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 5, 2: 20, 3: 2}\n",
            "TOTAL EPOCH: 18430, Episode: 244, Epoch: 20, Loss: 0.005409929435700178, Mean Reward: 0.25, Actions in batch: {0: 2, 1: 1, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 18450, Episode: 244, Epoch: 40, Loss: 0.0003134047146886587, Mean Reward: 2.3125, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 18470, Episode: 244, Epoch: 60, Loss: 0.001635548658668995, Mean Reward: 1.9375, Actions in batch: {0: 5, 1: 3, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 18490, Episode: 244, Epoch: 80, Loss: 0.0020893802866339684, Mean Reward: 2.03125, Actions in batch: {1: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 18510, Episode: 245, Epoch: 0, Loss: 8.592578524257988e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 18530, Episode: 245, Epoch: 20, Loss: 0.00011902341793756932, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 18550, Episode: 245, Epoch: 40, Loss: 3.6442237615119666e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 18570, Episode: 245, Epoch: 60, Loss: 2.1574503250576527e-07, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 25, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 18582, Episode: 246, Epoch: 0, Loss: 0.0012384600704535842, Mean Reward: 3.15625, Actions in batch: {0: 3, 1: 3, 2: 19, 3: 7}\n",
            "TOTAL EPOCH: 18602, Episode: 246, Epoch: 20, Loss: 1.932294253492728e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 22, 3: 6}\n",
            "TOTAL EPOCH: 18622, Episode: 246, Epoch: 40, Loss: 0.0034261057153344154, Mean Reward: 1.0, Actions in batch: {0: 1, 1: 1, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 18642, Episode: 246, Epoch: 60, Loss: 0.0002631586685311049, Mean Reward: 2.34375, Actions in batch: {0: 2, 1: 2, 2: 28}\n",
            "TOTAL EPOCH: 18662, Episode: 246, Epoch: 80, Loss: 4.323741450207308e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 20, 3: 4}\n",
            "TOTAL EPOCH: 18682, Episode: 247, Epoch: 0, Loss: 0.0027868880424648523, Mean Reward: 1.21875, Actions in batch: {0: 2, 1: 6, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 18702, Episode: 247, Epoch: 20, Loss: 0.0023383875377476215, Mean Reward: 1.46875, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 18722, Episode: 247, Epoch: 40, Loss: 1.0237001333734952e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 18742, Episode: 247, Epoch: 60, Loss: 1.0309715435141698e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 18762, Episode: 247, Epoch: 80, Loss: 0.0015237508341670036, Mean Reward: 2.3125, Actions in batch: {0: 3, 1: 3, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 18782, Episode: 248, Epoch: 0, Loss: 0.0035001439973711967, Mean Reward: 1.46875, Actions in batch: {0: 1, 1: 4, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 18802, Episode: 248, Epoch: 20, Loss: 0.00011948866449529305, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 18822, Episode: 248, Epoch: 40, Loss: 5.2076924475841224e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 18842, Episode: 248, Epoch: 60, Loss: 0.0053769927471876144, Mean Reward: 0.4375, Actions in batch: {0: 1, 1: 6, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 18862, Episode: 248, Epoch: 80, Loss: 1.7313470380031504e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 25, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 18872, Episode: 249, Epoch: 0, Loss: 8.105047163553536e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 18892, Episode: 249, Epoch: 20, Loss: 0.003329403465613723, Mean Reward: 1.125, Actions in batch: {0: 2, 1: 6, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 18912, Episode: 249, Epoch: 40, Loss: 0.004023380111902952, Mean Reward: 0.6875, Actions in batch: {0: 4, 1: 3, 2: 20, 3: 5}\n",
            "TOTAL EPOCH: 18932, Episode: 249, Epoch: 60, Loss: 0.0010618931846693158, Mean Reward: 2.78125, Actions in batch: {0: 4, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 18952, Episode: 249, Epoch: 80, Loss: 0.0015647741965949535, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 21, 3: 7}\n",
            "TOTAL EPOCH: 18972, Episode: 250, Epoch: 0, Loss: 0.000630018999800086, Mean Reward: 2.625, Actions in batch: {0: 4, 1: 5, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 18992, Episode: 250, Epoch: 20, Loss: 0.012243540957570076, Mean Reward: 0.15625, Actions in batch: {0: 2, 1: 1, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 19012, Episode: 250, Epoch: 40, Loss: 0.0015439142007380724, Mean Reward: 2.96875, Actions in batch: {0: 1, 1: 2, 2: 20, 3: 9}\n",
            "TOTAL EPOCH: 19032, Episode: 250, Epoch: 60, Loss: 0.004937090910971165, Mean Reward: 0.625, Actions in batch: {0: 1, 1: 2, 2: 29}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 19042, Episode: 251, Epoch: 0, Loss: 3.704725531861186e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 19062, Episode: 251, Epoch: 20, Loss: 0.004215477034449577, Mean Reward: 0.28125, Actions in batch: {0: 3, 1: 1, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 19082, Episode: 251, Epoch: 40, Loss: 0.0016757289413362741, Mean Reward: 2.25, Actions in batch: {1: 6, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 19102, Episode: 251, Epoch: 60, Loss: 5.697920641978271e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 19122, Episode: 251, Epoch: 80, Loss: 9.147032687906176e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 19142, Episode: 252, Epoch: 0, Loss: 0.0022357385605573654, Mean Reward: 1.25, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 19162, Episode: 252, Epoch: 20, Loss: 0.0010828351369127631, Mean Reward: 4.03125, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 19182, Episode: 252, Epoch: 40, Loss: 4.398030455377011e-07, Mean Reward: 0.0, Actions in batch: {2: 32}\n",
            "TOTAL EPOCH: 19202, Episode: 252, Epoch: 60, Loss: 1.9518691260600463e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 5, 2: 21, 3: 2}\n",
            "TOTAL EPOCH: 19222, Episode: 252, Epoch: 80, Loss: 1.6661837207720964e-06, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 1, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 19242, Episode: 253, Epoch: 0, Loss: 0.0003487079229671508, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 21, 3: 6}\n",
            "TOTAL EPOCH: 19262, Episode: 253, Epoch: 20, Loss: 2.0617223981389543e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 19282, Episode: 253, Epoch: 40, Loss: 2.3118086573958863e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 5, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 19302, Episode: 253, Epoch: 60, Loss: 0.002607515314593911, Mean Reward: 2.65625, Actions in batch: {0: 3, 1: 6, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 19322, Episode: 253, Epoch: 80, Loss: 1.815396717574913e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 22, 3: 1}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 19333, Episode: 254, Epoch: 0, Loss: 0.005200234241783619, Mean Reward: 0.5, Actions in batch: {0: 3, 1: 5, 2: 19, 3: 5}\n",
            "TOTAL EPOCH: 19353, Episode: 254, Epoch: 20, Loss: 0.0018750152084976435, Mean Reward: 1.0625, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 19373, Episode: 254, Epoch: 40, Loss: 4.3366021884594375e-08, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 19393, Episode: 254, Epoch: 60, Loss: 3.5383592944526754e-07, Mean Reward: 0.0, Actions in batch: {1: 3, 2: 29}\n",
            "TOTAL EPOCH: 19413, Episode: 254, Epoch: 80, Loss: 0.004054243676364422, Mean Reward: 0.9375, Actions in batch: {0: 4, 2: 22, 3: 6}\n",
            "TOTAL EPOCH: 19433, Episode: 255, Epoch: 0, Loss: 0.008880040608346462, Mean Reward: 0.1875, Actions in batch: {0: 2, 1: 4, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 19453, Episode: 255, Epoch: 20, Loss: 0.00026805378729477525, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 19473, Episode: 255, Epoch: 40, Loss: 0.0018247503321617842, Mean Reward: 1.65625, Actions in batch: {1: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 19493, Episode: 255, Epoch: 60, Loss: 0.007531046401709318, Mean Reward: 0.5625, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 19513, Episode: 255, Epoch: 80, Loss: 0.003647593781352043, Mean Reward: 1.65625, Actions in batch: {0: 5, 1: 2, 2: 23, 3: 2}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 19529, Episode: 256, Epoch: 0, Loss: 0.00012191251153126359, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 19549, Episode: 256, Epoch: 20, Loss: 7.766221301608311e-07, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 4, 2: 19, 3: 6}\n",
            "TOTAL EPOCH: 19569, Episode: 256, Epoch: 40, Loss: 0.00014092428318690509, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 19589, Episode: 256, Epoch: 60, Loss: 4.6562225179513916e-05, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 4, 2: 19, 3: 4}\n",
            "TOTAL EPOCH: 19609, Episode: 256, Epoch: 80, Loss: 0.002421368146315217, Mean Reward: 1.15625, Actions in batch: {0: 2, 1: 8, 2: 20, 3: 2}\n",
            "TOTAL EPOCH: 19629, Episode: 257, Epoch: 0, Loss: 0.0030852528288960457, Mean Reward: 1.0625, Actions in batch: {0: 1, 1: 2, 2: 29}\n",
            "TOTAL EPOCH: 19649, Episode: 257, Epoch: 20, Loss: 0.0016654501669108868, Mean Reward: 2.21875, Actions in batch: {0: 3, 1: 1, 2: 28}\n",
            "TOTAL EPOCH: 19669, Episode: 257, Epoch: 40, Loss: 0.003881952026858926, Mean Reward: 1.0, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 19689, Episode: 257, Epoch: 60, Loss: 0.0010872189886868, Mean Reward: 2.4375, Actions in batch: {1: 5, 2: 22, 3: 5}\n",
            "TOTAL EPOCH: 19709, Episode: 257, Epoch: 80, Loss: 0.00033450324553996325, Mean Reward: 2.0, Actions in batch: {0: 4, 1: 4, 2: 20, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 19715, Episode: 258, Epoch: 0, Loss: 6.728459266014397e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 19735, Episode: 258, Epoch: 20, Loss: 0.0015088506042957306, Mean Reward: 0.875, Actions in batch: {0: 2, 1: 3, 2: 19, 3: 8}\n",
            "TOTAL EPOCH: 19755, Episode: 258, Epoch: 40, Loss: 9.32256443775259e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 19775, Episode: 258, Epoch: 60, Loss: 0.0034805727191269398, Mean Reward: 0.90625, Actions in batch: {0: 3, 1: 1, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 19795, Episode: 258, Epoch: 80, Loss: 4.936218829243444e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 19815, Episode: 259, Epoch: 0, Loss: 0.008968018926680088, Mean Reward: 0.78125, Actions in batch: {0: 1, 1: 2, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 19835, Episode: 259, Epoch: 20, Loss: 0.004200308118015528, Mean Reward: 0.5, Actions in batch: {1: 3, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 19855, Episode: 259, Epoch: 40, Loss: 0.0007651825435459614, Mean Reward: 0.8125, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 19875, Episode: 259, Epoch: 60, Loss: 0.0024549236986786127, Mean Reward: 2.0, Actions in batch: {0: 1, 2: 27, 3: 4}\n",
            "TOTAL EPOCH: 19895, Episode: 259, Epoch: 80, Loss: 0.00025359875871799886, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 23, 3: 1}\n",
            "TOTAL EPOCH: 19915, Episode: 260, Epoch: 0, Loss: 0.0012894923565909266, Mean Reward: 3.09375, Actions in batch: {0: 2, 1: 3, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 19935, Episode: 260, Epoch: 20, Loss: 7.216891390271485e-05, Mean Reward: 0.0, Actions in batch: {1: 5, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 19955, Episode: 260, Epoch: 40, Loss: 0.00016129342839121819, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 19975, Episode: 260, Epoch: 60, Loss: 0.00012215050810482353, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 18, 3: 8}\n",
            "TOTAL EPOCH: 19995, Episode: 260, Epoch: 80, Loss: 0.003036655019968748, Mean Reward: 1.25, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 20015, Episode: 261, Epoch: 0, Loss: 4.637935489881784e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 20035, Episode: 261, Epoch: 20, Loss: 0.004860102664679289, Mean Reward: 0.5625, Actions in batch: {0: 2, 1: 1, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 20055, Episode: 261, Epoch: 40, Loss: 0.0033629382960498333, Mean Reward: 1.5625, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 20075, Episode: 261, Epoch: 60, Loss: 0.00010093684250023216, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 20095, Episode: 261, Epoch: 80, Loss: 5.361099465517327e-05, Mean Reward: 1.6875, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 20115, Episode: 262, Epoch: 0, Loss: 0.0009549299720674753, Mean Reward: 2.0, Actions in batch: {0: 1, 1: 2, 2: 29}\n",
            "TOTAL EPOCH: 20135, Episode: 262, Epoch: 20, Loss: 2.2167587303556502e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 6, 2: 18, 3: 6}\n",
            "TOTAL EPOCH: 20155, Episode: 262, Epoch: 40, Loss: 2.229599886049982e-05, Mean Reward: 0.0, Actions in batch: {2: 31, 3: 1}\n",
            "TOTAL EPOCH: 20175, Episode: 262, Epoch: 60, Loss: 0.00011653517867671326, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 20195, Episode: 262, Epoch: 80, Loss: 0.00012717454228550196, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 30, 3: 1}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 20203, Episode: 263, Epoch: 0, Loss: 2.513096660550218e-06, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 20223, Episode: 263, Epoch: 20, Loss: 0.003943471238017082, Mean Reward: 2.0625, Actions in batch: {0: 4, 1: 6, 2: 21, 3: 1}\n",
            "TOTAL EPOCH: 20243, Episode: 263, Epoch: 40, Loss: 1.2846752724726684e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 20263, Episode: 263, Epoch: 60, Loss: 9.984905773308128e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 26, 3: 5}\n",
            "TOTAL EPOCH: 20283, Episode: 263, Epoch: 80, Loss: 8.256286673713475e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 20303, Episode: 264, Epoch: 0, Loss: 3.9962149458006024e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 20323, Episode: 264, Epoch: 20, Loss: 0.003269190900027752, Mean Reward: 1.21875, Actions in batch: {0: 5, 1: 3, 2: 22, 3: 2}\n",
            "TOTAL EPOCH: 20343, Episode: 264, Epoch: 40, Loss: 0.00014995796664152294, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 20363, Episode: 264, Epoch: 60, Loss: 0.0007008759421296418, Mean Reward: 2.78125, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 20383, Episode: 264, Epoch: 80, Loss: 2.2044270735932514e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 29}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 20396, Episode: 265, Epoch: 0, Loss: 0.003186834277585149, Mean Reward: 3.34375, Actions in batch: {0: 2, 1: 2, 2: 23, 3: 5}\n",
            "TOTAL EPOCH: 20416, Episode: 265, Epoch: 20, Loss: 0.00014303276839200407, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 20436, Episode: 265, Epoch: 40, Loss: 7.3460441853967495e-06, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 3, 2: 20, 3: 3}\n",
            "TOTAL EPOCH: 20456, Episode: 265, Epoch: 60, Loss: 1.8164168068324216e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 2: 23, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 20471, Episode: 266, Epoch: 0, Loss: 0.007467299699783325, Mean Reward: 0.21875, Actions in batch: {0: 1, 1: 4, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 20491, Episode: 266, Epoch: 20, Loss: 6.733799091307446e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 20511, Episode: 266, Epoch: 40, Loss: 0.0004659916739910841, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 2, 2: 28}\n",
            "TOTAL EPOCH: 20531, Episode: 266, Epoch: 60, Loss: 1.5900721336947754e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 29}\n",
            "TOTAL EPOCH: 20551, Episode: 266, Epoch: 80, Loss: 3.27814291267714e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 20571, Episode: 267, Epoch: 0, Loss: 6.1787181948602665e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 20591, Episode: 267, Epoch: 20, Loss: 0.00014443934196606278, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 21, 3: 6}\n",
            "TOTAL EPOCH: 20611, Episode: 267, Epoch: 40, Loss: 2.895630314014852e-05, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 20631, Episode: 267, Epoch: 60, Loss: 0.0001066315162461251, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 23, 3: 6}\n",
            "TOTAL EPOCH: 20651, Episode: 267, Epoch: 80, Loss: 2.3743858037050813e-05, Mean Reward: 0.0, Actions in batch: {1: 3, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 20671, Episode: 268, Epoch: 0, Loss: 3.627557362051448e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 20691, Episode: 268, Epoch: 20, Loss: 1.2465632607927546e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 6, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 20711, Episode: 268, Epoch: 40, Loss: 0.004942616913467646, Mean Reward: 0.40625, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 20731, Episode: 268, Epoch: 60, Loss: 0.003123805159702897, Mean Reward: 2.78125, Actions in batch: {0: 1, 1: 1, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 20751, Episode: 268, Epoch: 80, Loss: 9.290031812270172e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 25, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 20760, Episode: 269, Epoch: 0, Loss: 6.355116056511179e-05, Mean Reward: 2.8125, Actions in batch: {0: 2, 1: 6, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 20780, Episode: 269, Epoch: 20, Loss: 0.0002312354336027056, Mean Reward: 0.0, Actions in batch: {2: 31, 3: 1}\n",
            "TOTAL EPOCH: 20800, Episode: 269, Epoch: 40, Loss: 0.005539825651794672, Mean Reward: 2.28125, Actions in batch: {1: 6, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 20820, Episode: 269, Epoch: 60, Loss: 7.239067053887993e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 20840, Episode: 269, Epoch: 80, Loss: 4.621416883310303e-05, Mean Reward: 1.875, Actions in batch: {0: 2, 1: 2, 2: 28}\n",
            "TOTAL EPOCH: 20860, Episode: 270, Epoch: 0, Loss: 6.27352565061301e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 20880, Episode: 270, Epoch: 20, Loss: 0.002851465716958046, Mean Reward: 2.125, Actions in batch: {0: 1, 1: 5, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 20900, Episode: 270, Epoch: 40, Loss: 0.00012012675870209932, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 25, 3: 4}\n",
            "TOTAL EPOCH: 20920, Episode: 270, Epoch: 60, Loss: 8.638004510430619e-05, Mean Reward: 0.0, Actions in batch: {1: 1, 2: 26, 3: 5}\n",
            "TOTAL EPOCH: 20940, Episode: 270, Epoch: 80, Loss: 0.0023642159067094326, Mean Reward: 2.5, Actions in batch: {1: 3, 2: 26, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 20948, Episode: 271, Epoch: 0, Loss: 0.001140702166594565, Mean Reward: 2.78125, Actions in batch: {0: 1, 1: 3, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 20968, Episode: 271, Epoch: 20, Loss: 0.00026438519125804305, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 20988, Episode: 271, Epoch: 40, Loss: 0.00012897184933535755, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 21008, Episode: 271, Epoch: 60, Loss: 0.00039040183764882386, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 21028, Episode: 271, Epoch: 80, Loss: 2.22840353671927e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 21048, Episode: 272, Epoch: 0, Loss: 0.00010887595999520272, Mean Reward: 0.0, Actions in batch: {1: 4, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 21068, Episode: 272, Epoch: 20, Loss: 4.693249866249971e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 21088, Episode: 272, Epoch: 40, Loss: 3.650008511613123e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 21108, Episode: 272, Epoch: 60, Loss: 2.0653010324167553e-06, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 21128, Episode: 272, Epoch: 80, Loss: 2.0915196728310548e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 28}\n",
            "TOTAL EPOCH: 21148, Episode: 273, Epoch: 0, Loss: 1.5326204447774217e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 21168, Episode: 273, Epoch: 20, Loss: 1.8921336959465407e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 21188, Episode: 273, Epoch: 40, Loss: 0.0009783251443877816, Mean Reward: 4.0, Actions in batch: {0: 2, 1: 5, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 21208, Episode: 273, Epoch: 60, Loss: 6.654223398072645e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 21228, Episode: 273, Epoch: 80, Loss: 2.5076416932279244e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 21248, Episode: 274, Epoch: 0, Loss: 1.793797963500765e-07, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 21268, Episode: 274, Epoch: 20, Loss: 0.002452152781188488, Mean Reward: 0.875, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 21288, Episode: 274, Epoch: 40, Loss: 0.00012043816241202876, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 21308, Episode: 274, Epoch: 60, Loss: 5.0763315812218934e-05, Mean Reward: 2.0625, Actions in batch: {0: 4, 1: 3, 2: 24, 3: 1}\n",
            "TOTAL EPOCH: 21328, Episode: 274, Epoch: 80, Loss: 0.00032264459878206253, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 21348, Episode: 275, Epoch: 0, Loss: 0.0012241858057677746, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 21368, Episode: 275, Epoch: 20, Loss: 0.001180528663098812, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 21388, Episode: 275, Epoch: 40, Loss: 0.001649332232773304, Mean Reward: 1.28125, Actions in batch: {0: 2, 1: 4, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 21408, Episode: 275, Epoch: 60, Loss: 0.00019522238289937377, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 21428, Episode: 275, Epoch: 80, Loss: 0.0002134788956027478, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 28}\n",
            "TOTAL EPOCH: 21448, Episode: 276, Epoch: 0, Loss: 8.00940251792781e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 21468, Episode: 276, Epoch: 20, Loss: 3.3172422263305634e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 22, 3: 5}\n",
            "TOTAL EPOCH: 21488, Episode: 276, Epoch: 40, Loss: 0.00446995859965682, Mean Reward: 0.34375, Actions in batch: {1: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 21508, Episode: 276, Epoch: 60, Loss: 0.00010069203563034534, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 30}\n",
            "TOTAL EPOCH: 21528, Episode: 276, Epoch: 80, Loss: 0.003826784435659647, Mean Reward: 1.96875, Actions in batch: {0: 2, 1: 4, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 21548, Episode: 277, Epoch: 0, Loss: 3.1821914490137715e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 21568, Episode: 277, Epoch: 20, Loss: 8.113887452054769e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 21588, Episode: 277, Epoch: 40, Loss: 0.0055775935761630535, Mean Reward: 0.21875, Actions in batch: {1: 1, 2: 30, 3: 1}\n",
            "TOTAL EPOCH: 21608, Episode: 277, Epoch: 60, Loss: 0.003570161759853363, Mean Reward: 0.84375, Actions in batch: {0: 1, 1: 5, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 21628, Episode: 277, Epoch: 80, Loss: 3.9817695096644456e-07, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 21648, Episode: 278, Epoch: 0, Loss: 0.0015358964446932077, Mean Reward: 1.34375, Actions in batch: {0: 2, 1: 1, 2: 24, 3: 5}\n",
            "TOTAL EPOCH: 21668, Episode: 278, Epoch: 20, Loss: 0.0009064387995749712, Mean Reward: 2.0, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 21688, Episode: 278, Epoch: 40, Loss: 0.004765551537275314, Mean Reward: 3.15625, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 21708, Episode: 278, Epoch: 60, Loss: 0.0013497988693416119, Mean Reward: 2.15625, Actions in batch: {0: 2, 1: 4, 2: 22, 3: 4}\n",
            "TOTAL EPOCH: 21728, Episode: 278, Epoch: 80, Loss: 4.823124618269503e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 27, 3: 4}\n",
            "TOTAL EPOCH: 21748, Episode: 279, Epoch: 0, Loss: 0.013654680922627449, Mean Reward: 0.0625, Actions in batch: {0: 3, 1: 2, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 21768, Episode: 279, Epoch: 20, Loss: 0.00026437841006554663, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 2, 2: 22, 3: 3}\n",
            "TOTAL EPOCH: 21788, Episode: 279, Epoch: 40, Loss: 4.763599281432107e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 21808, Episode: 279, Epoch: 60, Loss: 6.241245955607155e-07, Mean Reward: 0.0, Actions in batch: {1: 3, 2: 29}\n",
            "TOTAL EPOCH: 21828, Episode: 279, Epoch: 80, Loss: 0.003195305122062564, Mean Reward: 2.28125, Actions in batch: {0: 2, 1: 4, 2: 21, 3: 5}\n",
            "TOTAL EPOCH: 21848, Episode: 280, Epoch: 0, Loss: 0.00046734086936339736, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 6, 2: 18, 3: 7}\n",
            "TOTAL EPOCH: 21868, Episode: 280, Epoch: 20, Loss: 0.003824732732027769, Mean Reward: 0.8125, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 21888, Episode: 280, Epoch: 40, Loss: 0.00225575128570199, Mean Reward: 2.28125, Actions in batch: {0: 1, 1: 3, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 21908, Episode: 280, Epoch: 60, Loss: 7.084593562467489e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 5, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 21928, Episode: 280, Epoch: 80, Loss: 0.00942376721650362, Mean Reward: 0.15625, Actions in batch: {0: 2, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 21948, Episode: 281, Epoch: 0, Loss: 3.101700485785841e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 27}\n",
            "TOTAL EPOCH: 21968, Episode: 281, Epoch: 20, Loss: 1.647270801186096e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 21988, Episode: 281, Epoch: 40, Loss: 1.807622356864158e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 4, 2: 23, 3: 1}\n",
            "TOTAL EPOCH: 22008, Episode: 281, Epoch: 60, Loss: 0.009205878712236881, Mean Reward: 0.75, Actions in batch: {0: 4, 1: 1, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 22028, Episode: 281, Epoch: 80, Loss: 2.1117641608725535e-06, Mean Reward: 0.0, Actions in batch: {1: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 22048, Episode: 282, Epoch: 0, Loss: 0.002934025600552559, Mean Reward: 1.1875, Actions in batch: {0: 6, 1: 3, 2: 22, 3: 1}\n",
            "TOTAL EPOCH: 22068, Episode: 282, Epoch: 20, Loss: 8.233015250880271e-05, Mean Reward: 0.0, Actions in batch: {1: 3, 2: 29}\n",
            "TOTAL EPOCH: 22088, Episode: 282, Epoch: 40, Loss: 0.0005835446063429117, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 22108, Episode: 282, Epoch: 60, Loss: 6.479089643107727e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 22128, Episode: 282, Epoch: 80, Loss: 3.5486275010043755e-05, Mean Reward: 2.15625, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 22148, Episode: 283, Epoch: 0, Loss: 0.002401367761194706, Mean Reward: 2.4375, Actions in batch: {0: 2, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 22168, Episode: 283, Epoch: 20, Loss: 0.006246708333492279, Mean Reward: 0.25, Actions in batch: {0: 1, 1: 2, 2: 25, 3: 4}\n",
            "TOTAL EPOCH: 22188, Episode: 283, Epoch: 40, Loss: 0.0006885819020681083, Mean Reward: 1.0625, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 22208, Episode: 283, Epoch: 60, Loss: 4.435657319845632e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 22228, Episode: 283, Epoch: 80, Loss: 0.0033480050042271614, Mean Reward: 1.8125, Actions in batch: {0: 1, 1: 5, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 22248, Episode: 284, Epoch: 0, Loss: 0.0007623234414495528, Mean Reward: 1.9375, Actions in batch: {0: 1, 1: 2, 2: 29}\n",
            "TOTAL EPOCH: 22268, Episode: 284, Epoch: 20, Loss: 0.000986405648291111, Mean Reward: 2.59375, Actions in batch: {1: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 22288, Episode: 284, Epoch: 40, Loss: 7.386432844214141e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 4, 2: 27}\n",
            "TOTAL EPOCH: 22308, Episode: 284, Epoch: 60, Loss: 0.0035912441089749336, Mean Reward: 0.84375, Actions in batch: {0: 1, 1: 5, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 22328, Episode: 284, Epoch: 80, Loss: 0.004846750758588314, Mean Reward: 2.25, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 22348, Episode: 285, Epoch: 0, Loss: 9.760151442606002e-05, Mean Reward: 0.0, Actions in batch: {1: 1, 2: 28, 3: 3}\n",
            "TOTAL EPOCH: 22368, Episode: 285, Epoch: 20, Loss: 9.93791691143997e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 28}\n",
            "TOTAL EPOCH: 22388, Episode: 285, Epoch: 40, Loss: 5.312957910064142e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 25, 3: 6}\n",
            "TOTAL EPOCH: 22408, Episode: 285, Epoch: 60, Loss: 0.0008544037118554115, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 4, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 22428, Episode: 285, Epoch: 80, Loss: 3.521175676723942e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 22448, Episode: 286, Epoch: 0, Loss: 0.0014172268565744162, Mean Reward: 2.34375, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 22468, Episode: 286, Epoch: 20, Loss: 0.0015236986801028252, Mean Reward: 1.8125, Actions in batch: {0: 6, 2: 25, 3: 1}\n",
            "TOTAL EPOCH: 22488, Episode: 286, Epoch: 40, Loss: 0.0015987507067620754, Mean Reward: 3.25, Actions in batch: {0: 4, 1: 1, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 22508, Episode: 286, Epoch: 60, Loss: 1.0473828027102172e-08, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 3, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 22528, Episode: 286, Epoch: 80, Loss: 0.013042885810136795, Mean Reward: 0.1875, Actions in batch: {1: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 22548, Episode: 287, Epoch: 0, Loss: 4.624452230928e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 22568, Episode: 287, Epoch: 20, Loss: 1.1258136510150507e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 22588, Episode: 287, Epoch: 40, Loss: 0.0004765136691275984, Mean Reward: 2.875, Actions in batch: {0: 3, 1: 1, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 22608, Episode: 287, Epoch: 60, Loss: 0.0024063163436949253, Mean Reward: 0.28125, Actions in batch: {1: 3, 2: 29}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 22615, Episode: 288, Epoch: 0, Loss: 0.013636279851198196, Mean Reward: 0.125, Actions in batch: {0: 1, 1: 1, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 22635, Episode: 288, Epoch: 20, Loss: 0.00029866251861676574, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 26, 3: 1}\n",
            "TOTAL EPOCH: 22655, Episode: 288, Epoch: 40, Loss: 2.1205062239459949e-07, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 24, 3: 3}\n",
            "TOTAL EPOCH: 22675, Episode: 288, Epoch: 60, Loss: 0.004849899560213089, Mean Reward: 0.96875, Actions in batch: {0: 1, 1: 1, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 22695, Episode: 288, Epoch: 80, Loss: 0.0008523999713361263, Mean Reward: 1.6875, Actions in batch: {0: 1, 1: 1, 2: 29, 3: 1}\n",
            "TOTAL EPOCH: 22715, Episode: 289, Epoch: 0, Loss: 0.003242421429604292, Mean Reward: 1.65625, Actions in batch: {1: 4, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 22735, Episode: 289, Epoch: 20, Loss: 1.219497562487959e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 22755, Episode: 289, Epoch: 40, Loss: 0.002349509857594967, Mean Reward: 1.75, Actions in batch: {0: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 22775, Episode: 289, Epoch: 60, Loss: 6.3389670685864985e-06, Mean Reward: 0.0, Actions in batch: {1: 1, 2: 26, 3: 5}\n",
            "TOTAL EPOCH: 22795, Episode: 289, Epoch: 80, Loss: 7.46565319786896e-07, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 22815, Episode: 290, Epoch: 0, Loss: 0.0006616151076741517, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 22835, Episode: 290, Epoch: 20, Loss: 9.284053703595418e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 22855, Episode: 290, Epoch: 40, Loss: 0.00015016974066384137, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 1, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 22875, Episode: 290, Epoch: 60, Loss: 0.0024628855753690004, Mean Reward: 1.21875, Actions in batch: {0: 2, 1: 1, 2: 25, 3: 4}\n",
            "TOTAL EPOCH: 22895, Episode: 290, Epoch: 80, Loss: 1.159297698904993e-05, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 2, 2: 21, 3: 3}\n",
            "TOTAL EPOCH: 22915, Episode: 291, Epoch: 0, Loss: 0.004350412171334028, Mean Reward: 0.75, Actions in batch: {0: 2, 1: 1, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 22935, Episode: 291, Epoch: 20, Loss: 0.002940715290606022, Mean Reward: 1.125, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 22955, Episode: 291, Epoch: 40, Loss: 6.443904567277059e-06, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 24, 3: 2}\n",
            "TOTAL EPOCH: 22975, Episode: 291, Epoch: 60, Loss: 9.830367275753815e-08, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 2, 2: 25, 3: 4}\n",
            "TOTAL EPOCH: 22995, Episode: 291, Epoch: 80, Loss: 0.0042843944393098354, Mean Reward: 0.5625, Actions in batch: {0: 1, 1: 3, 2: 26, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 23010, Episode: 292, Epoch: 0, Loss: 6.262584793148562e-05, Mean Reward: 2.0625, Actions in batch: {0: 2, 1: 2, 2: 28}\n",
            "TOTAL EPOCH: 23030, Episode: 292, Epoch: 20, Loss: 0.0032711573876440525, Mean Reward: 2.28125, Actions in batch: {0: 1, 1: 4, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 23050, Episode: 292, Epoch: 40, Loss: 1.8296474081580527e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 28, 3: 2}\n",
            "TOTAL EPOCH: 23070, Episode: 292, Epoch: 60, Loss: 0.00010910225682891905, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 3, 2: 23, 3: 4}\n",
            "TOTAL EPOCH: 23090, Episode: 292, Epoch: 80, Loss: 0.004783359356224537, Mean Reward: 1.5625, Actions in batch: {0: 1, 2: 30, 3: 1}\n",
            "TOTAL EPOCH: 23110, Episode: 293, Epoch: 0, Loss: 7.262141298269853e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 29}\n",
            "TOTAL EPOCH: 23130, Episode: 293, Epoch: 20, Loss: 1.421116576239001e-05, Mean Reward: 0.0, Actions in batch: {1: 2, 2: 25, 3: 5}\n",
            "TOTAL EPOCH: 23150, Episode: 293, Epoch: 40, Loss: 0.003115940373390913, Mean Reward: 1.25, Actions in batch: {0: 1, 1: 3, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 23170, Episode: 293, Epoch: 60, Loss: 0.003533294191583991, Mean Reward: 0.9375, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 23190, Episode: 293, Epoch: 80, Loss: 3.54617259290535e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 22, 3: 6}\n",
            "TOTAL EPOCH: 23210, Episode: 294, Epoch: 0, Loss: 0.0007886275998316705, Mean Reward: 0.0, Actions in batch: {0: 2, 2: 27, 3: 3}\n",
            "TOTAL EPOCH: 23230, Episode: 294, Epoch: 20, Loss: 0.0002617788268253207, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 30, 3: 1}\n",
            "TOTAL EPOCH: 23250, Episode: 294, Epoch: 40, Loss: 7.49099999666214e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 26, 3: 3}\n",
            "TOTAL EPOCH: 23270, Episode: 294, Epoch: 60, Loss: 2.4608791591163026e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 5, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 23290, Episode: 294, Epoch: 80, Loss: 0.009907221421599388, Mean Reward: 0.15625, Actions in batch: {1: 1, 2: 29, 3: 2}\n",
            "TOTAL EPOCH: 23310, Episode: 295, Epoch: 0, Loss: 0.0001465807727072388, Mean Reward: 0.0, Actions in batch: {1: 4, 2: 28}\n",
            "TOTAL EPOCH: 23330, Episode: 295, Epoch: 20, Loss: 4.951936716679484e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 5, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 23350, Episode: 295, Epoch: 40, Loss: 3.7683606933569536e-05, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 5, 2: 22, 3: 1}\n",
            "TOTAL EPOCH: 23370, Episode: 295, Epoch: 60, Loss: 0.0018235695315524936, Mean Reward: 2.4375, Actions in batch: {0: 1, 1: 3, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 23390, Episode: 295, Epoch: 80, Loss: 0.00010718119301600382, Mean Reward: 0.0, Actions in batch: {2: 31, 3: 1}\n",
            "TOTAL EPOCH: 23410, Episode: 296, Epoch: 0, Loss: 0.008090212009847164, Mean Reward: 0.1875, Actions in batch: {0: 2, 2: 26, 3: 4}\n",
            "TOTAL EPOCH: 23430, Episode: 296, Epoch: 20, Loss: 0.0006080474704504013, Mean Reward: 1.1875, Actions in batch: {0: 3, 1: 4, 2: 21, 3: 4}\n",
            "TOTAL EPOCH: 23450, Episode: 296, Epoch: 40, Loss: 2.8242429834790528e-06, Mean Reward: 0.0, Actions in batch: {0: 3, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 23470, Episode: 296, Epoch: 60, Loss: 7.040399395918939e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 25, 3: 4}\n",
            "TOTAL EPOCH: 23490, Episode: 296, Epoch: 80, Loss: 0.0038436013273894787, Mean Reward: 0.9375, Actions in batch: {0: 2, 1: 2, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 23510, Episode: 297, Epoch: 0, Loss: 0.004977391101419926, Mean Reward: 0.8125, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 23530, Episode: 297, Epoch: 20, Loss: 0.005179101601243019, Mean Reward: 0.59375, Actions in batch: {0: 2, 1: 2, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 23550, Episode: 297, Epoch: 40, Loss: 5.068700556876138e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 26, 3: 2}\n",
            "TOTAL EPOCH: 23570, Episode: 297, Epoch: 60, Loss: 0.0016897546593099833, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 1, 2: 29}\n",
            "TOTAL EPOCH: 23590, Episode: 297, Epoch: 80, Loss: 4.404916035127826e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 23610, Episode: 298, Epoch: 0, Loss: 2.3097749362932518e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 1, 2: 24, 3: 4}\n",
            "TOTAL EPOCH: 23630, Episode: 298, Epoch: 20, Loss: 0.0030995027627795935, Mean Reward: 0.3125, Actions in batch: {1: 3, 2: 29}\n",
            "TOTAL EPOCH: 23650, Episode: 298, Epoch: 40, Loss: 1.3867618690710515e-05, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 23670, Episode: 298, Epoch: 60, Loss: 0.0030587762594223022, Mean Reward: 1.3125, Actions in batch: {0: 1, 1: 2, 2: 28, 3: 1}\n",
            "TOTAL EPOCH: 23690, Episode: 298, Epoch: 80, Loss: 1.666693606239278e-05, Mean Reward: 0.0, Actions in batch: {0: 1, 2: 31}\n",
            "TOTAL EPOCH: 23710, Episode: 299, Epoch: 0, Loss: 0.0029926756396889687, Mean Reward: 1.375, Actions in batch: {0: 6, 2: 23, 3: 3}\n",
            "TOTAL EPOCH: 23730, Episode: 299, Epoch: 20, Loss: 2.7508544917509425e-06, Mean Reward: 0.0, Actions in batch: {0: 1, 1: 3, 2: 25, 3: 3}\n",
            "TOTAL EPOCH: 23750, Episode: 299, Epoch: 40, Loss: 1.0291719263477717e-05, Mean Reward: 0.0, Actions in batch: {0: 3, 1: 2, 2: 25, 3: 2}\n",
            "TOTAL EPOCH: 23770, Episode: 299, Epoch: 60, Loss: 5.7962824939750135e-06, Mean Reward: 0.0, Actions in batch: {0: 2, 1: 1, 2: 27, 3: 2}\n",
            "TOTAL EPOCH: 23790, Episode: 299, Epoch: 80, Loss: 0.005744504742324352, Mean Reward: 0.5, Actions in batch: {0: 2, 1: 3, 2: 25, 3: 2}\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "# Create a DQN Agent\n",
        "model, optimizer = declare_model_and_optimier()\n",
        "model.to(model.device)\n",
        "\n",
        "# Use MSE as the loss function\n",
        "criterion = nn.MSELoss()\n",
        "buffer_size, number_of_episodes, number_of_epochs_per_episode, epochs_before_new_qloss, num_action_in_a_row = 10000, 300, 100, 10000, 4\n",
        "num_stack_frames=4\n",
        "\n",
        "\n",
        "losses = []\n",
        "rewards = []\n",
        "\n",
        "minibatch_size = 32\n",
        "\n",
        "qnet_loss_lifespan = 0\n",
        "max_episode_reward = 0\n",
        "for episode in range(number_of_episodes):\n",
        "    env = gym.make(\"ALE/Breakout-v5\", frameskip=1)\n",
        "    env = gym.wrappers.AtariPreprocessing(env, grayscale_obs=False)\n",
        "    env = gym.wrappers.FrameStack(env, num_stack=num_stack_frames)\n",
        "\n",
        "    # print(np.array(env.obs_buffer[0]).shape)\n",
        "    # env.reset()\n",
        "    s_t = preprocess_image_stack(env.reset()[0])\n",
        "    # print(s_t.shape)\n",
        "\n",
        "    epsilon = max(1.0 - (episode / number_of_episodes), 0.2)\n",
        "\n",
        "    # if (episode // 10) % 20 == 0:\n",
        "    #     qnet_loss_lifespan = 0\n",
        "    #     qnet_loss, _ = declare_model_and_optimier() # Reset the QNet Loss\n",
        "\n",
        "\n",
        "    loss_per_episode = []\n",
        "    reward_per_episode = []\n",
        "    qnet_loss, _ = declare_model_and_optimier(model_state_dict=model.state_dict())\n",
        "    qnet_loss.to(model.device)\n",
        "    qnet_loss.eval()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    for epoch in range(number_of_epochs_per_episode):\n",
        "\n",
        "        # Reduce linearly epsilon with regards to number of epochs\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # print(s_t.shape)\n",
        "            action = model.step(s_t)\n",
        "\n",
        "        # print(\"action: \", action)\n",
        "        for _ in range(num_action_in_a_row):\n",
        "            if done:\n",
        "              continue\n",
        "            s_t1, reward, done, _, info = env.step(action)\n",
        "            total_reward += reward\n",
        "        reward_per_episode.append(reward)\n",
        "\n",
        "        s_t1 = preprocess_image_stack(s_t1)\n",
        "\n",
        "        if len(replay_buffer) > buffer_size:\n",
        "            replay_buffer = replay_buffer[(int((1/10) * buffer_size)) + int(minibatch_size * ((1/10) * buffer_size)) % minibatch_size:]\n",
        "        replay_buffer.append((\n",
        "            torch.tensor(np.array(s_t.to('cpu'))[-1, :, :]),\n",
        "            action,\n",
        "            total_reward,\n",
        "            torch.tensor(np.array(s_t.to('cpu'))[-1, :, :]),\n",
        "            done,\n",
        "        ))\n",
        "        s_t = s_t1\n",
        "\n",
        "        qnet_loss_lifespan += 1\n",
        "        if qnet_loss_lifespan % epochs_before_new_qloss == 0 and episode > 5:\n",
        "            qnet_loss, _ = declare_model_and_optimier(model_state_dict=model.state_dict())\n",
        "            qnet_loss.train()\n",
        "            qnet_loss.to(model.device)\n",
        "\n",
        "            print(\"New QNet Loss\")\n",
        "\n",
        "        # Run some epochs before updating the Q-network\n",
        "        # if epoch % 3 != 0 and not done:\n",
        "            # continue\n",
        "\n",
        "        # Sample random minibatch of transitions from D\n",
        "        minibatch = None\n",
        "        if len(replay_buffer) < minibatch_size:\n",
        "          minibatch = replay_buffer\n",
        "        else:\n",
        "          idx = random.randint(0, len(replay_buffer) - minibatch_size)\n",
        "          minibatch = replay_buffer[idx: idx+minibatch_size]\n",
        "        #   if qnet_loss_lifespan < 1000:\n",
        "            # idx = random.randint(0, len(replay_buffer) - minibatch_size)\n",
        "            # minibatch = replay_buffer[idx: idx+minibatch_size]\n",
        "        #   else:\n",
        "            # minibatch = random.sample(replay_buffer, minibatch_size)\n",
        "        # if episode > 9:\n",
        "        #     minibatch = random.sample(replay_buffer, minibatch_size)\n",
        "        # else:\n",
        "        #     minibatch = [replay_buffer[-1]] if len(replay_buffer) > 0 else []\n",
        "        #     if minibatch == []:\n",
        "        #         continue\n",
        "\n",
        "\n",
        "        # Unpack the transition\n",
        "        loss_s_t, loss_action, loss_reward, loss_s_t1, loss_done = zip(*minibatch)\n",
        "        loss_s_t = torch.stack(loss_s_t).float().to(model.device)\n",
        "        loss_action = torch.tensor(loss_action).long().to(model.device)\n",
        "        loss_reward = torch.tensor(loss_reward).float().to(model.device)\n",
        "        loss_s_t1 = torch.stack(loss_s_t1).float().to(model.device)\n",
        "\n",
        "        outputs = torch.max(model(loss_s_t), dim=1).values.to(model.device)\n",
        "\n",
        "        # y_j = 0.99 * torch.max(qnet_loss(loss_s_t1), dim=1).values + loss_reward\n",
        "        y_j = loss_reward\n",
        "        y_j = torch.nn.functional.normalize(y_j, dim=0)\n",
        "        y_j = y_j.float().to(model.device)\n",
        "\n",
        "        loss = criterion(outputs, y_j)\n",
        "        loss_per_episode.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clamp the gradients between -1 and 1\n",
        "        # for param in model.parameters():\n",
        "            # param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(\"TOTAL EPOCH: {}, Episode: {}, Epoch: {}, Loss: {}, Mean Reward: {}, Actions in batch: {}\".format(qnet_loss_lifespan ,episode, epoch, loss.item(), loss_reward.mean().cpu().detach(), {k:v for k,v in zip(*np.unique(loss_action.to('cpu'), return_counts=True))}))\n",
        "\n",
        "        if done:\n",
        "            print(\"Max reward for this episode: \", total_reward)\n",
        "            if total_reward > max_episode_reward:\n",
        "                model.to('cpu')\n",
        "                torch.save(model.state_dict(), \"best_model.pt\")\n",
        "                print(\"Saving model that went for \", epoch, \" epochs with reward \", total_reward)\n",
        "                model.to(model.device)\n",
        "                max_episode_reward = total_reward\n",
        "            break\n",
        "\n",
        "\n",
        "    # Remove outliers from loss_per_episode\n",
        "    loss_per_episode = [x for x in loss_per_episode if (np.mean(loss_per_episode) - 2 * np.std(loss_per_episode) < x < np.mean(loss_per_episode) + 2 * np.std(loss_per_episode))]\n",
        "    losses.append(np.mean(loss_per_episode))\n",
        "\n",
        "    # Remove outliers from reward_per_episode\n",
        "    rewards.append(np.mean(reward_per_episode))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "jnnseeEdSEEx",
        "outputId": "d66d30db-8513-45f2-be7c-ddb3bdf111bc"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "# set y_axis to log scale\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LUT1srKSmOP",
        "outputId": "2aacba9b-fd99-4d83-95bd-076827156a79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5.0, 23809)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_episode_reward, qnet_loss_lifespan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p5dLrK6SEEy",
        "outputId": "57b14500-1375-4d32-f209-eecf058c41f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
            "Requirement already satisfied: joblib in /home/arnaudb/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages (1.3.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QbhpsNemSEEz"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.to('cpu')\n",
        "torch.save(model.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhFT8oqJeAv4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "replay_buffer = fill_replay_buffer([], 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New QNet Loss\n",
            "TOTAL EPOCH: 0, Loss: 0.021461859345436096, Mean Reward: 0.0625, Actions in batch: {0: 9, 1: 4, 2: 12, 3: 7}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 100, Loss: 1.8312485750016094e-08, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 6, 2: 14, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 200, Loss: 1.2553149630889493e-08, Mean Reward: 0.5, Actions in batch: {0: 5, 1: 5, 2: 12, 3: 10}\n",
            "Max reward for this episode:  3.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 300, Loss: 2.2378124242550257e-08, Mean Reward: 0.59375, Actions in batch: {0: 4, 1: 13, 2: 9, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 400, Loss: 2.1318948384418945e-08, Mean Reward: 2.28125, Actions in batch: {0: 5, 1: 8, 2: 12, 3: 7}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 500, Loss: 2.8747113134386382e-08, Mean Reward: 0.0, Actions in batch: {0: 11, 1: 8, 2: 7, 3: 6}\n",
            "Max reward for this episode:  1.0\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 600, Loss: 2.6894557692003218e-08, Mean Reward: 0.03125, Actions in batch: {0: 4, 1: 8, 2: 8, 3: 12}\n",
            "Max reward for this episode:  1.0\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 700, Loss: 1.9378921578550035e-08, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 6, 2: 10, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 800, Loss: 1.70924927545002e-08, Mean Reward: 1.0, Actions in batch: {0: 7, 1: 9, 2: 12, 3: 4}\n",
            "Max reward for this episode:  6.0\n",
            "TOTAL EPOCH: 900, Loss: 4.1554361729367884e-08, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 6, 3: 10}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  0.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 1000, Loss: 2.0340776174521125e-08, Mean Reward: 0.65625, Actions in batch: {0: 7, 1: 7, 2: 5, 3: 13}\n",
            "Max reward for this episode:  1.0\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 1100, Loss: 1.9695859165835827e-08, Mean Reward: 1.1875, Actions in batch: {0: 13, 1: 8, 2: 6, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 1200, Loss: 7.441620653025893e-09, Mean Reward: 0.625, Actions in batch: {0: 8, 1: 9, 2: 9, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1300, Loss: 2.111740826649111e-08, Mean Reward: 2.0, Actions in batch: {0: 4, 1: 9, 2: 14, 3: 5}\n",
            "Max reward for this episode:  3.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1400, Loss: 2.8986898215066503e-08, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 8, 2: 10, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1500, Loss: 8.958510377965467e-09, Mean Reward: 0.5625, Actions in batch: {0: 8, 1: 6, 2: 11, 3: 7}\n",
            "Max reward for this episode:  1.0\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 1600, Loss: 8.939027296150925e-09, Mean Reward: 1.03125, Actions in batch: {0: 7, 1: 3, 2: 14, 3: 8}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 1700, Loss: 5.318156581779476e-09, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 8, 2: 15, 3: 3}\n",
            "Max reward for this episode:  4.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 1800, Loss: 1.5347097814810695e-08, Mean Reward: 0.0, Actions in batch: {0: 10, 1: 7, 2: 10, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 1900, Loss: 7.257501266622057e-09, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 8, 3: 9}\n",
            "Max reward for this episode:  3.0\n",
            "Max reward for this episode:  0.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 2000, Loss: 1.3432144285729919e-08, Mean Reward: 0.09375, Actions in batch: {0: 7, 1: 10, 2: 9, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2100, Loss: 5.4643845004420655e-09, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 9, 2: 9, 3: 9}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2200, Loss: 1.6398109536908123e-08, Mean Reward: 0.96875, Actions in batch: {0: 6, 1: 6, 2: 11, 3: 9}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2300, Loss: 1.1888793061132219e-08, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 12, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 2400, Loss: 1.1998397830836893e-08, Mean Reward: 0.03125, Actions in batch: {0: 10, 1: 4, 2: 11, 3: 7}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2500, Loss: 5.770818933825694e-09, Mean Reward: 1.0625, Actions in batch: {0: 8, 1: 8, 2: 8, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2600, Loss: 8.055108580151682e-09, Mean Reward: 0.75, Actions in batch: {0: 5, 1: 11, 2: 8, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2700, Loss: 8.71112959544007e-09, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 10, 2: 10, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 2800, Loss: 2.155107914347809e-08, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 8, 2: 8, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 2900, Loss: 1.2429551610182443e-08, Mean Reward: 0.5625, Actions in batch: {0: 8, 1: 9, 2: 7, 3: 8}\n",
            "Max reward for this episode:  4.0\n",
            "Max reward for this episode:  0.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 3000, Loss: 1.348729217198752e-08, Mean Reward: 0.625, Actions in batch: {0: 4, 1: 9, 2: 13, 3: 6}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 3100, Loss: 1.235972124646878e-08, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 12, 3: 5}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 3200, Loss: 1.2954290085076536e-08, Mean Reward: 0.28125, Actions in batch: {0: 11, 1: 5, 2: 8, 3: 8}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 3300, Loss: 2.0717498827593772e-08, Mean Reward: 1.0, Actions in batch: {0: 5, 1: 6, 2: 19, 3: 2}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 3400, Loss: 6.219926351747063e-09, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 3, 2: 15, 3: 5}\n",
            "Max reward for this episode:  3.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3500, Loss: 1.5117695539856868e-08, Mean Reward: 0.0, Actions in batch: {0: 5, 1: 7, 2: 11, 3: 9}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 3600, Loss: 2.0493331476245658e-08, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 10, 2: 8, 3: 8}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 3700, Loss: 2.3246235159035677e-08, Mean Reward: 0.65625, Actions in batch: {0: 13, 1: 10, 2: 5, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3800, Loss: 8.179699584331956e-09, Mean Reward: 0.8125, Actions in batch: {0: 9, 1: 5, 2: 10, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 3900, Loss: 6.879314451424534e-09, Mean Reward: 1.4375, Actions in batch: {0: 9, 1: 6, 2: 13, 3: 4}\n",
            "Max reward for this episode:  4.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 4000, Loss: 4.124703245622641e-09, Mean Reward: 1.375, Actions in batch: {0: 4, 1: 2, 2: 22, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 4100, Loss: 1.3955506084073477e-08, Mean Reward: 2.25, Actions in batch: {0: 11, 1: 11, 2: 7, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4200, Loss: 7.1583743377345854e-09, Mean Reward: 2.6875, Actions in batch: {0: 10, 1: 3, 2: 13, 3: 6}\n",
            "Max reward for this episode:  4.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4300, Loss: 7.345109853673648e-09, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 10, 2: 9, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4400, Loss: 1.708967189983923e-08, Mean Reward: 0.90625, Actions in batch: {0: 6, 1: 8, 2: 7, 3: 11}\n",
            "Max reward for this episode:  4.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4500, Loss: 1.1829766499715788e-08, Mean Reward: 0.75, Actions in batch: {0: 7, 1: 9, 2: 12, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4600, Loss: 7.470699614486875e-09, Mean Reward: 0.375, Actions in batch: {0: 2, 1: 3, 2: 18, 3: 9}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4700, Loss: 3.3565887846975784e-09, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 7, 2: 13, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 4800, Loss: 8.240432336492631e-09, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 12, 2: 8, 3: 8}\n",
            "TOTAL EPOCH: 4900, Loss: 3.0980626952725743e-09, Mean Reward: 0.9375, Actions in batch: {0: 6, 1: 5, 2: 17, 3: 4}\n",
            "Max reward for this episode:  4.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 5000, Loss: 4.1957313179352695e-09, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 6, 2: 13, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 5100, Loss: 7.910540666955512e-09, Mean Reward: 1.0, Actions in batch: {0: 6, 1: 6, 2: 18, 3: 2}\n",
            "Max reward for this episode:  1.0\n",
            "TOTAL EPOCH: 5200, Loss: 1.0933048244510246e-08, Mean Reward: 3.15625, Actions in batch: {0: 6, 1: 3, 2: 21, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5300, Loss: 6.515164407971952e-09, Mean Reward: 0.625, Actions in batch: {0: 4, 1: 3, 2: 21, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5400, Loss: 1.598921173240342e-08, Mean Reward: 0.6875, Actions in batch: {0: 3, 1: 10, 2: 8, 3: 11}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5500, Loss: 5.708370220958159e-09, Mean Reward: 2.9375, Actions in batch: {0: 11, 1: 7, 2: 8, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 5600, Loss: 3.7578459455289703e-08, Mean Reward: 2.75, Actions in batch: {0: 7, 1: 7, 2: 12, 3: 6}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 5700, Loss: 5.187549056984153e-09, Mean Reward: 0.09375, Actions in batch: {0: 8, 1: 4, 2: 11, 3: 9}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5800, Loss: 8.002638551829477e-09, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 6, 2: 13, 3: 5}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 5900, Loss: 4.979851642161748e-09, Mean Reward: 1.5, Actions in batch: {0: 3, 1: 5, 2: 18, 3: 6}\n",
            "Max reward for this episode:  2.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 6000, Loss: 3.974111706384065e-09, Mean Reward: 0.25, Actions in batch: {0: 10, 1: 5, 2: 9, 3: 8}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6100, Loss: 5.303135708345508e-09, Mean Reward: 0.3125, Actions in batch: {0: 7, 1: 4, 2: 16, 3: 5}\n",
            "Max reward for this episode:  2.0\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6200, Loss: 1.3972056844835379e-09, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 1, 2: 26, 3: 1}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 6300, Loss: 1.397607896080899e-08, Mean Reward: 0.71875, Actions in batch: {0: 6, 1: 11, 2: 9, 3: 6}\n",
            "TOTAL EPOCH: 6400, Loss: 2.2916359920799323e-09, Mean Reward: 2.28125, Actions in batch: {0: 3, 1: 4, 2: 25}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 6500, Loss: 7.138409863216566e-09, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 3, 2: 22, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6600, Loss: 1.276084127255217e-08, Mean Reward: 0.25, Actions in batch: {0: 5, 1: 15, 2: 8, 3: 4}\n",
            "TOTAL EPOCH: 6700, Loss: 5.170265993115208e-09, Mean Reward: 0.21875, Actions in batch: {0: 4, 1: 9, 2: 16, 3: 3}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 6800, Loss: 2.17487228226787e-09, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 5, 2: 19, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 6900, Loss: 2.739666937756624e-09, Mean Reward: 1.625, Actions in batch: {0: 8, 1: 8, 2: 12, 3: 4}\n",
            "Max reward for this episode:  0.0\n",
            "Max reward for this episode:  0.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 7000, Loss: 8.011078023173468e-09, Mean Reward: 0.0, Actions in batch: {0: 4, 1: 2, 2: 26}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7100, Loss: 2.2923021258947074e-09, Mean Reward: 2.0, Actions in batch: {0: 1, 1: 4, 2: 24, 3: 3}\n",
            "Max reward for this episode:  0.0\n",
            "TOTAL EPOCH: 7200, Loss: 1.1152114787194023e-08, Mean Reward: 0.0, Actions in batch: {0: 7, 1: 8, 2: 5, 3: 12}\n",
            "Max reward for this episode:  4.0\n",
            "TOTAL EPOCH: 7300, Loss: 3.576301033092477e-09, Mean Reward: 2.0, Actions in batch: {0: 2, 1: 5, 2: 23, 3: 2}\n",
            "TOTAL EPOCH: 7400, Loss: 2.5749975485211962e-09, Mean Reward: 0.875, Actions in batch: {0: 4, 1: 8, 2: 18, 3: 2}\n",
            "Max reward for this episode:  5.0\n",
            "TOTAL EPOCH: 7500, Loss: 1.8656862721400103e-08, Mean Reward: 3.0, Actions in batch: {0: 2, 1: 2, 2: 27, 3: 1}\n",
            "TOTAL EPOCH: 7600, Loss: 2.516771679950125e-09, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 8, 2: 13, 3: 2}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 7700, Loss: 3.5634162287578874e-09, Mean Reward: 0.4375, Actions in batch: {0: 8, 1: 9, 2: 7, 3: 8}\n",
            "TOTAL EPOCH: 7800, Loss: 4.4602201931809304e-09, Mean Reward: 0.0625, Actions in batch: {0: 2, 1: 1, 2: 25, 3: 4}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 7900, Loss: 4.657827901155542e-09, Mean Reward: 1.59375, Actions in batch: {0: 4, 1: 2, 2: 24, 3: 2}\n",
            "Max reward for this episode:  0.0\n",
            "New QNet Loss\n",
            "TOTAL EPOCH: 8000, Loss: 2.72165623371734e-09, Mean Reward: 0.0, Actions in batch: {0: 9, 1: 7, 2: 10, 3: 6}\n",
            "Max reward for this episode:  3.0\n",
            "TOTAL EPOCH: 8100, Loss: 7.575143179394672e-09, Mean Reward: 0.0, Actions in batch: {0: 8, 1: 7, 2: 15, 3: 2}\n",
            "TOTAL EPOCH: 8200, Loss: 2.032841450727574e-09, Mean Reward: 0.0, Actions in batch: {0: 6, 1: 2, 2: 21, 3: 3}\n",
            "Max reward for this episode:  2.0\n",
            "TOTAL EPOCH: 8300, Loss: 1.6738854746733978e-09, Mean Reward: 2.1875, Actions in batch: {1: 2, 2: 27, 3: 3}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    112\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 113\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTOTAL EPOCH: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Mean Reward: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, Actions in batch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, loss\u001b[39m.\u001b[39mitem(), loss_reward\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach(), {k:v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39munique(loss_action\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), return_counts\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))}))\n",
            "File \u001b[0;32m~/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages/torch/optim/rmsprop.py:117\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    113\u001b[0m     momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, grads, square_avgs, momentum_buffer_list, grad_avgs)\n\u001b[0;32m--> 117\u001b[0m     rmsprop(\n\u001b[1;32m    118\u001b[0m         params_with_grad,\n\u001b[1;32m    119\u001b[0m         grads,\n\u001b[1;32m    120\u001b[0m         square_avgs,\n\u001b[1;32m    121\u001b[0m         grad_avgs,\n\u001b[1;32m    122\u001b[0m         momentum_buffer_list,\n\u001b[1;32m    123\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    124\u001b[0m         alpha\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39malpha\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    125\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    126\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    127\u001b[0m         momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    128\u001b[0m         centered\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcentered\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    129\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    130\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    131\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages/torch/optim/rmsprop.py:232\u001b[0m, in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, foreach, maximize, differentiable, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_rmsprop\n\u001b[0;32m--> 232\u001b[0m func(\n\u001b[1;32m    233\u001b[0m     params,\n\u001b[1;32m    234\u001b[0m     grads,\n\u001b[1;32m    235\u001b[0m     square_avgs,\n\u001b[1;32m    236\u001b[0m     grad_avgs,\n\u001b[1;32m    237\u001b[0m     momentum_buffer_list,\n\u001b[1;32m    238\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    239\u001b[0m     alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    240\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    241\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    242\u001b[0m     momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    243\u001b[0m     centered\u001b[39m=\u001b[39;49mcentered,\n\u001b[1;32m    244\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    245\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    246\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/EPITA/ING3/rele/.venv/lib/python3.11/site-packages/torch/optim/rmsprop.py:300\u001b[0m, in \u001b[0;36m_single_tensor_rmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered, maximize, differentiable)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mif\u001b[39;00m is_complex_param:\n\u001b[1;32m    299\u001b[0m         buf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(buf)\n\u001b[0;32m--> 300\u001b[0m     buf\u001b[39m.\u001b[39;49mmul_(momentum)\u001b[39m.\u001b[39maddcdiv_(grad, avg)\n\u001b[1;32m    301\u001b[0m     param\u001b[39m.\u001b[39madd_(buf, alpha\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mlr)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Create a DQN Agent\n",
        "model, optimizer = declare_model_and_optimier()\n",
        "model.to(model.device)\n",
        "\n",
        "# Use MSE as the loss function\n",
        "criterion = nn.MSELoss()\n",
        "buffer_size = 10000\n",
        "num_epochs = 10000\n",
        "epochs_before_new_qloss = 1000\n",
        "num_action_in_a_row = 4\n",
        "num_stack_frames=4\n",
        "minibatch_size = 32\n",
        "\n",
        "# Pepare the environment\n",
        "env = gym.make(\"ALE/Breakout-v5\", frameskip=1)\n",
        "env = gym.wrappers.AtariPreprocessing(env, grayscale_obs=False)\n",
        "env = gym.wrappers.FrameStack(env, num_stack=num_stack_frames)\n",
        "s_t = preprocess_image_stack(env.reset()[0])\n",
        "\n",
        "losses = []\n",
        "rewards = []\n",
        "max_episode_reward = 0\n",
        "\n",
        "loss_per_episode = []\n",
        "reward_per_episode = 0\n",
        "\n",
        "qnet_loss, _ = declare_model_and_optimier(model_state_dict=model.state_dict())\n",
        "qnet_loss.to(model.device)\n",
        "done = False\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Reduce linearly epsilon with regards to number of epochs\n",
        "    epsilon = max(1.0 - (epoch / num_epochs), 0.1)\n",
        "\n",
        "    # Get action from the model or random\n",
        "    if np.random.rand() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = model.step(s_t)\n",
        "\n",
        "    # Do action and get reward\n",
        "    for _ in range(num_action_in_a_row):\n",
        "        s_t1, reward, done, _, info = env.step(action)\n",
        "        reward_per_episode += reward\n",
        "        if done: break\n",
        "    \n",
        "    if done:\n",
        "        print(\"Max reward for this episode: \", reward_per_episode)\n",
        "\n",
        "        env.reset()\n",
        "        s_t = preprocess_image_stack(env.reset()[0])\n",
        "        max_episode_reward = max(max_episode_reward, reward_per_episode)\n",
        "        rewards.append(reward_per_episode)\n",
        "        losses.append(np.mean(loss_per_episode))\n",
        "        loss_per_episode = []\n",
        "        reward_per_episode = 0\n",
        "        done = False\n",
        "        continue\n",
        "\n",
        "    # Preprocess the next frame\n",
        "    s_t1 = preprocess_image_stack(s_t1)\n",
        "\n",
        "    # Update replay buffer\n",
        "    if len(replay_buffer) > buffer_size:\n",
        "        replay_buffer = replay_buffer[(int((1/10) * buffer_size)) + int(minibatch_size * ((1/10) * buffer_size)) % minibatch_size:]\n",
        "    replay_buffer.append((\n",
        "        torch.tensor(np.array(s_t.to('cpu'))[-1, :, :]),\n",
        "        action,\n",
        "        reward_per_episode,\n",
        "        torch.tensor(np.array(s_t.to('cpu'))[-1, :, :]),\n",
        "        done,\n",
        "    ))\n",
        "\n",
        "    # Current state is now the next state\n",
        "    s_t = s_t1\n",
        "\n",
        "\n",
        "    # Update the goal network\n",
        "    if epoch % epochs_before_new_qloss == 0:\n",
        "        qnet_loss, _ = declare_model_and_optimier(model_state_dict=model.state_dict())\n",
        "        qnet_loss.to(model.device)\n",
        "        print(\"New QNet Loss\")\n",
        "\n",
        "    # Sample random minibatch of transitions from D\n",
        "    idx = random.randint(0, len(replay_buffer) - minibatch_size)\n",
        "    minibatch = replay_buffer[idx: idx+minibatch_size]\n",
        "\n",
        "    # Unpack the transitions in minibatch\n",
        "    loss_s_t, loss_action, loss_reward, loss_s_t1, loss_done = zip(*minibatch)\n",
        "\n",
        "    # Convert to tensors the minibatch\n",
        "    loss_s_t = torch.stack(loss_s_t).float().to(model.device)\n",
        "    loss_action = torch.tensor(loss_action).long().to(model.device)\n",
        "    loss_reward = torch.tensor(loss_reward).float().to(model.device)\n",
        "    loss_s_t1 = torch.stack(loss_s_t1).float().to(model.device)\n",
        "    loss_done = torch.tensor(loss_done).float().to(model.device)\n",
        "\n",
        "    # Compute each outputs for minibatch\n",
        "    outputs = torch.max(model(loss_s_t), dim=1).values.to(model.device)\n",
        "\n",
        "    # Compute labels for each outputs\n",
        "    Y_j = 0.99 * torch.max(qnet_loss(loss_s_t1), dim=1).values * loss_done + loss_reward\n",
        "    y_j = torch.nn.functional.normalize(y_j, dim=0)\n",
        "    y_j = y_j.float().to(model.device)\n",
        "\n",
        "    # Backpropagate the loss\n",
        "    loss = criterion(outputs, y_j)\n",
        "    loss_per_episode.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(\"TOTAL EPOCH: {}, Loss: {}, Mean Reward: {}, Actions in batch: {}\".format(epoch, loss.item(), loss_reward.mean().cpu().detach(), {k:v for k,v in zip(*np.unique(loss_action.to('cpu'), return_counts=True))}))\n",
        "\n",
        "\n",
        "max_episode_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.to('cpu')\n",
        "torch.save(model.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(rewards)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
